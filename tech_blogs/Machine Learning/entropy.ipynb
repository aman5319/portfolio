{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Entropy\n",
    "author: Aman Pandey\n",
    "date: \"10/07/2025\"\n",
    "description: Entropy\n",
    "format: \n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "editor: visual\n",
    "categories: [math]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "\n",
    "Entropy is the universe’s most universal law, shaping everything from energy flow to information.\n",
    "\n",
    "- In mathematics, it’s a formula that measures uncertainty.\n",
    "- In physics, it’s the law that governs how heat and energy spread.\n",
    "- In chemistry, it’s the tally of molecular disorder.\n",
    "- In information theory, it’s the currency of surprise in messages.\n",
    "- In machine learning, it’s the score of impurity in decision-making.\n",
    "\n",
    "\n",
    "But are all of these really talking about the same thing?</br>\n",
    "Are they all just using one tool entropy dressed in different clothes?</br>\n",
    "Let’s unpack each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common way to describe entropy is as disorder, random, more mixed and less ordered. But on a very fundamental level let's look it in a way where I want to know how something is distributed in a system.\n",
    "\n",
    "Here are two important words **system** and **distributed**.\n",
    "\n",
    "**System** - In science, a system is simply the part of the universe you choose to focus on or study. Everything outside that part is called the surroundings or environment. For example you want to study about what is happening inside a bag or a kettle or a kitchen.\n",
    "\n",
    "Types of systems based on interaction with surroundings:\n",
    "\n",
    "**Closed system**\n",
    "\n",
    "    Exchanges energy (like heat or work) but not matter with its surroundings.\n",
    "\n",
    "    Example: A sealed, insulated container where heat can pass through the walls, but no gas or liquid escapes or enters.\n",
    "\n",
    "**Open system**\n",
    "\n",
    "    Exchanges both energy and matter with its surroundings.\n",
    "\n",
    "    Example: A boiling pot of water without a lid  steam (matter) escapes, and heat (energy) flows in/out.\n",
    "\n",
    "**Isolated system**\n",
    "\n",
    "    Exchanges neither energy nor matter with its surroundings.\n",
    "\n",
    "    Example: An ideal thermos bottle perfectly insulated so nothing gets in or out (theoretical, as perfect isolation is impossible).\n",
    "\n",
    "**Distributed** - Defining Distribution can be tricky because first we have to define exactly what that thing is we want to measure the distribution and then we can ask whether that thing is distributed uniformly, skewed or follows any particular pattern\n",
    "\n",
    "For example, if I treat a living room as an isolated system containing 100 pieces of clothing, I might ask how they’re distributed: neatly folded in one corner or scattered randomly across the room.\n",
    "\n",
    "Suppose a person enters the living room blindfolded, playing a game where they try to pick up a piece of clothing every 5 seconds, repeating this 100 times. If the clothes are scattered randomly (high entropy), they’re more likely to pick up a piece because the clothes are spread out. If the clothes are neatly folded in one corner (low entropy), they’re less likely to pick one up unless they happen to walk to that corner.\n",
    "\n",
    "Entropy describes how energy, information, heat, clothes, or anything else is distributed within a system. It doesn’t measure the quantity or intensity of the thing, only the likelihood of its arrangement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the example of hot and cold metal bars to solidfy the idea of energy distribution aka entropy.\n",
    "\n",
    "A 300°C iron metal rod is there what are the factors that can affect the energy distribution.\n",
    "\n",
    "1. Temperature: It defines how particles move. Higher temperature leads to higher kinetic energyatoms vibrate more vigorously. This increases the number of accessible microstates, affecting the distribution and raising entropy.\n",
    "2. Pressure or Volume: These can affect the density of states. For solids like iron, volume changes are small, but in gases, they significantly impact how particles spread out.\n",
    "3. Number of Particles: More particles mean more interactions and jiggling, leading to higher entropy (entropy is extensive).\n",
    "4. Microstates (W): From a statistical view, entropy depends on the number of possible positions, momenta, and quantum states of particles (S = k ln W).\n",
    "5. State of Matter (Solid/Liquid/Gas): Solids have low entropy due to ordered structures; liquids have more disorder; gases have the highest entropy from free particle motion.\n",
    "\n",
    "Now here i have listed few, it can be more, the more things we get to know about a particular system the more accurately we can predict or calculate its entropy in a given framework we are working.\n",
    "\n",
    "As you can see these parameters don’t act independently; they’re deeply interrelated. Measuring the effect of each on entropy in isolation is complicated because changing one often changes others. Here’s a more precise view:\n",
    "\n",
    "Why they are interdependent:\n",
    "\n",
    "    Temperature T affects how particles move, which influences heat capacity Cp​ because heat capacity often varies with temperature.\n",
    "\n",
    "    Changing volume VV changes the pressure P (for gases) and the available phase space, which in turn affects energy levels and particle behavior.\n",
    "\n",
    "    Number of particles N changes density, which impacts how particles interact, affecting heat capacity and accessible microstates W.\n",
    "\n",
    "    The microstates W depend on all of these combined temperature, volume, particle number, and quantum states collectively define how many microstates are available.\n",
    "\n",
    "How to measure the affect of each parameter considering interdependence:\n",
    "\n",
    "    Control variables carefully in experiments or simulations\n",
    "\n",
    "    Change one parameter while keeping others fixed as much as possible (e.g., vary temperature at constant volume and particle number).\n",
    "\n",
    "    Measure entropy change experimentally or compute it via statistical methods.\n",
    "\n",
    "\n",
    "Now let's suppose we want to calculate the entropy of the iron rod at 300C we have to first define the system in this way.\n",
    "\n",
    "Step 1: Define the system and assumptions\n",
    "\n",
    "    The rod is uniform and homogeneous (same material properties throughout).\n",
    "\n",
    "    Heat transfer is slow enough to assume quasi-static (reversible) heating.\n",
    "\n",
    "    The rod’s mass m, specific heat capacity Cp​(T), are known or measurable.\n",
    "\n",
    "    Ignore volume changes if thermal expansion is negligible (common for solids).\n",
    "\n",
    "Step 2: Define a reference\n",
    "    Entropy is always measured relative to a reference state (often at a baseline temperature or zero entropy state).\n",
    "    \n",
    "    You don’t just “pick a random state and say the entropy is X” without context.\n",
    "\n",
    "    You measure or calculate the change in entropy ΔS=S_final−S_initial​ when the system moves from one state to another (e.g., energy x at time t1​ to energy y at time t2​).\n",
    "    \n",
    "    Absolute entropy values can be tabulated (like standard molar entropy), but these are always relative to a defined zero-point.\n",
    "\n",
    "\n",
    "Step 3: Use physics formulae to calculate one;\n",
    "    If the system was loosing energy after time t2 we would say the entropy of the rod has decreased,why because kinetic energy has reduced, leading to less jiggle, leading to less movement and microstate but surroundings gains that energy, increasing their entropy by a greater amount (because the surroundings are usually at a lower temperature or can spread the heat more effectively).\n",
    "\n",
    "    If the system was gaining energy from outside source after time t2 we would say the entropy has increased it has gained that energy from the surrounding.\n",
    "        \n",
    "\n",
    "\n",
    "After going through example it will be clear, three things that defines entropy completely -\n",
    "\n",
    "- Distribution of what exactly are we measuring (energy, heat, information, clothes)\n",
    "- Variables affecting the distribution (tempeature, pressure, state)\n",
    "- how do we measure the distribution.\n",
    "    - For example, In a 100 sq, meter room if 100 clothes are thrown at random we can measure the distrbution by counting how many clothes are there in each 1 sq, meter box.\n",
    "    - For the heated rod, count the number of microstates an atom can take (e.g., unique vibrational or electronic states).\n",
    "\n",
    "The above defines the whole of Thermodynamic entropy (physical entropy)\n",
    "\n",
    "    This depends on the actual physical state of the system: energy, temperature, molecular configurations, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have noticed one thing we were till now measuring the distribution using counting method this is actually Boltzmann Entropy (Simple Microstates Counting)\n",
    "\n",
    "Boltzmann Entropy (Simple Microstates Counting)\n",
    "\n",
    "    Applies to a system in a single macrostate, where all microstates are equally likely.\n",
    "\n",
    "    Entropy is based on counting how many microstates Ω correspond to that macrostate:\n",
    "\n",
    "    Example: Imagine you have 3 coins lying on a table, and you only care about how many coins show heads.\n",
    "\n",
    "        Macrostate: Exactly 2 coins are heads.\n",
    "\n",
    "        Microstates: The specific arrangements that have 2 heads and 1 tail. There are 3 such microstates (HTH, HHT, THH).\n",
    "\n",
    "    Since all these microstates are equally probable, Boltzmann entropy counts these 3 microstates.\n",
    "\n",
    "Gibbs Entropy (Probability Weighted)\n",
    "\n",
    "    Applies when the system is in a statistical mixture of microstates, each with a probability pipi​.\n",
    "\n",
    "    Entropy accounts for the uncertainty over which microstate the system is actually in:\n",
    "\n",
    "    Example: Same 3 coins, but now you know the probability of each microstate is different:\n",
    "\n",
    "        Suppose probability that the coins are in microstate 1 (HTH) is 0.5,\n",
    "\n",
    "        Microstate 2 (HHT) is 0.3,\n",
    "\n",
    "        Microstate 3 (THH) is 0.2.\n",
    "\n",
    "    Now entropy measures the uncertainty weighted by these probabilities.\n",
    "\n",
    "\n",
    "There is another one more of way looking at entropy from Clausius -\n",
    "\n",
    " **entropy as the unavailability of energy to do useful work**  a practical way to understand what entropy means beyond just counting microstates.\n",
    "\n",
    "\n",
    "What does this mean?\n",
    "\n",
    "Energy can exist in many forms: heat, mechanical work, chemical energy, etc.\n",
    "\n",
    "Not all energy in a system can be converted into useful work. Some energy is \"lost\" or dispersed in ways that can't be harnessed to do things like move a piston, run a motor, or power a machine.\n",
    "\n",
    "Entropy quantifies that \"lost\" or unavailable portion of energy  the part that is spread out or randomized so much that you cannot concentrate or convert it back into work.\n",
    "\n",
    "\n",
    "### Intuition:\n",
    "\n",
    "* High-quality energy (like work or high-temperature heat) can be fully converted into useful work. i.e a heated iron rod can be used to heat up a water.\n",
    "Entropy is lower.\n",
    "\n",
    "* Low-quality energy (like heat evenly spread in the environment at room temperature) can’t be fully converted to work. a room temperature rod can't be used to heat up water hence entropy is higher.\n",
    "\n",
    "* Entropy measures how much energy has \"degraded\" from high-quality to low-quality, hence reducing the capacity to do work.\n",
    "\n",
    "When energy disperses, it spreads over many particles and states  increasing the number of accessible microstates.\n",
    "\n",
    "The system moves from an ordered, low-entropy state (few microstates, energy concentrated, usable) to a disordered, high-entropy state (many microstates, energy spread out, less usable).\n",
    "\n",
    "So “lost energy” is the energy tied up in many microscopic configurations that can’t be coordinated to do macroscopic work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting example\n",
    "Absolutely! Here's a clear, jargon-free write-up using the sun example that connects entropy across different domains  thermodynamics and information  step by step, perfect for your blog:\n",
    "\n",
    "\n",
    "**Step 1: The Sun as a Hot, Complex System**\n",
    "\n",
    "The sun is incredibly hot and full of activity. Inside it, trillions of tiny particles move in countless ways. Because of all this complexity, scientists say the sun has *high entropy*. This means there are many possible ways the sun’s particles can be arranged inside it  a lot of microscopic disorder.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2: Looking at the Sun and Space Together**\n",
    "\n",
    "Now, if we consider not just the sun but the space around it  the light and heat it sends out  we get a bigger system. The sun transfers energy to space all the time, and this movement adds to the total disorder or entropy of this larger system. hence sun's entropy is lower but overall system entropy is hight\n",
    "\n",
    "---\n",
    "\n",
    "**Step 3: What About Predicting the Sun’s State?**\n",
    "\n",
    "Even though the sun is complex inside, from far away, it looks very stable. For example, we know it will be hot tomorrow, just like today. Because of this, *our uncertainty* about the sun’s overall state is very low. We don’t need much new information to describe what the sun will be like.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 4: Two Different Kinds of Entropy**\n",
    "\n",
    "Here’s the key: The word \"entropy\" is used in two related but different ways.\n",
    "\n",
    "* One kind of entropy measures *how complex or disordered* a system is inside  like the sun’s many tiny particle arrangements.\n",
    "\n",
    "* The other kind measures *how uncertain or surprised* we are about what the system will do  like how much information we need to describe the sun’s state tomorrow.\n",
    "\n",
    "---\n",
    "Step 6: The Role of Granularity  How Closely You Look Matters\n",
    "\n",
    "The “overall state” of the sun depends on how closely or in detail you choose to look.\n",
    "\n",
    "    If you only care about big things  like whether the sun is hot or not  then the sun’s state seems very simple and predictable, and you need little information to describe it.\n",
    "\n",
    "    But if you zoom in to tiny details  the exact position and energy of every particle inside the sun  the state becomes incredibly complex and uncertain, and you’d need a huge amount of information to describe it fully.\n",
    "\n",
    "So, the amount of entropy or uncertainty  you assign depends on the level of detail you’re considering.\n",
    "\n",
    "An object with low entropy has less uncertainty about its state  but this depends on how detailed or ‘fine-grained’ the description of the state is.\n",
    "\n",
    "what will happen if we get to know more things the uncertaininty decreases hence the information entropy decreases when information is hidden uncertainty grows, and information entropy increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Entropy\n",
    "A key measure in information theory is entropy. Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process.\n",
    "\n",
    "For example, consider rain in two locations: the Amazon rainforest and a tropical city. In the Amazon, it rains almost every day, so the outcome is fairly predictable low uncertainty, low entropy. In the tropical city, rain is less predictable, so the uncertainty is higher higher entropy.\n",
    "\n",
    "A very important question can arise why are we measuring uncertainty and not certainty itself, because if we talk about any process and if something is guranteed it conveys no new information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next question is how do we calculate uncertainty in shanon's paper the way he defines the property of entropy is as following -\n",
    "\n",
    "\n",
    "Suppose we have a set of possible events whose probabilities of occurrence are (p1; p2; : : : ; pn).\n",
    "These probabilities are known but that is all we know concerning which event will occur.\n",
    "\n",
    "If take weather prediction example p1 can be probability of rain, p2 can be probability of strong wind, p3 could be probability of high humidity.\n",
    "\n",
    "can we measure of how much “choice” is involved in the selection of the event or of how uncertain we are of the outcome?\n",
    "\n",
    "If there is such a measure, say H( p1; p2; : : : ; pn), it is reasonable to require of it the following properties:\n",
    "\n",
    "Axiom 1. H should be continuous in the pi.\n",
    "    What it means: \n",
    "\n",
    "    Continuity here means that small changes in the probabilities of events should lead to small changes in entropy. Entropy shouldn’t “jump” suddenly if the probabilities change just a little. For a fair coin the probability of Head and Tail is same and entropy is at its maximum H.\n",
    "\n",
    "    Now, slightly bias it: p_h = 0.51 and p_t =0.49\n",
    "\n",
    "The uncertainty has decreased a tiny bit, because the coin is slightly more predictable now. Entropy changes slightly  it doesn’t suddenly drop to zero or skyrocket.\n",
    "\n",
    "If H were not continuous, then a tiny change in probability could make the entropy jump wildly, which wouldn’t make sense small changes in our knowledge shouldn’t cause a huge change in measured uncertainty.\n",
    "\n",
    "Axiom 2. If all the pi are equal, pi = 1/n, then H should be a monotonic increasing function of n.  With equally likely events there is more choice, or uncertainty, when there are more possible events.\n",
    "    What it means: \n",
    "        For a fair coin and a fair dice both have equal probability of each event but rolling a die has more probable chances means more uncertainty.\n",
    "        Even though each outcome is equally likely in both cases, rolling a die is less predictable than flipping a coin more possible choices, more uncertainty, higher entropy.\n",
    "\n",
    "Axiom 3. If a choice be broken down into two successive choices, the original H should be the weighted sum of the individual values of H.\n",
    "    What it means: \n",
    "        if we are calculating the probability of an event by breaking it into multiple choices which we do in Law of total probability calculation same way entropy should also add up.\n",
    "\n",
    "![image](images/tree.png)\n",
    "         \n",
    "Hence we are adding up the entropy at each step from start to the end node.\n",
    "\n",
    "The only function which can do all this $$ H(i) = - \\log(pi) $$\n",
    "\n",
    "Hence entropy is converted in log probabilities, now we are just calculating entropy of one event but in a random process there can be multiple outcomes and so instead of thinking about uncertainity of one event let's think about average uncertainity.\n",
    "\n",
    "$$\n",
    "H = - \\sum_{i=1}^{n} p_i \\log(p_i), \\quad \\text{where } p_i \\text{ is the probability of the $i$-th event}\n",
    "$$\n",
    "\n",
    "The reason for log being base 2 because we are dealing in binary signals either 1 or 0.\n",
    "\n",
    "Hence -log(pi) is the suprise of seeing the outcome of the ith event \n",
    "\n",
    "If this was a random variable X with Probability mass function as P where P(X) is pmf evaluated at X then Entropy can be written as\n",
    "\n",
    "$$\n",
    "H(X) = E [- \\log(P(X))]\n",
    "$$\n",
    "\n",
    "E is expectation i.e weighted sum of all outcomes. One thing to note here the entropy is calculated here for independent events.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Joint Entropy\n",
    "\n",
    "### (a) Independent Random Variables\n",
    "\n",
    "If \\(X\\) and \\(Y\\) are independent:\n",
    "\n",
    "$$\n",
    "P(X=x, Y=y) = P(X=x) \\cdot P(Y=y)\n",
    "$$\n",
    "\n",
    "The joint entropy is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(X,Y) &= - \\sum_x \\sum_y P(X=x, Y=y) \\log P(X=x, Y=y) \\\\\n",
    "       &= - \\sum_x \\sum_y P(X=x) P(Y=y) \\log \\big(P(X=x) P(Y=y)\\big) \\\\\n",
    "       &= - \\sum_x \\sum_y P(X=x) P(Y=y) (\\log \\big(P(X=x)) +\\log \\big(P(Y=y)\\big)) \\\\\n",
    "       &= - \\sum_x  P(X=x) \\log \\big(P(X=x)) - \\sum_y P(Y=y)  \\log \\big(P(Y=y)\\big) \\\\\n",
    "\n",
    "       &= H(X) + H(Y)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    " Dependent Random Variables\n",
    "\n",
    "If \\(X\\) and \\(Y\\) are dependent:\n",
    "\n",
    "$$\n",
    "P(X=x, Y=y) \\neq P(X=x) \\cdot P(Y=y)\n",
    "$$\n",
    "\n",
    "The joint entropy is:\n",
    "\n",
    "$$\n",
    "H(X,Y) = - \\sum_x \\sum_y P(X=x, Y=y) \\log P(X=x, Y=y)\n",
    "$$\n",
    "\n",
    "But it **cannot** be split as \\(H(X)+H(Y)\\). Instead, we use **conditional entropy**:\n",
    "\n",
    "$$\n",
    "H(X,Y) = H(X) + H(Y|X)\n",
    "$$\n",
    "\n",
    "Where conditional entropy is:\n",
    "\n",
    "$$\n",
    "H(Y|X) = - \\sum_x \\sum_y P(X=x, Y=y) \\log P(Y=y | X=x)\n",
    "$$\n",
    "\n",
    "- If \\(X\\) and \\(Y\\) are independent: \\(H(Y|X) = H(Y)\\) → reduces to additive case.\n",
    "\n",
    "\n",
    "Intuition\n",
    "\n",
    "Independent: Knowing X tells nothing about  Y → total uncertainty is sum.\n",
    "\n",
    "Dependent: Knowing X reduces uncertainty about Y → total uncertainty is less than the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional Entropy\n",
    "![](images/conditional_1.png)\n",
    "\n",
    "Proof of joint Entropy Formulae\n",
    "![](images/conditional_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual Information\n",
    "\n",
    "Relative Entropy / KL Divergence\n",
    "\n",
    "Cross Entropy"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
