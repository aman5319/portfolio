<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Aman Pandey">
<meta name="dcterms.date" content="2021-08-16">
<meta name="description" content="Object Detection - SSD | YOLO | R-CNN | Fast R-CNN | Faster R-CNN">

<title>Aman’s Blog - Object Detection</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../posts/Machine Learning/Attention.html" rel="next">
<link href="../../posts/Computer Vision/convolution.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Aman’s Blog - Object Detection">
<meta property="og:description" content="Object Detection - SSD | YOLO | R-CNN | Fast R-CNN | Faster R-CNN">
<meta property="og:image" content="https://aman5319.github.io/portfolio/posts/Computer Vision/images/intro.jpeg">
<meta property="og:site-name" content="Aman's Blog">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Aman’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target=""><i class="bi bi-file-person" role="img">
</i> 
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://drive.google.com/drive/folders/1pTPR0vPX1UoQYfyvQLu8Pn-Wc6hqrjGf?usp=sharing" rel="" target="_blank"><i class="bi bi-file-pdf" role="img">
</i> 
 <span class="menu-text">Resume</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/aman5319" rel="" target="_blank"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/aman5319/" rel="" target="_blank"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Posts</li><li class="breadcrumb-item"><a href="../../posts/Computer Vision/convolution.html">Computer Vision</a></li><li class="breadcrumb-item"><a href="../../posts/Computer Vision/object_detection.html">Object Detection</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Object Detection</h1>
                  <div>
        <div class="description">
          Object Detection - SSD | YOLO | R-CNN | Fast R-CNN | Faster R-CNN
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">deep-learning</div>
                <div class="quarto-category">vision</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Aman Pandey </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 16, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Posts</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Computer Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Computer Vision/convolution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolution and Common architectures</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Computer Vision/object_detection.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
 <span class="menu-text">Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Machine Learning/Attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Different types of Attentions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Machine Learning/data_model_manage_prod.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data and Model Management | Model Monitoring and Logging.</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Machine Learning/metrics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Metrics for Machine Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Machine Learning/Regularization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Improve Model’s Prediction power using Regularization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Machine Learning/Weight_Initializer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Weight Intialization</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
 <span class="menu-text">NLP</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/NLP/embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Word Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/NLP/linguistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linguistics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/NLP/nlp_overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">NLP Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/NLP/rnn_lstm_gru.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">RNN Models</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
 <span class="menu-text">Optimization</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Optimization/optimisers_in_dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Optimizers in - Deep Learning</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false">
 <span class="menu-text">Python</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Python/big_data_processing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Big Data Processing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Python/functional_programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Functional Programming capabilites of python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Python/parallel_processing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Parallelism and Concurrency</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Python/profiling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Memory and Time Profiling</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#computer-vision---introduction" id="toc-computer-vision---introduction" class="nav-link active" data-scroll-target="#computer-vision---introduction">Computer Vision - Introduction</a>
  <ul class="collapse">
  <li><a href="#general-object-detection-framework" id="toc-general-object-detection-framework" class="nav-link" data-scroll-target="#general-object-detection-framework">General object detection framework</a></li>
  <li><a href="#concepts" id="toc-concepts" class="nav-link" data-scroll-target="#concepts">Concepts</a>
  <ul class="collapse">
  <li><a href="#bounding-box-representation" id="toc-bounding-box-representation" class="nav-link" data-scroll-target="#bounding-box-representation">Bounding Box Representation</a></li>
  <li><a href="#iou-jaccard-index" id="toc-iou-jaccard-index" class="nav-link" data-scroll-target="#iou-jaccard-index">IOU (Jaccard Index)</a></li>
  <li><a href="#evaluation-metric" id="toc-evaluation-metric" class="nav-link" data-scroll-target="#evaluation-metric">Evaluation Metric</a></li>
  </ul></li>
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset">Dataset</a></li>
  <li><a href="#one-stage-detector" id="toc-one-stage-detector" class="nav-link" data-scroll-target="#one-stage-detector">One Stage Detector</a>
  <ul class="collapse">
  <li><a href="#yolo" id="toc-yolo" class="nav-link" data-scroll-target="#yolo">YOLO</a></li>
  <li><a href="#ssd" id="toc-ssd" class="nav-link" data-scroll-target="#ssd">SSD</a></li>
  </ul></li>
  <li><a href="#two-stage-detectors" id="toc-two-stage-detectors" class="nav-link" data-scroll-target="#two-stage-detectors">Two Stage Detectors</a>
  <ul class="collapse">
  <li><a href="#r-cnn" id="toc-r-cnn" class="nav-link" data-scroll-target="#r-cnn">R-CNN</a></li>
  <li><a href="#fast-r-cnn" id="toc-fast-r-cnn" class="nav-link" data-scroll-target="#fast-r-cnn">Fast R-CNN</a></li>
  <li><a href="#faster-r-cnn" id="toc-faster-r-cnn" class="nav-link" data-scroll-target="#faster-r-cnn">Faster R-CNN</a></li>
  </ul></li>
  <li><a href="#understanding-basic-faster-r-cnn-architecture" id="toc-understanding-basic-faster-r-cnn-architecture" class="nav-link" data-scroll-target="#understanding-basic-faster-r-cnn-architecture">Understanding Basic Faster R-CNN architecture</a></li>
  <li><a href="#comparison" id="toc-comparison" class="nav-link" data-scroll-target="#comparison">Comparison</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  <li><a href="#thank-you" id="toc-thank-you" class="nav-link" data-scroll-target="#thank-you">Thank you</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/aman5319/portfolio/edit/main/posts/Computer Vision/object_detection.md" class="toc-action">Edit this page</a></p><p><a href="https://github.com/aman5319/portfolio/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="computer-vision---introduction" class="level1">
<h1>Computer Vision - Introduction</h1>
<p><strong>1. Object Classification</strong>- Tells you what the “main subject” of the image is</p>
<p><strong>2. Object Localization</strong>- Predict and draw bounding boxes around on object in an image</p>
<p><strong>3. Object Detection</strong>- Find multiple objects, classify them, and locate where they are in the image.</p>
<p><img src="images/intro.jpeg" class="img-fluid"></p>
<p><strong>Why it is difficult</strong></p>
<ol type="1">
<li>Can have varying number of objects in an image and we do not know ahead of time how many we would expect in an image.</li>
<li>Choosing a right crop is not a trivial task as we may encounter any number of images which :
<ol type="1">
<li>can be at any place.</li>
<li>can be of any aspect ratio.</li>
<li>can be of any size.</li>
</ol></li>
</ol>
<section id="general-object-detection-framework" class="level2">
<h2 class="anchored" data-anchor-id="general-object-detection-framework">General object detection framework</h2>
<p>Typically, there are three steps in an object detection framework.</p>
<ol type="1">
<li><strong>Object localization component</strong> A model or algorithm is used to generate regions of interest or region proposals. These region proposals are a large set of bounding boxes spanning the full image.</li>
</ol>
<p>Some of the famous approaches:</p>
<ul>
<li><strong>Selective Search</strong> - A clustering based approach which attempts to group pixels and generate proposals based on the generated clusters.</li>
<li><strong>Region Proposal</strong> using Deep Learning Model (Features extracted from the image to generate regions) - Based on the features from a deep learning model</li>
<li><strong>Brute Force</strong> - Similar to a sliding window that is applied to the image, over several ratios and scales. These regions are generated automatically, without taking into account the image features.</li>
</ul>
<p><img src="images/anchor_boxes.png" class="img-fluid"></p>
<ol start="2" type="1">
<li><strong>Object classification component</strong> In the second step, visual features are extracted for each of the bounding boxes, they are evaluated and it is determined whether and which objects are present in the proposals based on visual features.</li>
</ol>
<p><strong>Some of the famous approaches:</strong></p>
<ul>
<li>Use pre-trained image classification models to extract visual features</li>
<li>Traditional Computer Vision (filter based approached, histogram methods, etc.)</li>
</ul>
<ol start="3" type="1">
<li><strong>Non maximum suppression</strong> In the final post-processing step, reduce the number of detections in a frame to the actual number of objects present to make sure overlapping boxes are combined into a single bounding box. NMS techniques are typically standard across the different detection frameworks, but it is an important step that might require hyper-parameter tweaking based on the scenario.</li>
</ol>
<p>Predicted <img src="images/NMS_1.svg" class="img-fluid"></p>
<p>Desired <img src="images/NMS_2.svg" class="img-fluid"></p>
</section>
<section id="concepts" class="level2">
<h2 class="anchored" data-anchor-id="concepts">Concepts</h2>
<section id="bounding-box-representation" class="level3">
<h3 class="anchored" data-anchor-id="bounding-box-representation">Bounding Box Representation</h3>
<p>Bounding box is represented using : x_min , y_min , x_max , y_max</p>
<pre><code>x_min: The x-coordinate of the top-left corner of the bounding box.
y_min: The y-coordinate of the top-left corner of the bounding box.
x_max: The x-coordinate of the bottom-right corner of the bounding box.
y_max: The y-coordinate of the bottom-right corner of the bounding box.</code></pre>
<p><img src="images/bounding_1.PNG" class="img-fluid"></p>
<p>But pixel values are next to useless if we don't know the actual dimensions of the image. A better way would be to represent all coordinates is in their fractional form.</p>
<p><img src="images/bounding_2.PNG" class="img-fluid"></p>
<ol type="1">
<li>From boundary coordinates to centre size coordinates x_min, y_min, x_max, y_max -&gt; c_x,c_y,w,h</li>
<li>From centre size coordinates to bounding box coordinates c_x , c_y ,w , h -&gt; x_min, y_min, x_max, y_max</li>
</ol>
<p>Using normalized coordinates (coordinates scaled between 0 and 1) is a common practice in object detection to make bounding box representations independent of image dimensions.</p>
<p><img src="images/bounding_3.PNG" class="img-fluid"></p>
</section>
<section id="iou-jaccard-index" class="level3">
<h3 class="anchored" data-anchor-id="iou-jaccard-index">IOU (Jaccard Index)</h3>
<p>How well the one box matches the the other box we can compute the IOU (or intersection-over-union, also known as the Jaccard index) between the two bounding boxes.</p>
<p>Steps to Calculate:</p>
<ol type="1">
<li>Define the intersection area as the area where the predicted and ground truth bounding boxes overlap.</li>
<li>Define the union area as the total area covered by both the predicted and ground truth bounding boxes.</li>
<li>The Jaccard Overlap, which is IOU, is calculated by dividing the intersection by the union. Jaccard Overlap = Intersection / Union</li>
</ol>
<p><img src="images/IOU.png" class="img-fluid"></p>
</section>
<section id="evaluation-metric" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-metric">Evaluation Metric</h3>
<ul>
<li>Mean Average Precision (mAP or <a href="mailto:mAP@0.5" class="email">mAP@0.5</a> or <a href="mailto:mAP@0.25" class="email">mAP@0.25</a>) -
<ul>
<li>It is a number from 0 to 100 and higher values are typically better</li>
</ul></li>
</ul>
<p><code>Precision</code> measures the accuracy of predictions. It tells you what percentage of the predicted objects are correct. In object detection, a high precision indicates that the model’s detections are mostly accurate.</p>
<p><code>Recall</code>, on the other hand, measures how well the model finds all the actual objects in the image. A high recall means that the model can detect most of the objects present in the image.</p>
<p><img src="images/prec_rec.PNG" class="img-fluid"></p>
<p>However the standard metric of Precision or Recall used in image classification problems cannot be directly applied here because we want both the classification and localization of a model need to be evaluated.This is where mAP(Mean Average-Precision) is comes into the picture.</p>
<p>For calculating Precision and Recall we need:</p>
<p><strong>True Positives (TP)</strong>: These are the correct detections made by the model, where the predicted bounding box significantly overlaps with the ground truth box.</p>
<p><strong>False Positives (FP)</strong>: These are incorrect detections, where the model predicts an object, but it doesn’t overlap significantly with the ground truth box.</p>
<p><strong>False Negatives (FN)</strong>: These are the objects that the model misses; it fails to detect them.</p>
<p>Let us see how can we calculate them in the context of Object Detection</p>
<p><strong>True Positive and False Positive</strong> Using <strong>IOU</strong> we can determine if the detection(a Positive) is correct(True) or not(False). Considering a threshold of 0.5 for IOU and 0.5 for confidence score So any score &gt;=0.5 and IOU &gt;=0.5 - True Positive any score &gt;=0.5 and IOU &lt; 0.5 - False Positive.</p>
<p>Using above information we can calculate:</p>
<ol type="1">
<li>Precision for each class = TP/(TP+FP)</li>
<li>Recall for each class = TP/(TP+FN)</li>
</ol>
<p>But the value of Precision and Recall is very much dependent on threshold assigned to Confidence score and IOU.</p>
<ul>
<li>For IOU, either we can decide a fixed threshold like in VOC Dataset or calculate in the range (say 0.5 to .95) in the case of COCO Dataset</li>
<li>The confidence factor varies across models, 50% confidence in my model design might probably be equivalent to an 80% confidence in someone else’s model design, which would vary the precision recall curve shape.</li>
</ul>
<p>To overcome that problem we go for ... <strong>Average Precision</strong> Area under the precision-recall curve (PR curve)</p>
<p>Let us take an oversimplified example where we just have 5 image in which we have 5 object of the same class. We consider 10 predictions and if IOU&gt;=0.5 we call it a correct prediction.</p>
<p><img src="images/prec_rec_table.PNG" class="img-fluid"></p>
<p>At rank #3 Precision is the proportion of TP = 2/3 = 0.67.</p>
<p>Recall is the proportion of TP out of the possible positives = 2/5 = 0.4. If we plot it <img src="images/prec_rec_plot.PNG" class="img-fluid"></p>
<p>Things to note: Precision will have a zig zag pattern because it goes down with false positives and goes up again with true positives.</p>
<p>A good classifier will be good at ranking correct images near the top of the list, and be able to retrieve a lot of correct images before retrieving any incorrect: its precision will stay high as recall increases. A poor classifier will have to take a large hit in precision to get higher recall.</p>
<p>Average Precision(AP) = <img src="images/ap_formula_raw.PNG" class="img-fluid"></p>
<p>Interpolated Average Precision (IAP) <a href="http://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf">Reference from the paper</a></p>
<p><strong><em>To smooth out the zigzag pattern in the Precision Recall Curve caused by small variations in the ranking of examples. Graphically, at each recall level, we replace each precision value with the maximum precision value to the right of that recall level.</em></strong></p>
<p><img src="images/interpolated_ap_formula.PNG" class="img-fluid"></p>
<p><img src="images/ap_formula.jpeg" class="img-fluid"></p>
<p>Further, there are variations on where to take the samples when computing the interpolated average precision. Some take samples at a fixed 11 points from 0 to 1: {0, 0.1, 0.2, …, 0.9, 1.0}. This is called the 11-point interpolated average precision. Others sample at every k where the recall changes(Area under the Curve)</p>
<p><strong>Interpolated Average Precision</strong></p>
<p><img src="images/ap.PNG" class="img-fluid"></p>
<p><img src="images/interpolated_ap.PNG" class="img-fluid"></p>
<p>we divide the recall value from 0 to 1.0 into 11 points — 0, 0.1, 0.2, …, 0.9 and 1.0. Next, we compute the average of maximum precision value for these 11 recall values.</p>
<p>In our example, AP = (5 × 1.0 + 4 × 0.57 + 2 × 0.5)/11</p>
<p>Issues with Interpolated AP:</p>
<ol type="1">
<li>It is less precise due to interpolation.</li>
<li>it lost the capability in measuring the difference for methods with low AP. Therefore, a new AP calculation is introduced after 2008 for PASCAL VOC.</li>
</ol>
<p><strong>AP (Area under curve AUC)</strong> By interpolating all points, the Average Precision (AP) can be interpreted as an approximated AUC of the Precision - Recall curve. The intention is to reduce the impact of the wiggles in the curve.</p>
<p>Instead of sampling at fixed values, sample the curve at all unique recall values (r₁, r₂, …), whenever the maximum precision value drops. With this change, we are measuring the exact area under the precision-recall curve after the zigzags are removed.Hence No approximation or interpolation is needed</p>
<p><img src="images/auc_plot.PNG" class="img-fluid"></p>
<p><img src="images/auc_formula.PNG" class="img-fluid"></p>
</section>
</section>
<section id="dataset" class="level2">
<h2 class="anchored" data-anchor-id="dataset">Dataset</h2>
<ol type="1">
<li><p><strong>COCO (Common Objects in Context)</strong>:</p>
<ul>
<li>COCO is one of the most widely used object detection datasets.</li>
<li>It contains a diverse set of images with 80 object categories.</li>
<li>The dataset includes over 200,000 labeled images for object detection and segmentation tasks.</li>
<li>Annotations include object bounding boxes, object category labels, and segmentation masks.</li>
<li>COCO also provides a set of evaluation metrics, making it suitable for benchmarking and comparing object detection algorithms.</li>
</ul></li>
<li><p><strong>PASCAL VOC (Visual Object Classes)</strong>:</p>
<ul>
<li>The PASCAL VOC dataset is a classic benchmark in computer vision.</li>
<li>It consists of 20 object categories, and the dataset is divided into train, validation, and test sets.</li>
<li>The annotations include bounding boxes and object category labels.</li>
<li>PASCAL VOC has been widely used for object detection, classification, and segmentation tasks.</li>
</ul></li>
<li><p><strong>ImageNet Object Detection Challenge</strong>:</p>
<ul>
<li>ImageNet is primarily known for its large-scale image classification dataset, but it also hosts object detection challenges.</li>
<li>The dataset contains thousands of object categories and millions of images.</li>
<li>It includes bounding box annotations for object detection tasks.</li>
</ul></li>
<li><p><strong>Open Images Dataset</strong>:</p>
<ul>
<li>Open Images is a dataset with millions of images and thousands of object categories.</li>
<li>It provides annotations for object detection, image classification, and visual relationship detection.</li>
<li>The dataset is diverse and covers a wide range of everyday objects and scenes.</li>
</ul></li>
<li><p><strong>KITTI Dataset</strong>:</p>
<ul>
<li>The KITTI dataset is focused on autonomous driving and contains images and LiDAR data.</li>
<li>It includes annotations for various tasks, including object detection, object tracking, and road segmentation.</li>
<li>Object detection annotations in KITTI cover categories such as cars, pedestrians, and cyclists.</li>
</ul></li>
<li><p><strong>YouTube-BoundingBoxes Dataset</strong>:</p>
<ul>
<li>This dataset contains object detection annotations for video frames extracted from YouTube videos.</li>
<li>It includes various object categories and is suitable for object detection in video content.</li>
</ul></li>
</ol>
<p><img src="images/object_detection_map.png" class="img-fluid"></p>
</section>
<section id="one-stage-detector" class="level2">
<h2 class="anchored" data-anchor-id="one-stage-detector">One Stage Detector</h2>
<section id="yolo" class="level3">
<h3 class="anchored" data-anchor-id="yolo">YOLO</h3>
<p>Yolo paper did a very simplifies a job. So what do want to predict</p>
<ol type="1">
<li>Bounding Boxes - 4 parameters</li>
<li>For each bounding box what is the object inside</li>
<li>(Optional) Confidence score for those boxes</li>
</ol>
<section id="architecture-of-yolo" class="level4">
<h4 class="anchored" data-anchor-id="architecture-of-yolo">architecture of YOLO</h4>
<p><img src="images/yolo_arch.png" class="img-fluid"></p>
<p>As we can see we have an input image of size 448*448 we do all convolution operation and convert it 7*7. Now we have got 49 grids. we predict the bounding box co-ordinate from these grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object.</p>
<p><img src="images/yolo_grid.png" class="img-fluid"></p>
</section>
<section id="yolo-loss-function-has-3-individual-loss-function." class="level4">
<h4 class="anchored" data-anchor-id="yolo-loss-function-has-3-individual-loss-function.">Yolo loss function has 3 individual loss function.</h4>
<p>Localization Loss (Box Coordinates Loss): This loss measures the error in predicting the coordinates of the bounding boxes. YOLO predicts the center coordinates (x, y), width (w), and height (h) of the bounding boxes. The loss is typically computed using Mean Squared Error (MSE) between the predicted box coordinates and the ground truth box coordinates for the object. The localization loss is usually represented as:</p>
<p>Localization Loss = λ_coord * ∑[over all grid cells] ∑[over all bounding boxes for the cell] [(x - x_true)^2 + (y - y_true)^2 + (sqrt(w) - sqrt(w_true))^2 + (sqrt(h) - sqrt(h_true))^2]</p>
<p>Here, λ_coord is a weight to balance the importance of the localization loss.</p>
<p>Confidence Loss (Objectness Loss): This loss measures the confidence of the model in predicting whether an object exists within a grid cell and how well the predicted bounding box overlaps with the ground truth box. It is computed using the binary cross-entropy loss. The confidence loss is typically represented as:</p>
<p>Confidence Loss = ∑[over all grid cells] ∑[over all bounding boxes for the cell] [I_obj * (C - C_true)^2 + I_noobj * (C - C_true)^2]</p>
<p>Here, I_obj is an indicator function that is 1 if an object exists in the cell and 0 otherwise. I_noobj is an indicator function that is 1 if no object exists in the cell and 0 otherwise. C is the predicted confidence score, and C_true is the ground truth confidence score.</p>
<p>Class Loss: This loss measures the error in predicting the class of the object present in each grid cell. YOLO typically uses categorical cross-entropy loss to compute the class loss. It is represented as:</p>
<p>Class Loss = λ_class * ∑[over all grid cells] ∑[over all bounding boxes for the cell] [I_obj * ∑(C_i - C_i_true)^2]</p>
<p>Here, λ_class is a weight to balance the importance of the class loss. C_i is the predicted class probability for class i, and C_i_true is the ground truth class label.</p>
<p>The total YOLO loss is the sum of the localization loss, confidence loss, and class loss:</p>
<p>Total Loss = Localization Loss + Confidence Loss + Class Loss</p>
</section>
<section id="limitation-of-yolo-is" class="level4">
<h4 class="anchored" data-anchor-id="limitation-of-yolo-is">Limitation of YOLO is</h4>
<p>YOLO imposes strong spatial constraints on bounding box predictions since each grid cell only predicts two boxes and can only have one class. This spatial constraint limits the number of nearby objects that our model can predict.</p>
<p>Model struggles with small objects that appear in groups, such as flocks of birds. Since model learns to predict bounding boxes from data, it struggles to generalize to objects in new or unusual aspect ratios or configurations. Our model also uses relatively coarse features for predicting bounding boxes since the architecture has multiple downsampling layers from the input image.</p>
</section>
</section>
<section id="ssd" class="level3">
<h3 class="anchored" data-anchor-id="ssd">SSD</h3>
<p>What better SSD does, it fixes the limitation of yolo by making detection at multiple scale. So rather than using final feature map to generate bounding boxes, it samples from different layers in between the architecture and generate recommendation</p>
<p>To achieve high detection accuracy they produce predictions of different scales from feature maps of different scales, and explicitly separate predictions by aspect ratio.</p>
<p>These design features lead to simple end-to-end training and high accuracy, even on low resolution input images, further improving the speed vs accuracy trade-off.</p>
<section id="architecture" class="level4">
<h4 class="anchored" data-anchor-id="architecture">Architecture</h4>
<p><img src="images/ssd_arch.png" class="img-fluid"></p>
<p><img src="images/ssd_gt.png" class="img-fluid"></p>
<p>Default Boxes (or Prior Boxes):</p>
<p>Default boxes, often referred to as “prior boxes,” are a set of pre-defined bounding boxes with specific sizes and aspect ratios that are placed at various positions on the feature maps generated by different layers of the convolutional neural network (CNN). These default boxes are used to predict the locations and sizes of objects in the input image. The network learns to adjust these default boxes during training to better match the actual objects in the image. By having default boxes at multiple scales and aspect ratios, SSD is capable of detecting objects of different sizes and proportions efficiently.</p>
<p><img src="images/ssd_aspect.png" class="img-fluid"></p>
</section>
<section id="objective-function-of-ssd" class="level4">
<h4 class="anchored" data-anchor-id="objective-function-of-ssd">Objective Function of SSD</h4>
<p>The primary goal of the objective function (or loss function) in SSD is to train the model to make accurate predictions about object locations and classes in an input image. The objective function is a combination of localization loss and confidence loss:</p>
<ol type="1">
<li><strong>Localization Loss</strong>: This component of the objective function measures how accurately the model predicts the locations (coordinates) of objects in the image. It’s typically calculated using metrics like Smooth L1 loss. The localization loss is minimized when the model’s predicted bounding box coordinates are close to the ground-truth coordinates of the objects.</li>
<li><strong>Confidence Loss</strong>: The confidence loss is concerned with how well the model classifies objects and background regions. It involves predicting the objectness score for each default box (prior box) to determine whether it contains an object or not. Cross-entropy loss is commonly used for this purpose. The confidence loss is reduced when the model assigns higher scores to true positive predictions and lower scores to false positives.</li>
</ol>
<p><strong>Hard Negative Mining and NMS in SSD:</strong></p>
<p>Now, let’s see how SSD utilizes hard negative mining and Non-Maximum Suppression:</p>
<ol type="1">
<li><p><strong>Hard Negative Mining</strong>:</p>
<ul>
<li>Hard negative mining is used to address the problem of unbalanced datasets, where there are many more background regions than actual objects in the image. This can lead to a biased model that tends to predict most regions as background.</li>
<li>During training, hard negative mining identifies and focuses on challenging background regions that the model has difficulty distinguishing from actual objects. These challenging negatives are typically the false positive predictions with the highest confidence scores.</li>
<li>By emphasizing these difficult-to-classify background regions, the model learns to make more accurate predictions about what constitutes an object and what doesn’t. The challenging negatives help reduce the false negative rate and improve overall accuracy.</li>
</ul></li>
</ol>
<p><img src="images/ssd_objective.png" class="img-fluid"></p>
</section>
</section>
</section>
<section id="two-stage-detectors" class="level2">
<h2 class="anchored" data-anchor-id="two-stage-detectors">Two Stage Detectors</h2>
<section id="r-cnn" class="level3">
<h3 class="anchored" data-anchor-id="r-cnn">R-CNN</h3>
<p><img src="http://image.slidesharecdn.com/lecture29-convolutionalneuralnetworks-visionspring2015-150504114140-conversion-gate02/95/lecture-29-convolutional-neural-networks-computer-vision-spring2015-31-638.jpg?cb=1430740006" class="img-fluid"></p>
<p><img src="https://proxy.duckduckgo.com/iu/?u=https%3A%2F%2Fjhui.github.io%2Fassets%2Frcnn%2Fbound.png&amp;f=1" class="img-fluid"></p>
<p><strong>R-CNN Working</strong></p>
<ol type="1">
<li>Take an input Image</li>
<li>Use Selective Search Algorithm to generate approximately ~2k proposals Selective Search:
<ol type="1">
<li>Generate initial sub-segmentation, we generate many candidate regions</li>
<li>Use greedy algorithm to recursively combine similar regions into larger ones</li>
<li>Use the generated regions to produce the final candidate region proposals`</li>
</ol></li>
<li>Warp all the proposals into a fix size proposals which will be input to the convolutions</li>
<li>Feed Each warped proposals(~2k) into Convolutions Network which gives 4096-dimensional Feature Vector</li>
<li>Each of the feature vector is send to SVMs for Classification of a object with in a region proposal</li>
<li>The Networks all gives 4 values which predicts the offsets of the predicted bounding compared to Ground truth.</li>
</ol>
<p><strong>Pros:</strong></p>
<ol type="1">
<li>It led the foundation for Two Stage Detectors</li>
<li>R-CNN achieves a mean-average precision (mAP) of53.7% on PASCAL VOC 2010.</li>
</ol>
<p><strong>Cons:</strong></p>
<ol type="1">
<li>Selective Search Algorithm and Convolution operation on each proposal makes training, time and memory consuming.</li>
<li>Inference is Extremely slow as it takes around 47 seconds for each test image.</li>
<li>The selective search algorithm is a fixed algorithm. Therefore, no learning is happening at that stage. This could lead to the generation of bad candidate region proposals.</li>
</ol>
</section>
<section id="fast-r-cnn" class="level3">
<h3 class="anchored" data-anchor-id="fast-r-cnn">Fast R-CNN</h3>
<p><img src="https://proxy.duckduckgo.com/iu/?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1600%2F1*E_P1vAEbGT4HNYjqMtIz4g.png&amp;f=1" class="img-fluid"></p>
<p><img src="https://proxy.duckduckgo.com/iu/?u=https%3A%2F%2Fantkillerfarm.github.io%2Fimages%2Farticle%2Ffast_rcnn_p_2.png&amp;f=1" class="img-fluid"></p>
<p>Here In Fast R-CNN few of previous drawbacks of R-CNN are solved</p>
<ol type="1">
<li>Using a Single Convolution Networks into which we pass the entire image which generates a feature map.</li>
<li>The Region of Interest(RoI) generated using selective search algorithm is then projected on the feature map.</li>
<li>The RoI is generated on the scale of original image but the feature map spatial dimension is small in comparison to RoI so, the mapping is done by converting RoI to feature map scale.</li>
<li>Because different size RoIs are cropped from the feature they are feed into <a href="https://deepsense.ai/region-of-interest-pooling-explained/">RoI pool layer</a> which performs a pooling operation and converts RoI into a fixed size feature map.</li>
<li>Pooled feature map are then feed into two separate branches one which does classification and other Regression.</li>
</ol>
<p><strong>For Classification :- Cross Entropy Loss</strong></p>
<p><strong>For Regression :- SmoothL1Loss</strong></p>
<p><a href="https://stats.stackexchange.com/questions/351874/how-to-interpret-smooth-l1-loss">SmoothL1Loss</a> -&gt; It combines both L1 Loss and L2 Loss</p>
<p><img src="images/smooth.png" class="img-fluid"></p>
<p><strong>Pros:</strong> The reason “Fast R-CNN” is faster than R-CNN is because you don’t have to feed 2000 region proposals to the convolutional neural network every time. Instead, the convolution operation is done only once per image and a feature map is generated from it.</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*m2QO_wbUPA05mY2q4v7mjg.png" class="img-fluid"> <strong>Cons:</strong> When you look at the performance of Fast R-CNN during testing time, including region proposals slows down the algorithm significantly when compared to not using region proposals. Therefore, region proposals become bottlenecks in Fast R-CNN algorithm affecting its performance.</p>
</section>
<section id="faster-r-cnn" class="level3">
<h3 class="anchored" data-anchor-id="faster-r-cnn">Faster R-CNN</h3>
<p><img src="https://lilianweng.github.io/lil-log/assets/images/faster-RCNN.png" class="img-fluid"></p>
<p>Both of the above algorithms(R-CNN &amp; Fast R-CNN) uses selective search to find out the region proposals. Selective search is a slow and time-consuming process affecting the performance of the network. Hence In Faster R-CNN the authors of paper introduced Regional Proposal Network (RPN)</p>
<p>The RPN produces two main outputs:</p>
<p>Objectness Score: The Objectness Score predicts whether an object exists within a Region of Interest (RoI). It essentially evaluates whether a particular region contains an object or not. This output comprises 2K predictions, where K represents the number of anchor boxes.</p>
<p>Bounding Boxes: The Bounding Boxes output predicts the coordinates for the bounding boxes. This information is vital for precisely localizing the object within the RoI. This output contains 4K coordinate predictions corresponding to the anchor boxes.</p>
<p>Using these outputs, the RPN generates RoIs, which are regions that are likely to contain objects of interest. These RoIs serve as the basis for object detection. This RPN is trained separately.</p>
<p>Bounding Boxes :- It Predicts bounding boxes with 4K coordinates.</p>
<p>Using RPN we generate RoI.</p>
<p>After RPN gives RoI it's all similar to Fast R-CNN.</p>
<p><img src="images/faster_r_cnn.png" class="img-fluid"></p>
</section>
</section>
<section id="understanding-basic-faster-r-cnn-architecture" class="level2">
<h2 class="anchored" data-anchor-id="understanding-basic-faster-r-cnn-architecture">Understanding Basic Faster R-CNN architecture</h2>
<p><img src="https://raw.githubusercontent.com/chenyuntc/cloud/master/faster-rcnn%E7%9A%84%E5%89%AF%E6%9C%AC%E7%9A%84%E5%89%AF%E6%9C%AC.png" class="img-fluid"></p>
<p>Faster R-CNN Architecture Details</p>
<p>The Faster R-CNN architecture can be broken down into several key components:</p>
<p>Backbone Network (VGG16): Faster R-CNN uses the VGG16 model as the backbone network to extract features from the input image. This network provides a hierarchical representation of the image, which is crucial for object detection.</p>
<p>Proposal Creator: This component creates RoIs using anchor boxes. Anchor boxes are predefined boxes of various sizes and aspect ratios, which the RPN uses to propose regions of interest.</p>
<p>Proposal Target Creator: After generating a set of RoIs, the Proposal Target Creator subsamples the top RoIs and assigns targets to them. This process is essential for efficient training and ensures that the model focuses on the most informative RoIs.</p>
<p>Anchor Target Generator: The Anchor Target Generator is responsible for generating targets for the anchor boxes. These targets are used to calculate the RPN loss and train the RPN network. It plays a critical role in fine-tuning the anchor box predictions.</p>
<p>Loss Calculation: Faster R-CNN calculates four distinct loss values: RPN_reg_loss (Regression loss for RPN), RPN_cls_loss (Classification loss for RPN), RoI_reg_loss (Regression loss for RoIs), and RoI_cls_loss (Classification loss for RoIs). These losses quantify the errors in object localization and classification. To compute the total loss, all four of these losses are added together.</p>
</section>
<section id="comparison" class="level2">
<h2 class="anchored" data-anchor-id="comparison">Comparison</h2>
<p><img src="https://i.ytimg.com/vi/v5bFVbQvFRk/maxresdefault.jpg" class="img-fluid"></p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/</p>
</section>
<section id="thank-you" class="level2">
<h2 class="anchored" data-anchor-id="thank-you">Thank you</h2>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp("https:\/\/aman5319\.github\.io\/portfolio\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
<script src="https://giscus.app/client.js" data-repo="aman5319/portfolio" data-repo-id="R_kgDOJ44pkw" data-category="Q&amp;A" data-category-id="DIC_kwDOJ44pk84CXwVt" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../posts/Computer Vision/convolution.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Convolution and Common architectures</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../posts/Machine Learning/Attention.html" class="pagination-link">
        <span class="nav-page-text">Different types of Attentions</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Blog made with 💜 and <a href="https://quarto.org/">Quarto</a>, by Aman Pandey. License: <a href="https://creativecommons.org/licenses/by-sa/2.0/">CC BY-SA 2.0</a>.</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:amanpandey@mailfence.com">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>



<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>