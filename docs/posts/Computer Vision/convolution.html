<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Aman Pandey">
<meta name="dcterms.date" content="2022-11-08">
<meta name="description" content="Understanding different types of CNN layers and some common architectures">

<title>Aman’s Blog - Convolution and Common architectures</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../posts/Computer Vision/Image_generation.html" rel="next">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Aman’s Blog - Convolution and Common architectures">
<meta property="og:description" content="Understanding different types of CNN layers and some common architectures">
<meta property="og:image" content="https://aman5319.github.io/portfolio/posts/Computer Vision/images/ffnvscnn.PNG">
<meta property="og:site-name" content="Aman's Blog">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Aman’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target=""><i class="bi bi-file-person" role="img">
</i> 
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://drive.google.com/drive/folders/1pTPR0vPX1UoQYfyvQLu8Pn-Wc6hqrjGf?usp=sharing" rel="" target="_blank"><i class="bi bi-file-pdf" role="img">
</i> 
 <span class="menu-text">Resume</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/aman5319" rel="" target="_blank"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/aman5319/" rel="" target="_blank"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Posts</li><li class="breadcrumb-item"><a href="../../posts/Computer Vision/convolution.html">Computer Vision</a></li><li class="breadcrumb-item"><a href="../../posts/Computer Vision/convolution.html">Convolution and Common architectures</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Convolution and Common architectures</h1>
                  <div>
        <div class="description">
          Understanding different types of CNN layers and some common architectures
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">deep-learning</div>
                <div class="quarto-category">convolution</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Aman Pandey </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 8, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Posts</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Computer Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Computer Vision/convolution.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Convolution and Common architectures</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Computer Vision/Image_generation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AUto encoders VAE -&gt; GAN -&gt; Diffusion</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Computer Vision/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
 <span class="menu-text">Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Machine Learning/Attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Different types of Attentions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Machine Learning/data_model_manage_prod.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data and Model Management | Model Monitoring and Logging.</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Machine Learning/different_types_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Different Types Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Machine Learning/experimentation_management.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Parallel Execution of different Configs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Machine Learning/metrics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Metrics for Machine Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Machine Learning/recommender_system.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recommender System</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Machine Learning/Regularization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Improve Model’s Prediction power using Regularization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Machine Learning/Weight_Initializer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Weight Intialization</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
 <span class="menu-text">Math</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Math/entropy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Entropy</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
 <span class="menu-text">NLP</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/NLP/embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Word Embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/NLP/linguistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linguistics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/NLP/nlp_overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">NLP Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/NLP/rnn_lstm_gru.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">RNN Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/NLP/tokenizer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Enriching Word Vectors with Subword Information</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/NLP/vector-search.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vector Search</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false">
 <span class="menu-text">Optimization</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Optimization/optimisers_in_dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Optimizers in - Deep Learning</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="false">
 <span class="menu-text">Python</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Python/big_data_processing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Big Data Processing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Python/c_extensions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">C Extensions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Python/functional_programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Functional Programming capabilites of python</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Python/large_data_processing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Large Data Processing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Python/parallel_processing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Parallelism and Concurrency</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../posts/Python/profiling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Memory and Time Profiling</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#why-cnn-and-not-regular-neural-nets" id="toc-why-cnn-and-not-regular-neural-nets" class="nav-link active" data-scroll-target="#why-cnn-and-not-regular-neural-nets">Why CNN and not Regular Neural Nets</a></li>
  <li><a href="#convolution" id="toc-convolution" class="nav-link" data-scroll-target="#convolution">Convolution</a></li>
  <li><a href="#convolution-over-volume" id="toc-convolution-over-volume" class="nav-link" data-scroll-target="#convolution-over-volume"><strong>Convolution over Volume</strong></a></li>
  <li><a href="#convolution-operation-with-multiple-filters" id="toc-convolution-operation-with-multiple-filters" class="nav-link" data-scroll-target="#convolution-operation-with-multiple-filters">Convolution Operation with Multiple Filters</a></li>
  <li><a href="#general-representation" id="toc-general-representation" class="nav-link" data-scroll-target="#general-representation"><strong>General Representation</strong></a></li>
  <li><a href="#one-convolution-layer" id="toc-one-convolution-layer" class="nav-link" data-scroll-target="#one-convolution-layer">One Convolution layer</a></li>
  <li><a href="#strides" id="toc-strides" class="nav-link" data-scroll-target="#strides">Strides</a></li>
  <li><a href="#padding" id="toc-padding" class="nav-link" data-scroll-target="#padding">Padding</a></li>
  <li><a href="#pooling" id="toc-pooling" class="nav-link" data-scroll-target="#pooling">Pooling</a></li>
  <li><a href="#general-representation-updated" id="toc-general-representation-updated" class="nav-link" data-scroll-target="#general-representation-updated"><strong>General Representation-Updated</strong></a></li>
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples">Examples</a></li>
  <li><a href="#x-1-convolution" id="toc-x-1-convolution" class="nav-link" data-scroll-target="#x-1-convolution">1 x 1 Convolution</a></li>
  <li><a href="#receptive-field" id="toc-receptive-field" class="nav-link" data-scroll-target="#receptive-field">Receptive Field</a></li>
  <li><a href="#things-to-remember" id="toc-things-to-remember" class="nav-link" data-scroll-target="#things-to-remember">Things to remember</a></li>
  <li><a href="#what-is-imagenet" id="toc-what-is-imagenet" class="nav-link" data-scroll-target="#what-is-imagenet">What is ImageNet</a></li>
  <li><a href="#lenet-51998" id="toc-lenet-51998" class="nav-link" data-scroll-target="#lenet-51998">LeNet-5(1998)</a></li>
  <li><a href="#alexnet2012" id="toc-alexnet2012" class="nav-link" data-scroll-target="#alexnet2012">AlexNet(2012)</a></li>
  <li><a href="#zfnet2013" id="toc-zfnet2013" class="nav-link" data-scroll-target="#zfnet2013">ZFNet(2013)</a></li>
  <li><a href="#vggnet2014" id="toc-vggnet2014" class="nav-link" data-scroll-target="#vggnet2014">VggNet(2014)</a></li>
  <li><a href="#inception-network-googlenet2014" id="toc-inception-network-googlenet2014" class="nav-link" data-scroll-target="#inception-network-googlenet2014">Inception Network (GoogleNet)(2014)</a>
  <ul class="collapse">
  <li><a href="#inception-v1" id="toc-inception-v1" class="nav-link" data-scroll-target="#inception-v1">Inception V1</a></li>
  <li><a href="#inception-v2" id="toc-inception-v2" class="nav-link" data-scroll-target="#inception-v2">Inception V2</a></li>
  </ul></li>
  <li><a href="#resnet2015" id="toc-resnet2015" class="nav-link" data-scroll-target="#resnet2015">ResNet(2015)</a></li>
  <li><a href="#resnet-wide" id="toc-resnet-wide" class="nav-link" data-scroll-target="#resnet-wide">ResNet-Wide</a></li>
  <li><a href="#densenet2017" id="toc-densenet2017" class="nav-link" data-scroll-target="#densenet2017">DenseNet(2017)</a></li>
  <li><a href="#mobilenet" id="toc-mobilenet" class="nav-link" data-scroll-target="#mobilenet">MobileNet</a>
  <ul class="collapse">
  <li><a href="#spatial-seperable-convolution" id="toc-spatial-seperable-convolution" class="nav-link" data-scroll-target="#spatial-seperable-convolution">Spatial Seperable Convolution</a></li>
  <li><a href="#depthwise-convolution" id="toc-depthwise-convolution" class="nav-link" data-scroll-target="#depthwise-convolution">Depthwise Convolution</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.dev/aman5319/portfolio/blob/main/posts/Computer Vision/convolution.ipynb" class="toc-action">Edit this page</a></p><p><a href="https://github.com/aman5319/portfolio/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="why-cnn-and-not-regular-neural-nets" class="level1">
<h1>Why CNN and not Regular Neural Nets</h1>
<p><strong>1. Regular Neural Nets don’t scale well to full images</strong></p>
<p>In MNIST dataset,images are only of size 28x28x1 (28 wide, 28 high, 1 color channels), so a single fully-connected neuron in a first hidden layer of a regular Neural Network would have 28x28x1 = 786 weights. This amount still seems manageable,</p>
<p><strong>But what if we move to larger images.</strong></p>
<p>For example, an image of more respectable size, e.g.&nbsp;200x200x3, would lead to neurons that have 200x200x3 = 120,000 weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting.</p>
<p><strong>2.Parameter Sharing</strong> <br> A feature detector that is useful in one part of the image is probably useful in another part of the image.Thus CNN are good in capturing translation invariance.</p>
<p><strong>Sparsity of connections</strong> In each layer,each output value depends only on a small number of inputs.This makes CNN networks easy to train on smaller training datasets and is less prone to overfitting.</p>
<p><strong>2.3D volumes of neurons.</strong> Convolutional Neural Networks take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular, unlike a regular Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth.</p>
<p><img src="images/ffnvscnn.PNG" class="img-fluid"></p>
</section>
<section id="convolution" class="level1">
<h1>Convolution</h1>
<p>In purely mathematical terms, convolution is a function derived from two given functions by integration which expresses how the shape of one is modified by the other.</p>
<p>However we are interested in understanding the actual convolution operation in the context of neural networks.</p>
<p><strong>An intuitive understanding of Convolution</strong> <br> Convolution is an operation done to extract features from the images as these features will be used by the network to learn about a particular image.In the case of a dog image,the feature could be the shape of a nose or the shape of an eye which will help the network later to identify an image as a dog.</p>
<p>Convolution operation is performed with the help of the following three elements:</p>
<p><strong>1.Input Image-</strong> The image to convolve on</p>
<p><strong>2.Feature Detector/Kernel/Filter-</strong> They are the bunch of numbers in a matrix form intended to extract features from an image.They can be 1dimensional ,2-dimensional or 3-dimensional</p>
<p><strong>3.Feature Map/Activation Map-</strong> The resultant of the convolution operation performed between image and feature detector gives a Feature Map.</p>
<p><img src="images/convolution.PNG" class="img-fluid"></p>
<p><img src="https://i1.wp.com/timdettmers.com/wp-content/uploads/2015/03/convolution.png" class="img-fluid"></p>
<p><img src="images/feature1.PNG" class="img-fluid"> <img src="images/feature2.PNG" class="img-fluid"> <img src="images/feature3.PNG" class="img-fluid"></p>
<p><strong>Convolution Operation</strong></p>
<p><img src="https://cdn-images-1.medium.com/max/1200/1*VVvdh-BUKFh2pwDD0kPeRA@2x.gif" class="img-fluid"></p>
<p><strong>Another way to look at it</strong></p>
<p><img src="images/conv3d1.png" class="img-fluid"></p>
<p>Let say we have an input of <span class="math inline">\(6 x 6\)</span> and a filter size <span class="math inline">\(3 x 3\)</span></p>
<p><strong>Feature map is of size <span class="math inline">\(4 x 4\)</span></strong> <img src="images/conv1.PNG" class="img-fluid"></p>
</section>
<section id="convolution-over-volume" class="level1">
<h1><strong>Convolution over Volume</strong></h1>
<p><strong>What if our input image has more than one channel?</strong></p>
<p>Let say we have an input of <span class="math inline">\(6 x 6 x 3\)</span> and a filter size <span class="math inline">\(3 x 3 x 3\)</span></p>
<p><strong>Feature map is of size <span class="math inline">\(4 x 4\)</span></strong></p>
<p><img src="images/conv3d.png" class="img-fluid"></p>
</section>
<section id="convolution-operation-with-multiple-filters" class="level1">
<h1>Convolution Operation with Multiple Filters</h1>
<p>Let say we have an input of <span class="math inline">\(6 x 6 x 3\)</span> and we use two filters size <span class="math inline">\(3 x 3\)</span></p>
<p><strong>Feature map is of size <span class="math inline">\(4 x 4 x 2\)</span></strong></p>
<p><img src="images/conv2.PNG" class="img-fluid"></p>
</section>
<section id="general-representation" class="level1">
<h1><strong>General Representation</strong></h1>
<p><br> <span class="math display">\[Input Image [n(h)*n(w)*n(c)]-Filter-[f(h)*f(w)*n(c)],n(c')--Feature Map--[(n-f+1)*(n-f+1)*n(c')]\]</span></p>
<p><strong><span class="math inline">\(n(h)\)</span></strong>-height of the image</p>
<p><strong><span class="math inline">\(n(w)\)</span></strong>-width of the image</p>
<p><strong><span class="math inline">\(n(c)\)</span></strong>-number of channel in an image</p>
<p><strong><span class="math inline">\(f(h)\)</span></strong>-height of the filter</p>
<p><strong><span class="math inline">\(f(w)\)</span></strong>-width of the filter</p>
<p><strong><span class="math inline">\(f(c')\)</span></strong>-no of the filter</p>
</section>
<section id="one-convolution-layer" class="level1">
<h1>One Convolution layer</h1>
<p><img src="images/convlayer.PNG" class="img-fluid"></p>
</section>
<section id="strides" class="level1">
<h1>Strides</h1>
<p><img src="images/stride.PNG" class="img-fluid"></p>
</section>
<section id="padding" class="level1">
<h1>Padding</h1>
<p><img src="images/padding.PNG" class="img-fluid"></p>
</section>
<section id="pooling" class="level1">
<h1>Pooling</h1>
<p><img src="images/pooling.PNG" class="img-fluid"></p>
<p><strong>Why we do Pooling?</strong></p>
<p>The idea of max pooling is: 1. to reduce the size of representation in such a way that we carry along those features which speaks louder in the image 2. to lower the number of parameters to be computed,speeding the computation 3. to make some of the features that detects significant things a bit more robust.</p>
<p><strong>Analogy that I like to draw</strong> <img src="https://qph.fs.quoracdn.net/main-qimg-0eb448b5633a1ff511ac2956f032f816-c.png" class="img-fluid"></p>
<p>A good analogy to draw here would be to look into the history of shapes of pyramid.</p>
<p>The Greek pyramid is the one without max pooling whereas the Mesopotamian pyramid is with max pooling involved where we are loosing more information but making our network simpler than the other one.</p>
<p><strong>But don’t we end up loosing information by max pooling?</strong></p>
<p>Yes we do but the question we need to ask is how much information we can afford to loose without impacting much on the model prediction.</p>
<p>Perhaps the criteria to choose how often(after how many convolutions) and at what part of the network (at the beginning or at the mid or at the end of the network) to use max pooling depends completely on what this network is being used for.</p>
<p>For eg: 1. Cats vs Dogs 2. Identify the age of a person</p>
</section>
<section id="general-representation-updated" class="level1">
<h1><strong>General Representation-Updated</strong></h1>
<p><br></p>
<p><strong>Including Padding and Stride</strong></p>
<p><span class="math display">\[Input Image [n(h)*n(w)*n(c)]-Filter-[f(h)*f(w)*n(c)],n(c')--Feature Map--[((n-f+2p)/s+1)*((n-f+2p)/s+1)*n(c')]\]</span></p>
<p><strong><span class="math inline">\(n(h)\)</span></strong>-height of the image</p>
<p><strong><span class="math inline">\(n(w)\)</span></strong>-width of the image</p>
<p><strong><span class="math inline">\(n(c)\)</span></strong>-number of channel in an image</p>
<p><strong><span class="math inline">\(f(h)\)</span></strong>-height of the filter</p>
<p><strong><span class="math inline">\(f(w)\)</span></strong>-width of the filter</p>
<p><strong><span class="math inline">\(f(c')\)</span></strong>-no of the filter</p>
<p><img src="images/network.PNG" class="img-fluid"></p>
</section>
<section id="examples" class="level1">
<h1>Examples</h1>
<p><strong>Input volume:</strong> 32x32x3 <br> 10 5x5 filters with stride 1, pad 2 <br> Output volume size: ?</p>
<p>32x32x10</p>
<p><strong>Input volume:</strong> 32x32x3 <br> 10 5x5 filters with stride 1, pad 2 <br></p>
<p>Number of parameters in this layer?</p>
</section>
<section id="x-1-convolution" class="level1">
<h1>1 x 1 Convolution</h1>
<p>At first,the idea of using 1x1 filter seems to not make sense as 1x1 convolution is just multiplying by numbers.We will not be learning any feature here.</p>
<p>But wait… What if we have a layer with dimension 32x32x196,here 196 is the number of channels and we want to do convolution</p>
<p>So 1x1x192 convolution will do the work of dimensionality reduction by looking at each of the 196 different positions and it will do the element wise product and give out one number.Using multiple such filters say 32 will give 32 variations of this number.</p>
<p><img src="images/1x1.gif" class="img-fluid"> <strong>Why do we use 1x1 filter</strong></p>
<ol type="1">
<li><p>1x 1 filter can help in shrinking the number of channels or increasing the number of channels without changing the height and width of the layer.</p></li>
<li><p>It adds nonlinearity in the network which is useful in some of the architectures like Inception network.</p></li>
</ol>
<p>#<img src="https://cdn-images-1.medium.com/max/1600/1*KdLQiGlPWSxYJ-dvYM3tyQ.png" class="img-fluid"></p>
</section>
<section id="receptive-field" class="level1">
<h1>Receptive Field</h1>
<p>The receptive field is defined as the region in the input space that a particular CNN’s feature is looking at (i.e.&nbsp;be affected by). A receptive field of a feature can be described by its center location and its size.</p>
<p><img src="images/receptive_field.png" class="img-fluid"></p>
</section>
<section id="things-to-remember" class="level1">
<h1>Things to remember</h1>
<ol type="1">
<li><p><strong>Filter</strong> will always have the same number of channel as the image.</p></li>
<li><p><strong>Convolving</strong> gives a <strong>2 D feature map</strong> although our image and kernel used are of 3 dimensional</p></li>
<li><p><strong>Padding</strong> -Preserves the feature size</p></li>
<li><p><strong>Pooling Operation</strong>- Reduces the filter size by half</p></li>
</ol>
</section>
<section id="what-is-imagenet" class="level1">
<h1>What is ImageNet</h1>
<p><a href="http://www.image-net.org/">ImageNet</a></p>
<p>ImageNet is formally a project aimed at (manually) labeling and categorizing images into almost 22,000 separate object categories for the purpose of computer vision research.</p>
<p>However, when we hear the term “ImageNet” in the context of deep learning and Convolutional Neural Networks, we are likely referring to the ImageNet Large Scale Visual Recognition Challenge, or ILSVRC for short.</p>
<p>The ImageNet project runs an annual software contest, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), where software programs compete to correctly classify and detect objects and scenes.</p>
<p>The goal of this image classification challenge is to train a model that can correctly classify an input image into 1,000 separate object categories.</p>
<p>Models are trained on ~1.2 million training images with another 50,000 images for validation and 100,000 images for testing.</p>
<p>These 1,000 image categories represent object classes that we encounter in our day-to-day lives, such as species of dogs, cats, various household objects, vehicle types, and much more. You can find the full list of object categories in the ILSVRC challenge</p>
<p>When it comes to image classification, the <strong>ImageNet</strong> challenge is the de facto benchmark for computer vision classification algorithms — and the leaderboard for this challenge has been dominated by Convolutional Neural Networks and deep learning techniques since 2012.</p>
</section>
<section id="lenet-51998" class="level1">
<h1>LeNet-5(1998)</h1>
<p><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">Gradient Based Learning Applied to Document Recognition</a></p>
<ol type="1">
<li>A pioneering 7-level convolutional network by LeCun that classifies digits,</li>
<li>Found its application by several banks to recognise hand-written numbers on checks (cheques)</li>
<li>These numbers were digitized in 32x32 pixel greyscale which acted as an input images.</li>
<li>The ability to process higher resolution images requires larger and more convolutional layers, so this technique is constrained by the availability of computing resources.</li>
</ol>
<p><img src="https://cdn-images-1.medium.com/max/2000/1*1TI1aGBZ4dybR6__DI9dzA.png" class="img-fluid"></p>
</section>
<section id="alexnet2012" class="level1">
<h1>AlexNet(2012)</h1>
<p><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet Classification with Deep Convolutional Networks</a></p>
<ol type="1">
<li>One of the most influential publications in the field by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton that started the revolution of CNN in Computer Vision.This was the first time a model performed so well on a historically difficult ImageNet dataset.</li>
<li>The network consisted 11x11, 5x5,3x3, convolutions and made up of 5 conv layers, max-pooling layers, dropout layers, and 3 fully connected layers.</li>
<li>Used ReLU for the nonlinearity functions (Found to decrease training time as ReLUs are several times faster than the conventional tanh function) and used SGD with momentum for training.</li>
<li>Used data augmentation techniques that consisted of image translations, horizontal reflections, and patch extractions.</li>
<li>Implemented dropout layers in order to combat the problem of overfitting to the training data.</li>
<li>Trained the model using batch stochastic gradient descent, with specific values for momentum and weight decay.</li>
<li>AlexNet was trained for 6 days simultaneously on two Nvidia Geforce GTX 580 GPUs which is the reason for why their network is split into two pipelines.</li>
<li>AlexNet significantly outperformed all the prior competitors and won the challenge by reducing the top-5 error from 26% to 15.3% <img src="images/alexnet.PNG" class="img-fluid"></li>
</ol>
</section>
<section id="zfnet2013" class="level1">
<h1>ZFNet(2013)</h1>
<p><a href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf">Visualizing and Understanding Convolutional Neural Networks</a> <br> This architecture was more of a fine tuning to the previous AlexNet structure by tweaking the hyper-parameters of AlexNet while maintaining the same structure but still developed some very keys ideas about improving performance.Few minor modifications done were the following: 1. AlexNet trained on 15 million images, while ZF Net trained on only 1.3 million images. 2. Instead of using 11x11 sized filters in the first layer (which is what AlexNet implemented), ZF Net used filters of size 7x7 and a decreased stride value. The reasoning behind this modification is that a smaller filter size in the first conv layer helps retain a lot of original pixel information in the input volume. A filtering of size 11x11 proved to be skipping a lot of relevant information, especially as this is the first conv layer. 3. As the network grows, we also see a rise in the number of filters used. 4. Used ReLUs for their activation functions, cross-entropy loss for the error function, and trained using batch stochastic gradient descent. 5. Trained on a GTX 580 GPU for twelve days. 6. Developed a visualization technique named <strong>Deconvolutional Network</strong>, which helps to examine different feature activations and their relation to the input space. Called <strong>deconvnet</strong> because it maps features to pixels (the opposite of what a convolutional layer does). 7. It achieved a top-5 error rate of 14.8% <img src="https://cdn-images-1.medium.com/max/1600/1*bFjBVvUL2Po_p2mKzC4iYQ.png" class="img-fluid"></p>
</section>
<section id="vggnet2014" class="level1">
<h1>VggNet(2014)</h1>
<p><a href="https://arxiv.org/pdf/1409.1556v6.pdf">VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION</a></p>
<p>This architecture is well konwn for <strong>Simplicity and depth</strong>.. VGGNet is very appealing because of its very uniform architecture.They proposed 6 different variations of VggNet however 16 layer with all 3x3 convolution produced the best result.</p>
<p>Few things to note: 1. The use of only 3x3 sized filters is quite different from AlexNet’s 11x11 filters in the first layer and ZF Net’s 7x7 filters. The authors’ reasoning is that the combination of two 3x3 conv layers has an effective receptive field of 5x5. This in turn simulates a larger filter while keeping the benefits of smaller filter sizes. One of the benefits is a decrease in the number of parameters. Also, with two conv layers, we’re able to use two ReLU layers instead of one. 2. 3 conv layers back to back have an effective receptive field of 7x7. 3. As the spatial size of the input volumes at each layer decrease (result of the conv and pool layers), the depth of the volumes increase due to the increased number of filters as you go down the network. 4. Interesting to notice that the number of filters doubles after each maxpool layer. This reinforces the idea of shrinking spatial dimensions, but growing depth. 5. Worked well on both image classification and localization tasks. The authors used a form of localization as regression (see page 10 of the paper for all details). 6. Built model with the Caffe toolbox. 7. Used scale jittering as one data augmentation technique during training. 8. Used ReLU layers after each conv layer and trained with batch gradient descent. 9. Trained on 4 Nvidia Titan Black GPUs for two to three weeks. 10. It achieved a top-5 error rate of 7.3%</p>
<p><img src="https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png" class="img-fluid"></p>
<p><img src="images/standardconvnet.PNG" class="img-fluid"> <strong>In Standard ConvNet, input image goes through multiple convolution and obtain high-level features.</strong></p>
<p>After Inception V1 ,the author proposed a number of upgrades which increased the accuracy and reduced the computational complexity.This lead to many new upgrades resulting in different versions of Inception Network : 1. Inception v2 2. Inception V3</p>
</section>
<section id="inception-network-googlenet2014" class="level1">
<h1>Inception Network (GoogleNet)(2014)</h1>
<p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">Going Deeper with Convolutions</a></p>
<p>Prior to this, most popular CNNs just stacked convolution layers deeper and deeper, hoping to get better performance,however <strong>Inception Network</strong> was one of the first CNN architectures that really strayed from the general approach of simply stacking conv and pooling layers on top of each other in a sequential structure and came up with the <strong>Inception Module</strong>.The Inception network was complex. It used a lot of tricks to push performance; both in terms of speed and accuracy. Its constant evolution lead to the creation of several versions of the network. The popular versions are as follows:</p>
<ol type="1">
<li>Inception v1.</li>
<li>Inception v2 and Inception v3.</li>
<li>Inception v4 and Inception-ResNet. <br></li>
</ol>
<p>Each version is an iterative improvement over the previous one.Let us go ahead and explore them one by one <img src="images/inception.PNG" class="img-fluid"></p>
<section id="inception-v1" class="level2">
<h2 class="anchored" data-anchor-id="inception-v1">Inception V1</h2>
<p><a href="https://arxiv.org/pdf/1409.4842v1.pdf">Inception v1</a></p>
<p><img src="images/dog.PNG" class="img-fluid"> <strong>Problems this network tried to solve:</strong> 1. <strong>What is the right kernel size for convolution</strong> <br> A larger kernel is preferred for information that is distributed more globally, and a smaller kernel is preferred for information that is distributed more locally. <br> <strong>Ans-</strong> Filters with multiple sizes.The network essentially would get a bit “wider” rather than “deeper” <br> <br> 3. <strong>How to stack convolution which can be less computationally expensive</strong> <br> Stacking them naively computationally expensive. <br> <strong>Ans-</strong>Limit the number of input channels by adding an extra 1x1 convolution before the 3x3 and 5x5 convolutions <br> <br> 2. <strong>How to avoid overfitting in a very deep network</strong> <br> Very deep networks are prone to overfitting. It also hard to pass gradient updates through the entire network. <br> <strong>Ans-</strong>Introduce two auxiliary classifiers (The purple boxes in the image). They essentially applied softmax to the outputs of two of the inception modules, and computed an auxiliary loss over the same labels. The total loss function is a weighted sum of the auxiliary loss and the real loss.</p>
<p>The total loss used by the inception net during training. <br> <strong>total_loss = real_loss + 0.3 * aux_loss_1 + 0.3 * aux_loss_2</strong> <br> <br></p>
<p><img src="images/inception_module.PNG" class="img-fluid"></p>
<p><strong>Points to note</strong></p>
<ol type="1">
<li>Used 9 Inception modules in the whole architecture, with over 100 layers in total! Now that is deep…</li>
<li>No use of fully connected layers! They use an average pool instead, to go from a 7x7x1024 volume to a 1x1x1024 volume. This saves a huge number of parameters.</li>
<li>Uses 12x fewer parameters than AlexNet.</li>
<li>Trained on “a few high-end GPUs within a week”.</li>
<li>It achieved a top-5 error rate of 6.67%</li>
</ol>
</section>
<section id="inception-v2" class="level2">
<h2 class="anchored" data-anchor-id="inception-v2">Inception V2</h2>
<p><a href="https://arxiv.org/pdf/1512.00567v3.pdf">Rethinking the Inception Architecture for Computer Vision</a></p>
<p>Upgrades were targeted towards: 1. Reducing representational bottleneck by replacing 5x5 convolution to two 3x3 convolution operations which further improves computational speed <br> The intuition was that, neural networks perform better when convolutions didn’t alter the dimensions of the input drastically. Reducing the dimensions too much may cause loss of information, known as a <strong>“representational bottleneck”</strong> <br> <img src="images/inceptionv2.PNG" class="img-fluid"> 2. Using smart factorization method where they factorize convolutions of filter size nxn to a combination of 1xn and nx1 convolutions. <br> For example, a 3x3 convolution is equivalent to first performing a 1x3 convolution, and then performing a 3x1 convolution on its output. They found this method to be 33% more cheaper than the single 3x3 convolution. <img src="images/inceptionv2_1.PNG" class="img-fluid"></p>
</section>
</section>
<section id="resnet2015" class="level1">
<h1>ResNet(2015)</h1>
<p><a href="https://arxiv.org/pdf/1512.03385.pdf">Deep Residual Learning for Image Recognition</a> <img src="images/resnet_concept.PNG" class="img-fluid"> <strong>In ResNet, identity mapping is proposed to promote the gradient propagation. Element-wise addition is used. It can be viewed as algorithms with a state passed from one ResNet module to another one.</strong></p>
<p><img src="https://cdn-images-1.medium.com/max/720/1*ByrVJspW-TefwlH7OLxNkg.png" class="img-fluid"> <img src="https://cdn-images-1.medium.com/max/720/1*2ns4ota94je5gSVjrpFq3A.png" class="img-fluid"></p>
</section>
<section id="resnet-wide" class="level1">
<h1>ResNet-Wide</h1>
<p><img src="https://cdn-images-1.medium.com/max/960/1*7JzJ1RGh1Y4VoG1M4dseTw.png" class="img-fluid"> left: a building block of [2], right: a building block of ResNeXt with cardinality = 32</p>
</section>
<section id="densenet2017" class="level1">
<h1>DenseNet(2017)</h1>
<p><a href="https://arxiv.org/pdf/1608.06993v3.pdf">Densely Connected Convolutional Networks</a> <br> It is a logical extension to ResNet.</p>
<p><strong>From the paper:</strong> Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion.</p>
<p><strong>DenseNet Architecture</strong> <img src="images/densenet_arc.PNG" class="img-fluid"></p>
<p>Let us explore different componenets of the network <br> <br> <strong>1. Dense Block</strong> <br> Feature map sizes are the same within the dense block so that they can be concatenated together easily. <img src="https://cdn-images-1.medium.com/max/960/1*9ysRPSExk0KvXR0AhNnlAA.gif" class="img-fluid"></p>
<p><strong>In DenseNet, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. Concatenation is used. Each layer is receiving a “collective knowledge” from all preceding layers.</strong> <img src="images/densenet_concept.PNG" class="img-fluid"></p>
<p>Since each layer receives feature maps from all preceding layers, network can be thinner and compact, i.e.&nbsp;number of channels can be fewer. The growth rate k is the additional number of channels for each layer.</p>
<p>The paper proposed different ways to implement DenseNet with/without B/C by adding some variations in the Dense block to further reduce the complexity,size and to bring more compression in the architecture.</p>
<pre><code>1. Dense Block (DenseNet)
    -  Batch Norm (BN)
    -  ReLU
    -  3×3 Convolution 
2. Dense Block(DenseNet B)
    -  Batch Norm (BN)
    -  ReLU
    -  1×1 Convolution
    -  Batch Norm (BN)
    -  ReLU
    -  3×3 Convolution
3. Dense Block(DenseNet C)
    -  If a dense block contains m feature-maps, The transition layer generate $\theta $ output feature maps, where                    $\theta \leq \theata \leq$ is referred to as the compression factor.
    -  $\theta$=0.5 was used in the experiemnt which reduced the number of feature maps by 50%.

4. Dense Block(DenseNet BC)
-  Combination of Densenet B and Densenet C</code></pre>
<p><br> <strong>2. Trasition Layer</strong> <br> The layers between two adjacent blocks are referred to as transition layers where the following operations are done to change feature-map sizes: - 1×1 Convolution - 2×2 Average pooling</p>
<p><strong>Points to Note:</strong> 1. it requires fewer parameters than traditional convolutional networks 2. Traditional convolutional networks with L layers have L connections — one between each layer and its subsequent layer — our network has L(L+1)/ 2 direct connections. 3. Improved flow of information and gradients throughout the network, which makes them easy to train 4. They alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. 5. Concatenating feature maps instead of summing learned by different layers increases variation in the input of subsequent layers and improves efficiency. This constitutes a major difference between DenseNets and ResNets. 6. It achieved a top-5 error rate of 6.66%</p>
</section>
<section id="mobilenet" class="level1">
<h1>MobileNet</h1>
<section id="spatial-seperable-convolution" class="level2">
<h2 class="anchored" data-anchor-id="spatial-seperable-convolution">Spatial Seperable Convolution</h2>
<p><img src="images/separable_convolution.gif" class="img-fluid"></p>
<p><strong>Divides a kernel into two, smaller kernels</strong></p>
<p><img src="https://cdn-images-1.medium.com/max/1440/1*mL53fW0tJpNWEePp54y1Sg.png" class="img-fluid"></p>
<p><strong>Instead of doing one convolution with 9 multiplications(parameters), we do two convolutions with 3 multiplications(parameters) each (6 in total) to achieve the same effect</strong></p>
<p><img src="https://cdn-images-1.medium.com/max/1680/1*o3mKhG3nHS-1dWa_plCeFw.png" class="img-fluid"></p>
<p><strong>With less multiplications, computational complexity goes down, and the network is able to run faster.</strong></p>
<p>This was used in an architecture called <a href="https://arxiv.org/pdf/1801.06434v1.pdf">Effnet</a> showing promising results.</p>
<p>The main issue with the spatial separable convolution is that not all kernels can be “separated” into two, smaller kernels. This becomes particularly bothersome during training, since of all the possible kernels the network could have adopted, it can only end up using one of the tiny portion that can be separated into two smaller kernels.</p>
</section>
<section id="depthwise-convolution" class="level2">
<h2 class="anchored" data-anchor-id="depthwise-convolution">Depthwise Convolution</h2>
<p><img src="images/depthwise.gif" class="img-fluid"></p>
<p>Say we need to increase the number of channels from 16 to 32 using 3x3 kernel. <br></p>
<p><strong>Normal Convolution</strong> <br> Total No of Parameters = 3 x 3 x 16 x 32 = 4608</p>
<p><img src="https://cdn-images-1.medium.com/max/1200/1*VvBTMkVRus6bWOqrK1SlLQ.png" class="img-fluid"></p>
<p><strong>Depthwise Convolution</strong></p>
<ol type="1">
<li>DepthWise Convolution = 16 x [3 x 3 x 1]</li>
<li>PointWise Convolution = 32 x [1 x 1 x 16]</li>
</ol>
<p>Total Number of Parameters = 656</p>
<p><strong>Mobile net uses depthwise seperable convolution to reduce the number of parameters</strong></p>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<p><a href="https://indoml.com/2018/03/07/student-notes-convolutional-neural-networks-cnn-introduction/">Convolution</a> <br> <a href="https://wiseodd.github.io/techblog/2016/07/18/convnet-maxpool-layer/">Max Pool</a> <br> <a href="http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture05.pdf">Standford Slides</a> <br> <a href="http://cs231n.github.io/convolutional-networks/">Standford Blog</a> <br> <a href="https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050">An intuitive guide to Convolutional Neural Networks</a> <br> <a href="https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2">Convolutional Neural Networks</a> <br> <a href="https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148">Understanding of Convolutional Neural Network</a> <br> <a href="https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807">Receptive Feild Calculation</a> <br> <a href="http://timdettmers.com/2015/03/26/convolution-deep-learning/">Understanding Convolution in Deep Learning</a> <br> <a href="http://setosa.io/ev/image-kernels/">Visualize Image Kernel</a> <br> <a href="https://arxiv.org/abs/1311.2901">Visualizing and Understanding Convolution Networks</a> <br> <a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf">Standford CS231n Lecture Notes</a> <br> <a href="https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html">The 9 Deep Learning Papers You Need To Know About</a> <br> <a href="https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5">CNN Architectures</a> <br> <a href="https://arxiv.org/pdf/1608.06037.pdf">Lets Keep It Simple</a> <br> <a href="https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/">CNN Architectures Keras</a> <br> <a href="https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202">Inception Versions</a> <br> <a href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803">DenseNet Review</a> <br> <a href="https://towardsdatascience.com/densenet-2810936aeebb">DenseNet</a> <br> <a href="http://teleported.in/posts/decoding-resnet-architecture/">ResNet</a> <br> <a href="https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035">ResNet Versions</a> <br> <a href="https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728">Depthwise Convolution</a></p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp("https:\/\/aman5319\.github\.io\/portfolio\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
<script src="https://giscus.app/client.js" data-repo="aman5319/portfolio" data-repo-id="R_kgDOJ44pkw" data-category="Q&amp;A" data-category-id="DIC_kwDOJ44pk84CXwVt" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../posts/Computer Vision/Image_generation.html" class="pagination-link">
        <span class="nav-page-text">AUto encoders VAE -&gt; GAN -&gt; Diffusion</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Blog made with 💜 and <a href="https://quarto.org/">Quarto</a>, by Aman Pandey. License: <a href="https://creativecommons.org/licenses/by-sa/2.0/">CC BY-SA 2.0</a>.</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:amanpandey@mailfence.com">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>



<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>