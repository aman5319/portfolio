<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.30">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Aman Pandey">
<meta name="dcterms.date" content="2025-07-10">
<meta name="description" content="Universe’s most universal law.">

<title>Entropy – Aman Pandey</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-3a01e2046221230fdceeea94b1ec5d67.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-a4a11d514c7d463668e07712114998e6.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-2d7b555c8139e1218026ae7dd17bce4a.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="Entropy – Aman Pandey">
<meta property="og:description" content="Universe’s most universal law.">
<meta property="og:image" content="https://aman5319.github.io/portfolio/tech_blogs/Machine Learning/images/entropy_ven.png">
<meta property="og:site_name" content="Aman Pandey">
<meta property="og:image:height" content="918">
<meta property="og:image:width" content="578">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Aman Pandey</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../tech_blogs"> 
<span class="menu-text">Tech Blogs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../non_tech_blogs"> 
<span class="menu-text">Non Tech Blogs</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Entropy</h1>
                  <div>
        <div class="description">
          Universe’s most universal law.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">deep-learning</div>
                <div class="quarto-category">math</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Aman Pandey </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 10, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#entropy" id="toc-entropy" class="nav-link active" data-scroll-target="#entropy">Entropy</a>
  <ul class="collapse">
  <li><a href="#physics" id="toc-physics" class="nav-link" data-scroll-target="#physics">Physics</a></li>
  <li><a href="#more-physics-way" id="toc-more-physics-way" class="nav-link" data-scroll-target="#more-physics-way">More Physics Way</a></li>
  <li><a href="#intuition-and-example" id="toc-intuition-and-example" class="nav-link" data-scroll-target="#intuition-and-example">Intuition and Example:</a></li>
  </ul></li>
  <li><a href="#information-entropy" id="toc-information-entropy" class="nav-link" data-scroll-target="#information-entropy">Information Entropy</a>
  <ul class="collapse">
  <li><a href="#conditional-entropy" id="toc-conditional-entropy" class="nav-link" data-scroll-target="#conditional-entropy">Conditional Entropy</a></li>
  <li><a href="#joint-entropy" id="toc-joint-entropy" class="nav-link" data-scroll-target="#joint-entropy">Joint Entropy</a></li>
  <li><a href="#information-gain" id="toc-information-gain" class="nav-link" data-scroll-target="#information-gain">Information Gain</a></li>
  <li><a href="#kl-divergence" id="toc-kl-divergence" class="nav-link" data-scroll-target="#kl-divergence">KL Divergence</a></li>
  <li><a href="#cross-entropy" id="toc-cross-entropy" class="nav-link" data-scroll-target="#cross-entropy">Cross Entropy</a></li>
  <li><a href="#physics-and-information-theory-relation" id="toc-physics-and-information-theory-relation" class="nav-link" data-scroll-target="#physics-and-information-theory-relation">Physics and Information Theory Relation</a></li>
  </ul></li>
  <li><a href="#why-doesnt-time-flows-backward" id="toc-why-doesnt-time-flows-backward" class="nav-link" data-scroll-target="#why-doesnt-time-flows-backward">Why doesn’t time flows backward?</a></li>
  <li><a href="#why-doesnt-milk-and-coffee-unmix-itself" id="toc-why-doesnt-milk-and-coffee-unmix-itself" class="nav-link" data-scroll-target="#why-doesnt-milk-and-coffee-unmix-itself">Why doesn’t milk and coffee unmix itself?</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/aman5319/portfolio/edit/main/tech_blogs/Machine Learning/entropy.md" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/aman5319/portfolio/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="entropy" class="level1">
<h1>Entropy</h1>
<p>Entropy is the universe’s most universal law, shaping everything from energy flow to information.</p>
<ul>
<li>In mathematics, it’s a formula that measures uncertainty.</li>
<li>In physics, it’s the law that governs how heat and energy spread.</li>
<li>In chemistry, it’s the tally of molecular disorder.</li>
<li>In information theory, it’s the currency of surprise in messages, data compression and transfer over noisy channels.</li>
<li>In machine learning, it’s the score of impurity in decision-making.</li>
</ul>
<p>But are all of these really talking about the same thing?<br> Are they all just using one tool entropy dressed in different clothes?<br> Let’s unpack each of them.</p>
<hr>
<section id="physics" class="level2">
<h2 class="anchored" data-anchor-id="physics">Physics</h2>
<p>The most common way to describe entropy is as disorder, random, more mixed and less ordered.<br>
But on a very fundamental level let’s look it in a way where I want to know how something is distributed in a system.</p>
<p>Here are two important words <strong>system</strong> and <strong>distributed</strong>.</p>
<p><strong>System</strong> - In science, a system is simply the part of the universe you choose to focus on or study.<br>
Everything outside that part is called the surroundings or environment.<br>
For example you want to study about what is happening inside a bag or a kettle or a kitchen.</p>
<p>Types of systems based on interaction with surroundings:</p>
<p><strong>Closed system</strong></p>
<ul>
<li>Exchanges energy (like heat or work) but not matter with its surroundings.</li>
<li>Example: A sealed, insulated container where heat can pass through the walls, but no gas or liquid escapes or enters.</li>
</ul>
<p><strong>Open system</strong></p>
<ul>
<li>Exchanges both energy and matter with its surroundings.</li>
<li>Example: A boiling pot of water without a lid steam (matter) escapes, and heat (energy) flows in/out.</li>
</ul>
<p><strong>Isolated system</strong> - Exchanges neither energy nor matter with its surroundings. - Example: An ideal thermos bottle perfectly insulated so nothing gets in or out (theoretical, as perfect isolation is impossible).</p>
<p><strong>Distributed</strong> - Defining Distribution can be tricky because first we have to define exactly what that thing is we want to measure the distribution and then we can ask whether that thing is distributed uniformly, skewed or follows any particular pattern</p>
<p><strong>For example</strong> - if I treat a living room as an isolated system containing 100 pieces of clothing, I might ask how they’re distributed: neatly folded in one corner or scattered randomly across the room.</p>
<ul>
<li>Suppose a person enters the living room blindfolded, playing a game where they try to pick up a piece of clothing every 5 seconds, repeating this 100 times.<br>
</li>
<li>If the clothes are scattered randomly (high entropy), they’re more likely to pick up a piece because the clothes are spread out.<br>
</li>
<li>If the clothes are neatly folded in one corner (low entropy), they’re less likely to pick one up unless they happen to walk to that corner.</li>
</ul>
<p>Entropy describes how energy, information, heat, clothes, or anything else is distributed within a system. It doesn’t measure the quantity or intensity of the thing, only the likelihood of its arrangement.</p>
<p>Now let’s look at the example of hot and cold metal bars to solidfy the idea of energy distribution aka entropy.</p>
<p>A 300°C iron metal rod is there what are the factors that can affect the energy distribution.</p>
<ol type="1">
<li>Temperature: It defines how particles move. Higher temperature leads to higher kinetic energyatoms vibrate more vigorously. This increases the number of accessible microstates, affecting the distribution and raising entropy.<br>
</li>
<li>Pressure or Volume: These can affect the density of states. For solids like iron, volume changes are small, but in gases, they significantly impact how particles spread out.<br>
</li>
<li>Number of Particles: More particles mean more interactions and jiggling, leading to higher entropy (entropy is extensive).<br>
</li>
<li>Microstates (W): From a statistical view, entropy depends on the number of possible positions, momenta, and quantum states of particles (S = k ln W).<br>
</li>
<li>State of Matter (Solid/Liquid/Gas): Solids have low entropy due to ordered structures; liquids have more disorder; gases have the highest entropy from free particle motion.</li>
</ol>
<p>Now here i have listed few, it can be more, the more things we get to know about a particular system the more accurately we can predict or calculate its entropy in a given framework we are working.</p>
<p>As you can see these parameters don’t act independently; they’re deeply interrelated. Measuring the effect of each on entropy in isolation is complicated because changing one often changes others. Here’s a more precise view:</p>
<p>Why they are interdependent:</p>
<ul>
<li>Temperature T affects how particles move, which influences heat capacity Cp​ because heat capacity often varies with temperature.<br>
</li>
<li>Changing volume VV changes the pressure P (for gases) and the available phase space, which in turn affects energy levels and particle behavior.<br>
</li>
<li>Number of particles N changes density, which impacts how particles interact, affecting heat capacity and accessible microstates W.<br>
</li>
<li>The microstates W depend on all of these combined temperature, volume, particle number, and quantum states collectively define how many microstates are available.</li>
</ul>
<p>How to measure the affect of each parameter considering interdependence:</p>
<ul>
<li>Control variables carefully in experiments or simulations</li>
<li>Change one parameter while keeping others fixed as much as possible (e.g., vary temperature at constant volume and particle number).</li>
<li>Measure entropy change experimentally or compute it via statistical methods.</li>
</ul>
<p>Now let’s suppose we want to calculate the entropy of the iron rod at 300C we have to first define the system in this way.</p>
<p><strong>Step 1: Define the system and assumptions</strong></p>
<ul>
<li>The rod is uniform and homogeneous (same material properties throughout).</li>
<li>Heat transfer is slow enough to assume quasi-static (reversible) heating.</li>
<li>The rod’s mass m, specific heat capacity Cp​(T), are known or measurable.</li>
<li>Ignore volume changes if thermal expansion is negligible (common for solids).</li>
</ul>
<p><strong>Step 2: Define a reference</strong></p>
<ul>
<li>Entropy is always measured relative to a reference state (often at a baseline temperature or zero entropy state).</li>
<li>You don’t just “pick a random state and say the entropy is X” without context.</li>
<li>You measure or calculate the change in entropy ΔS=S_final−S_initial​ when the system moves from one state to another (e.g., energy x at time t1​ to energy y at time t2​).</li>
<li>Absolute entropy values can be tabulated (like standard molar entropy), but these are always relative to a defined zero-point.</li>
</ul>
<p><strong>Step 3: Use physics formulae to calculate one</strong> - If the system was loosing energy after time t2 we would say the entropy of the rod has decreased,why because kinetic energy has reduced, leading to less jiggle, leading to less movement and microstate but surroundings gains that energy, increasing their entropy by a greater amount (because the surroundings are usually at a lower temperature or can spread the heat more effectively).</p>
<ul>
<li>If the system was gaining energy from outside source after time t2 we would say the entropy has increased it has gained that energy from the surrounding.</li>
</ul>
<p>After going through example it will be clear, three things that defines entropy completely -</p>
<ul>
<li>Distribution of what exactly are we measuring (energy, heat, information, clothes)</li>
<li>Variables affecting the distribution (tempeature, pressure, state)</li>
<li>how do we measure the distribution.
<ul>
<li>For example, In a 100 sq, meter room if 100 clothes are thrown at random we can measure the distrbution by counting how many clothes are there in each 1 sq, meter box.</li>
<li>For the heated rod, count the number of microstates an atom can take (e.g., unique vibrational or electronic states).</li>
</ul></li>
</ul>
<p>The above defines the whole of Thermodynamic entropy (physical entropy).<br>
This depends on the actual physical state of the system: energy, temperature, molecular configurations, etc.</p>
<hr>
</section>
<section id="more-physics-way" class="level2">
<h2 class="anchored" data-anchor-id="more-physics-way">More Physics Way</h2>
<p>If you have noticed one thing we were till now measuring the distribution using counting method this is actually Boltzmann Entropy (Simple Microstates Counting)</p>
<p><strong>Boltzmann Entropy (Simple Microstates Counting)</strong></p>
<ul>
<li>Applies to a system in a single macrostate, where all microstates are equally likely.</li>
<li>Entropy is based on counting how many microstates Ω correspond to that macrostate:</li>
<li>Example: Imagine you have 3 coins lying on a table, and you only care about how many coins show heads.
<ul>
<li>Macrostate: Exactly 2 coins are heads.</li>
<li>Microstates: The specific arrangements that have 2 heads and 1 tail. There are 3 such microstates (HTH, HHT, THH).</li>
</ul></li>
<li>Since all these microstates are equally probable, Boltzmann entropy counts these 3 microstates.</li>
</ul>
<p><strong>Gibbs Entropy (Probability Weighted)</strong></p>
<ul>
<li>Applies when the system is in a statistical mixture of microstates, each with a probability pipi​.</li>
<li>Entropy accounts for the uncertainty over which microstate the system is actually in:</li>
<li>Example: Same 3 coins, but now you know the probability of each microstate is different:
<ul>
<li>Suppose probability that the coins are in microstate 1 (HTH) is 0.5,</li>
<li>Microstate 2 (HHT) is 0.3,</li>
<li>Microstate 3 (THH) is 0.2.</li>
</ul></li>
<li>Now entropy measures the uncertainty weighted by these probabilities.</li>
</ul>
<p>There is another one more of way looking at <strong>entropy from Clausius</strong> -</p>
<pre><code>entropy as the unavailability of energy to do useful
work a practical way to understand what entropy means
beyond just counting microstates.</code></pre>
<p>What does this mean?</p>
<p>Energy can exist in many forms: heat, mechanical work, chemical energy, etc.</p>
<p>Not all energy in a system can be converted into useful work. Some energy is “lost” or dispersed in ways that can’t be harnessed to do things like move a piston, run a motor, or power a machine.</p>
<p>Entropy quantifies that “lost” or unavailable portion of energy the part that is spread out or randomized so much that you cannot concentrate or convert it back into work.</p>
<hr>
</section>
<section id="intuition-and-example" class="level2">
<h2 class="anchored" data-anchor-id="intuition-and-example">Intuition and Example:</h2>
<ul>
<li><p>High-quality energy (like work or high-temperature heat) can be fully converted into useful work. i.e a heated iron rod can be used to heat up a water. Entropy is lower.</p></li>
<li><p>Low-quality energy (like heat evenly spread in the environment at room temperature) can’t be fully converted to work. a room temperature rod can’t be used to heat up water hence entropy is higher.</p></li>
<li><p>Entropy measures how much energy has “degraded” from high-quality to low-quality, hence reducing the capacity to do work.</p></li>
</ul>
<p>When energy disperses, it spreads over many particles and states increasing the number of accessible microstates.</p>
<p>The system moves from an ordered, low-entropy state (few microstates, energy concentrated, usable) to a disordered, high-entropy state (many microstates, energy spread out, less usable).</p>
<p>So “lost energy” is the energy tied up in many microscopic configurations that can’t be coordinated to do macroscopic work.</p>
<p>Interesting example Absolutely! Here’s a clear, jargon-free write-up using the sun example that connects entropy across different domains thermodynamics and information step by step, perfect for your blog:</p>
<hr>
<p><strong>Step 1- The Sun as a Hot, Complex System</strong></p>
<p>The sun is incredibly hot and full of activity. Inside it, trillions of tiny particles move in countless ways. Because of all this complexity, scientists say the sun has <em>high entropy</em>. This means there are many possible ways the sun’s particles can be arranged inside it a lot of microscopic disorder.</p>
<hr>
<p><strong>Step 2- Looking at the Sun and Space Together</strong></p>
<p>Now, if we consider not just the sun but the space around it the light and heat it sends out we get a bigger system. The sun transfers energy to space all the time, and this movement adds to the total disorder or entropy of this larger system. hence sun’s entropy is lower but overall system entropy is hight</p>
<hr>
<p><strong>Step 3- What About Predicting the Sun’s State?</strong></p>
<p>Even though the sun is complex inside, from far away, it looks very stable. For example, we know it will be hot tomorrow, just like today. Because of this, <em>our uncertainty</em> about the sun’s overall state is very low. We don’t need much new information to describe what the sun will be like.</p>
<hr>
<p><strong>Step 4- Two Different Kinds of Entropy</strong></p>
<p>Here’s the key: The word “entropy” is used in two related but different ways.</p>
<ul>
<li>One kind of entropy measures <em>how complex or disordered</em> a system is inside like the sun’s many tiny particle arrangements.</li>
<li>The other kind measures <em>how uncertain or surprised</em> we are about what the system will do like how much information we need to describe the sun’s state tomorrow.</li>
</ul>
<hr>
<p><strong>Step 6- The Role of Granularity How Closely You Look Matters</strong></p>
<p>The “overall state” of the sun depends on how closely or in detail you choose to look.</p>
<ul>
<li><p>If you only care about big things like whether the sun is hot or not then the sun’s state seems very simple and predictable, and you need little information to describe it.</p></li>
<li><p>But if you zoom in to tiny details the exact position and energy of every particle inside the sun the state becomes incredibly complex and uncertain, and you’d need a huge amount of information to describe it fully.</p></li>
</ul>
<p>So, the amount of entropy or uncertainty you assign depends on the level of detail you’re considering.</p>
<p>An object with low entropy has less uncertainty about its state but this depends on how detailed or ‘fine-grained’ the description of the state is.</p>
<p>what will happen if we get to know more things the uncertaininty decreases hence the information entropy decreases when information is hidden uncertainty grows, and information entropy increases.</p>
<hr>
</section>
</section>
<section id="information-entropy" class="level1">
<h1>Information Entropy</h1>
<p>A key measure in information theory is entropy. Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process.</p>
<p>For example, consider rain in two locations: the Amazon rainforest and a tropical city. In the Amazon, it rains almost every day, so the outcome is fairly predictable low uncertainty, low entropy. In the tropical city, rain is less predictable, so the uncertainty is higher higher entropy.</p>
<p>A very important question can arise why are we measuring uncertainty and not certainty itself, because if we talk about any process and if something is guranteed it conveys no new information.</p>
<p>Next question is how do we calculate uncertainty in shanon’s paper the way he defines the property of entropy is as following -</p>
<p>Suppose we have a set of possible events whose probabilities of occurrence are (p1; p2; : : : ; pn).</p>
<p>These probabilities are known but that is all we know concerning which event will occur.</p>
<p>If take weather prediction example p1 can be probability of rain, p2 can be probability of strong wind, p3 could be probability of high humidity.</p>
<p>can we measure of how much “choice” is involved in the selection of the event or of how uncertain we are of the outcome?</p>
<p>If there is such a measure, say H( p1; p2; : : : ; pn), it is reasonable to require of it the following properties:</p>
<p><strong>Axiom 1.</strong></p>
<pre><code>- H should be continuous in the p_i.
What it means: 

Continuity here means that small changes in the probabilities of events should lead to small changes in entropy.  
Entropy shouldn’t “jump” suddenly if the probabilities change just a little.  
For a fair coin the probability of Head and Tail is same and entropy is at its maximum H.

Now, slightly bias it: p_h = 0.51 and p_t =0.49</code></pre>
<p>The uncertainty has decreased a tiny bit, because the coin is slightly more predictable now.<br>
Entropy changes slightly it doesn’t suddenly drop to zero or skyrocket.</p>
<p>If H were not continuous, then a tiny change in probability could make the entropy jump wildly, which wouldn’t make sense small changes in our knowledge shouldn’t cause a huge change in measured uncertainty.</p>
<p><strong>Axiom 2.</strong></p>
<pre><code>If all the pi are equal, pi = 1/n, then H should be a monotonic increasing function of n.  
With equally likely events there is more choice, or uncertainty, when there are more possible events.  

What it means: 
    For a fair coin and a fair dice both have equal probability of each event but
    rolling a die has more probable chances means more uncertainty.  
    Even though each outcome is equally likely in both cases, rolling a die is less
    predictable than flipping a coin more possible choices, more uncertainty, higher entropy.</code></pre>
<p><strong>Axiom 3.</strong></p>
<pre><code>If a choice be broken down into two successive choices, the original H should be the
weighted sum of the individual values of H.  
What it means: 
    if we are calculating the probability of an event by breaking it into multiple choices  
    which we do in Law of total probability calculation same way entropy should also add up.</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/tree.png" class="img-fluid figure-img"></p>
<figcaption>image</figcaption>
</figure>
</div>
<p>Hence we are adding up the entropy at each step from start to the end node.</p>
<p>The only function which can do all this <span class="math display">\[ H(i) = - \log(pi) \]</span></p>
<p>Hence entropy is converted in log probabilities, now we are just calculating entropy of one event but in a random process there can be multiple outcomes and so instead of thinking about uncertainity of one event let’s think about average uncertainity.</p>
<p>Note:- Shanon Gave how this formulae satisfies few more properties and how with limit we can get the proof of this. But we are skipping it.</p>
<p><span class="math display">\[
H = - \sum_{i=1}^{n} p_i \log(p_i), \quad \text{where } p_i \text{ is the probability of the $i$-th event}
\]</span></p>
<p>The reason for log being base 2 because we are dealing in binary signals either 1 or 0.</p>
<p>Hence -log(pi) is the suprise of seeing the outcome of the ith event</p>
<p>If this was a random variable X with Probability mass function as P where P(X) is pmf evaluated at X then Entropy can be written as</p>
<p><span class="math display">\[
H(X) = E [- \log(P(X))]
\]</span></p>
<p>E is expectation i.e weighted sum of all outcomes. One thing to note here the entropy is calculated here for independent events.</p>
<hr>
<section id="conditional-entropy" class="level3">
<h3 class="anchored" data-anchor-id="conditional-entropy">Conditional Entropy</h3>
<p><img src="images/conditional_1.png" class="img-fluid"></p>
<section id="a-independent-random-variables" class="level4">
<h4 class="anchored" data-anchor-id="a-independent-random-variables">(a) Independent Random Variables</h4>
<p>If (X) and (Y) are independent:</p>
<p><span class="math display">\[
P(X=x, Y=y) = P(X=x) \cdot P(Y=y)
\]</span></p>
<p>The joint entropy is:</p>
<span class="math display">\[\begin{aligned}
H(X,Y) &amp;= - \sum_x \sum_y P(X=x, Y=y) \log P(X=x, Y=y) \\
       &amp;= - \sum_x \sum_y P(X=x) P(Y=y) \log \big(P(X=x) P(Y=y)\big) \\
       &amp;= - \sum_x \sum_y P(X=x) P(Y=y) (\log \big(P(X=x)) +\log \big(P(Y=y)\big)) \\
       &amp;= - \sum_x  P(X=x) \log \big(P(X=x)) - \sum_y P(Y=y)  \log \big(P(Y=y)\big) \\

       &amp;= H(X) + H(Y)
\end{aligned}\]</span>
<p>Intuition</p>
<p>Independent: Knowing X tells nothing about Y → total uncertainty is sum.</p>
<p>If X and Y are independent, then:</p>
<p><span class="math display">\[
H(Y∣X)=H(Y)
\]</span></p>
<p>because knowing X tells you nothing about Y.</p>
<hr>
</section>
</section>
<section id="joint-entropy" class="level3">
<h3 class="anchored" data-anchor-id="joint-entropy">Joint Entropy</h3>
<p><img src="images/conditional_2.png" class="img-fluid"></p>
<p>Dependent Random Variables</p>
<p>If (X) and (Y) are dependent:</p>
<p><span class="math display">\[
P(X=x, Y=y) \neq P(X=x) \cdot P(Y=y)
\]</span></p>
<p>The joint entropy is:</p>
<p><span class="math display">\[
H(X,Y) = - \sum_x \sum_y P(X=x, Y=y) \log P(X=x, Y=y)
\]</span></p>
<p>But it <strong>cannot</strong> be split as (H(X)+H(Y)). Instead, we use <strong>conditional entropy</strong>:</p>
<p><span class="math display">\[
H(X,Y) = H(X) + H(Y|X)
\]</span></p>
<p>Where conditional entropy is:</p>
<p><span class="math display">\[
H(Y|X) = - \sum_x \sum_y P(X=x, Y=y) \log P(Y=y | X=x)
\]</span></p>
<ul>
<li>If (X) and (Y) are independent: (H(Y|X) = H(Y)) → reduces to additive case.</li>
</ul>
<p><strong>Intuition</strong></p>
<p>Dependent: Knowing X reduces uncertainty about Y → total uncertainty is less than the sum.<br>
But if X and Y are dependent, then knowing X reduces the uncertainty about Y. In that case: <span class="math display">\[
H(Y∣X)≤H(Y)
\]</span> with strict inequality unless they’re independent.</p>
<hr>
</section>
<section id="information-gain" class="level3">
<h3 class="anchored" data-anchor-id="information-gain">Information Gain</h3>
<p>Now we have learned Entropy of a random variable X as H(X) If two random dependent random<br>
variable X and Y exists then knowing X reduces uncertainty of Y by amount H(Y|X) called Conditional Probability.<br>
If I have to calculate entropy of an event with two dependent random variables then H(X,Y) called the Joint Probability.</p>
<p>Then another natural question can be asked if know the entropy of X as H(X) as a whole and if i get to know about Y then by what amount entropy of X is reduced.</p>
<p>i.e <span class="math display">\[
H(X) - H(X|Y) = ?
\]</span></p>
<p>This question mark is called Information gain which is quite obvious.<br>
If i get to know Y then by what amount did i gain information on X.<br>
Similar way there will be some Information gain I when we reverse the situation.</p>
<p><span class="math display">\[
I(Y) = H(Y) - H(Y|X)
\]</span></p>
<p>now another way of looking at this we know joint probability from both side is symetric i.e</p>
<span class="math display">\[\begin{aligned}

H(X,Y)=H(X)+H(Y∣X)\\
H(X,Y)=H(Y)+H(X∣Y) \\
\text{hence by re-arranging you will get} \\
H(X)+H(Y∣X) = H(Y)+H(X∣Y) \\
H(X)- H(X∣Y) = H(Y) - H(Y∣X) \\
I(X;Y) = H(X)- H(X∣Y) = H(Y) - H(Y∣X) \\

\end{aligned}\]</span>
<p>If this make sense then you will understand Joint probability is talking about total information about both the variables information gain is talking about the information gain for one variable and because it is equivalent we call it mutual information.</p>
<p><img src="images/entropy_ven.png" class="img-fluid"></p>
<hr>
</section>
<section id="kl-divergence" class="level3">
<h3 class="anchored" data-anchor-id="kl-divergence">KL Divergence</h3>
<p>Information gain also be defined by unpacking <span class="math display">\[
I(X;Y)=\sum_{x,y}​p(x,y)\log \frac{p(x,y)}{​p(x).p(y)}
\]</span></p>
<p>Now look at the ratio inside the log:</p>
<ul>
<li>Numerator: the true joint probability p(x,y).</li>
<li>Denominator: the independent assumption p(x)p(y).</li>
</ul>
<p>This says:</p>
<p>Mutual information measures how far reality (the true joint) is from the case where X and Y are independent.</p>
<p>That’s exactly a KL divergence:</p>
<p><span class="math display">\[   
I(X;Y)=D_{KL}(p(x,y)∣∣p(x)p(y))
\]</span></p>
<p>Intuition</p>
<p>KL divergence is just the “price of assuming the wrong distribution.”</p>
<ul>
<li>If you thought X and Y were independent, you’d model their joint as p(x)p(y).</li>
<li>The true joint is p(x,y).</li>
<li>KL divergence tells you how many extra bits you waste, on average, by using the wrong assumption.</li>
</ul>
<p>So KL comes in very organically: it’s the distance between your mental model (independence) and reality (the true joint).</p>
<p>More standard way of defining KL Divergence is</p>
<p><span class="math display">\[
D_{KL}(P∣∣Q)=\sum_{z}P(z)\log⁡\frac{P(z)}{Q(z)}
\]</span> ​</p>
<hr>
<p><strong>Step 1: Why the ratio <span class="math inline">\(\frac{P}{Q}\)</span>?</strong></p>
<ul>
<li><p>Imagine you expected the world to follow distribution <span class="math inline">\(Q\)</span>.</p></li>
<li><p>But in reality, the world follows <span class="math inline">\(P\)</span>.</p></li>
<li><p>At each outcome <span class="math inline">\(z\)</span>, the ratio</p>
<p><span class="math display">\[
\frac{P(z)}{Q(z)}
\]</span></p>
<p>tells you <strong>how “surprised” you are</strong> when comparing reality <span class="math inline">\(P\)</span> vs.&nbsp;your assumption <span class="math inline">\(Q\)</span>.</p>
<ul>
<li>If <span class="math inline">\(P(z) &gt; Q(z)\)</span>: outcome happens more often than you thought → ratio &gt; 1.</li>
<li>If <span class="math inline">\(P(z) &lt; Q(z)\)</span>: outcome happens less often than you thought → ratio &lt; 1.</li>
</ul></li>
</ul>
<p>So the log term is literally a <strong>correction factor in “bits” or “nats”</strong>.</p>
<hr>
<p><strong>tep 2: Why weight by <span class="math inline">\(P(z)\)</span>?</strong></p>
<p>If you only look at the ratio, you’re ignoring how often those outcomes really occur.</p>
<ul>
<li>For example, suppose you miss a rare outcome (<span class="math inline">\(P(z)\)</span> tiny) then the ratio might be huge, but it barely matters because it happens rarely.</li>
<li>If you consistently mis-estimate a common outcome (<span class="math inline">\(P(z)\)</span> large), then the error really matters.</li>
</ul>
<p>Thus, you <strong>average the correction</strong> using the <em>true</em> distribution <span class="math inline">\(P\)</span>. That’s why it’s an expectation:</p>
<p><span class="math display">\[
D_{KL}(P||Q) = \mathbb{E}_{z\sim P}\Big[\log \frac{P(z)}{Q(z)}\Big]
\]</span></p>
<hr>
<p><strong>Step 3: Intuitive Example</strong></p>
<p>Say we flip a coin:</p>
<ul>
<li>Reality: <span class="math inline">\(P = (0.9 \text{ heads}, 0.1 \text{ tails})\)</span>.</li>
<li>Your belief: <span class="math inline">\(Q = (0.5, 0.5)\)</span>.</li>
</ul>
<p>Now compute:</p>
<p><span class="math display">\[
D_{KL}(P||Q) = 0.9 \log \frac{0.9}{0.5} + 0.1 \log \frac{0.1}{0.5}
\]</span></p>
<ul>
<li><p>For heads: ratio = <span class="math inline">\(0.9/0.5 = 1.8\)</span>. You consistently under-estimated how often heads appear, so log term is positive → penalty is big, and since heads are frequent, the weight (0.9) amplifies it.</p></li>
<li><p>For tails: ratio = <span class="math inline">\(0.1/0.5 = 0.2\)</span>. You thought tails were more common than reality. Log term is negative, but it only happens 10% of the time, so its effect is small.</p></li>
</ul>
<p>Total KL &gt; 0 means your model <span class="math inline">\(Q\)</span> is “diverging” from truth.</p>
<hr>
<p><strong>Step 4: Geometric view</strong></p>
<p>Think of KL as <strong>how tilted one distribution is compared to another</strong>.</p>
<ul>
<li><p><span class="math inline">\(P\)</span> is the “true shape.”</p></li>
<li><p><span class="math inline">\(Q\)</span> is your “assumed shape.”</p></li>
<li><p>The ratio <span class="math inline">\(\frac{P}{Q}\)</span> shows the local stretching or shrinking.</p></li>
<li><p>The weighting by <span class="math inline">\(P\)</span> means: only care about distortion where reality actually lives.</p></li>
<li><p>Ratio: <em>local mismatch</em>.</p></li>
<li><p>Log: <em>expressed in information units</em>.</p></li>
<li><p>Weighted by <span class="math inline">\(P\)</span>: <em>average mismatch under reality</em>.</p></li>
</ul>
<p>KL divergence has a very particular range:</p>
<p><span class="math display">\[
D_{KL}(P||Q) \;\; \in \; [0, \infty]
\]</span></p>
<p><strong>Why lower bound is 0</strong></p>
<ul>
<li>KL divergence is always <strong>non-negative</strong> (Gibbs’ inequality).</li>
<li><span class="math inline">\(D_{KL}(P||Q) = 0\)</span> <strong>iff</strong> <span class="math inline">\(P = Q\)</span> exactly (they’re identical distributions everywhere).</li>
</ul>
<p><strong>Why no finite upper bound</strong></p>
<ul>
<li>If there’s any point <span class="math inline">\(z\)</span> where <span class="math inline">\(P(z) &gt; 0\)</span> but <span class="math inline">\(Q(z) = 0\)</span>, then the ratio <span class="math inline">\(\frac{P(z)}{Q(z)}\)</span> blows up → KL divergence goes to <span class="math inline">\(\infty\)</span>.</li>
<li>Even if both are &gt;0 everywhere, if <span class="math inline">\(Q\)</span> assigns tiny probabilities where <span class="math inline">\(P\)</span> assigns big ones, KL can become arbitrarily large.</li>
</ul>
<hr>
<p><strong>Example (discrete coin):</strong></p>
<ol type="1">
<li><p><span class="math inline">\(P=(0.5, 0.5)\)</span>, <span class="math inline">\(Q=(0.5, 0.5)\)</span>:</p>
<p><span class="math display">\[
D_{KL}(P||Q)=0
\]</span></p></li>
<li><p><span class="math inline">\(P=(0.9, 0.1)\)</span>, <span class="math inline">\(Q=(0.5, 0.5)\)</span>:</p>
<p><span class="math display">\[
D_{KL}(P||Q)\approx 0.368
\]</span></p></li>
<li><p><span class="math inline">\(P=(0.9, 0.1)\)</span>, <span class="math inline">\(Q=(1.0, 0.0)\)</span>:</p>
<ul>
<li>For tails, <span class="math inline">\(P=0.1\)</span>, <span class="math inline">\(Q=0\)</span>. Ratio → ∞.</li>
<li>So KL → ∞.</li>
</ul></li>
</ol>
<hr>
<ul>
<li>KL is <strong>asymmetric</strong>: <span class="math inline">\(D_{KL}(P||Q) \neq D_{KL}(Q||P)\)</span>.</li>
<li>Both are ≥ 0, but can blow up in different places.</li>
<li><strong>Minimum</strong> = 0 (when the two distributions are identical).</li>
<li><strong>Maximum</strong> = ∞ (when <span class="math inline">\(Q\)</span> misses events that <span class="math inline">\(P\)</span> considers possible).</li>
</ul>
<hr>
</section>
<section id="cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="cross-entropy">Cross Entropy</h3>
<p>Let’s take Data D, with input features as X and actual labels as A, Model M with θ parameters trying to predict the labels where Y is the predicted labels.</p>
<p><span class="math display">\[
Y = M_{\theta}(X)  ≈ p(A∣X;θ)
\]</span></p>
<p>Hence A is my actual label distribution and Y be the predicted label distribution.</p>
<p>Couple of things to notice 1. Entropy of truth A is H(A).</p>
<ol start="2" type="1">
<li><p>Entropy of H(Actual Lables A |for features X) will be &gt;0 in a real world setting but in a determinsitc problem where knowing X can actually predict A easily like a square function then this value is 0.</p></li>
<li><p>H(A|X) becomes Bayes irreducible uncertainty i.e entropy or unpredicitability nature of A when we fully know about X i.e because of the randomness or leftover ambiguity.</p></li>
<li><p>The goal of learning is to do H(Y) ≈ H(A), so that the model has captured all the predictable structure in A - it’s uncertain only where A itself is uncertain.</p></li>
<li><p>The Information gain between X and A is a fixed number our Y is closer approximation of Y then IG(A;X)&gt; IG(A;Y) for a perfectly trained model the value will be same.</p></li>
</ol>
<p>By Data Processing Inequality (since A→X→Y) <span class="math display">\[
I(A;Y)≤I(A;X).
\]</span></p>
<span class="math display">\[\begin{aligned}
IG(A;X) = H(A) - H(A|X) \\
H(A) = IG(A;X) + H(A|X) \\
\end{aligned}\]</span>
<p>Entropy of A = Things that will modeled by model M + Things that our model can’t be learn</p>
<p>True best possible model is the Bayes classifier - <span class="math display">\[
P^*(A∣X)
\]</span></p>
<p>Our model gives some approximation: <span class="math display">\[
Q(A∣X)
\]</span></p>
<p>The KL/excess risk measures how far your predicted probabilities are from the Bayes-optimal probabilities, in expectation <span class="math display">\[
E_{X}[D_{KL}(P^*(A∣X)  ∣∣  Q(A∣X))].
\]</span></p>
<p>This KL term = estimation/approximation error the smaller the error the better the learning.</p>
<p>Hence It should be clear, that</p>
<p>Entropy of A = Entropy of A with respect to Q - irreducible error - KL terms</p>
<p>Ignoring the irreducible error and some rearraging we get,</p>
<p>Entropy of A with respect to Q = Entropy of A + KL term</p>
<p>this can be formally written as</p>
<p><span class="math display">\[
H(P^*, Q) = H(P^*) + KL terms
\]</span> To prove it we can write</p>
<span class="math display">\[\begin{aligned}
D_{KL}(P^*(A∣X)  ∣∣  Q(A∣X)) = \sum P^*(X) (\log P^*(A∣X)- \log Q(A∣X)) \\
D_{KL}(P^*(A∣X)  ∣∣  Q(A∣X)) = \sum P^*(X) \log P^*(A∣X)- \sum P^*(X) \log Q(A∣X) \\
D_{KL}(P^*(A∣X)  ∣∣  Q(A∣X)) = -H(P^*) + H(P,Q) \\
H(P,Q) = H(P^*) + D_{KL}(P^*(A∣X)  ∣∣  Q(A∣X)) \\
\end{aligned}\]</span>
<p>H(P,Q) is cross entropy it is not joint entropy notation it is Cross-Entropy.</p>
<pre><code>Hence Entropy - The truth  
Cross Entropy - You trying to predict the truth  
KL terms - The error i.e how far you are from truth.</code></pre>
<span class="math display">\[\begin{aligned}
Missing Information = IG(A;X) - IG(A;Y) \\
E_{X}[D_{KL}(P^*(A∣X)  ∣∣  Q(A∣X))] = \text{Missing Information} \\
\end{aligned}\]</span>
</section>
<section id="physics-and-information-theory-relation" class="level2">
<h2 class="anchored" data-anchor-id="physics-and-information-theory-relation">Physics and Information Theory Relation</h2>
<p>From Wikipedia -</p>
<p>Relationship to thermodynamic entropy</p>
<p>The inspiration for adopting the word entropy in information theory came from the close resemblance between Shannon’s formula and very similar known formulae from statistical mechanics.</p>
<p>In statistical thermodynamics the most general formula for the thermodynamic entropy S of a thermodynamic system is the Gibbs entropy.</p>
<p><span class="math display">\[
S=-k_{\text{B}}\sum _{i}p_{i}\ln p_{i}\
\]</span></p>
<p>where kB is the Boltzmann constant, and pi is the probability of a microstate. The Gibbs entropy was defined by J. Willard Gibbs in 1878 after earlier work by Ludwig Boltzmann (1872).</p>
</section>
</section>
<section id="why-doesnt-time-flows-backward" class="level1">
<h1>Why doesn’t time flows backward?</h1>
</section>
<section id="why-doesnt-milk-and-coffee-unmix-itself" class="level1">
<h1>Why doesn’t milk and coffee unmix itself?</h1>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/aman5319\.github\.io\/portfolio\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="aman5319/portfolio" data-repo-id="R_kgDOJ44pkw" data-category="Q&amp;A" data-category-id="DIC_kwDOJ44pk84CXwVt" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Blog made with 💜 and <a href="https://quarto.org/">Quarto</a>, by Aman Pandey. License: <a href="https://creativecommons.org/licenses/by-sa/2.0/">CC BY-SA 2.0</a>.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/aman5319/portfolio/edit/main/tech_blogs/Machine Learning/entropy.md" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/aman5319/portfolio/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:amanpandey@mailfence.com">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>