[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nBig Data Processing\n\n\n\n\n\n\n\nbig-data\n\n\naws\n\n\n\n\nExploring different tools and technologies to store, intergrate, analyse, process, visualize data at large scale in a batch and stream fashion both with custom tools and AWS managed services\n\n\n\n\n\n\nOct 6, 2023\n\n\nAman Pandey\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nData and Model Management | Model Monitoring and Logging.\n\n\n\n\n\n\n\nmachine-learning\n\n\ndeep-learning\n\n\n\n\nIn this post, we discuss ways of managing different versions of Data and Model artificats. Configuration Management and Data Validation. Monitoring the model using TIG stack. Logging and Alert.\n\n\n\n\n\n\nSep 16, 2023\n\n\nAman Pandey\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nDifferent types of Attentions\n\n\n\n\n\n\n\ndeep-learning\n\n\nattention\n\n\n\n\nA dive into different types of attention from it start with Bahdanau to Flash Attention.\n\n\n\n\n\n\nJun 22, 2023\n\n\nAman Pandey\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nImprove Model’s Prediction power using Regularization\n\n\n\n\n\n\n\ndeep-learning\n\n\n\n\nDifferent ways to increase the model’s prediction power using different regularization Techniques\n\n\n\n\n\n\nMar 20, 2023\n\n\nAman Pandey\n\n\n16 min\n\n\n\n\n\n\n  \n\n\n\n\nConvolution and Common architectures\n\n\n\n\n\n\n\ndeep-learning\n\n\nconvolution\n\n\n\n\nUnderstanding different types of CNN layers and some common architectures\n\n\n\n\n\n\nNov 8, 2022\n\n\nAman Pandey\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nOptimizers in - Deep Learning\n\n\n\n\n\n\n\ndeep-learning\n\n\noptimization\n\n\n\n\n3D visualization of different optimizers.\n\n\n\n\n\n\nNov 4, 2022\n\n\nAman Pandey\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nMetrics for Machine Learning\n\n\n\n\n\n\n\nmachine-learning\n\n\ndeep-learning\n\n\nmetrics\n\n\n\n\nA deep dive into different types of Metrics for evaluating Machine Learning Models.\n\n\n\n\n\n\nMay 22, 2022\n\n\nAman Pandey\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nWeight Intialization\n\n\n\n\n\n\n\ndeep-learning\n\n\n\n\nLearning different ways of weight Intialization their effects and common pitfalls.\n\n\n\n\n\n\nJun 24, 2021\n\n\nAman Pandey\n\n\n16 min\n\n\n\n\n\n\n  \n\n\n\n\nMemory and Time Profiling\n\n\n\n\n\n\n\npython\n\n\n\n\nLet’s save some time and memory, coz both are expensive.\n\n\n\n\n\n\nMay 22, 2021\n\n\nAman Pandey\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nFunctional Programming capabilites of python\n\n\n\n\n\n\n\npython\n\n\n\n\nLambda, Comphrensions, Decorators and Generators with some sweet python tips and tricks.\n\n\n\n\n\n\nNov 4, 2020\n\n\nAman Pandey\n\n\n6 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/Optimization/optimisers_in_dl.html",
    "href": "posts/Optimization/optimisers_in_dl.html",
    "title": "Optimizers in - Deep Learning",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import make_circles\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import dataset ,dataloader\nfrom termcolor import colored\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.animation as animation\nfrom matplotlib import colors as mcolors\n\nfrom IPython.display import HTML\n\ndevice = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEED=1\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True"
  },
  {
    "objectID": "posts/Optimization/optimisers_in_dl.html#sgd",
    "href": "posts/Optimization/optimisers_in_dl.html#sgd",
    "title": "Optimizers in - Deep Learning",
    "section": "SGD",
    "text": "SGD\n\\[\ndata = data - lr* grad\n\\]\n\n\nCode\ndef SGD(vector):\n    vectors = [vector.squeeze(0).tolist()]\n    for i in range(EPOCHS):\n        z = LOSS_FUNC(vector)\n        z.backward()\n        with torch.no_grad():\n            vector.data = vector.data - LR *  vector.grad\n            vector.grad.zero_()\n            vectors.append(vector.squeeze(0).tolist())\n    return np.array(vectors)\n\n\n\n\nCode\nanim = Animation(SGD)(VECTOR)\nHTML(anim)\n\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/Optimization/optimisers_in_dl.html#sgd-with-momentum",
    "href": "posts/Optimization/optimisers_in_dl.html#sgd-with-momentum",
    "title": "Optimizers in - Deep Learning",
    "section": "SGD with Momentum",
    "text": "SGD with Momentum\nThe real momentum update looks like\n\\[\nvelocity = viscosity * velocity - lr* grad \\\\\ndata = data - velocity\n\\]\nBut In pytorch they do the other way\n\\[\nvelocity = viscosity * velocity + grad \\\\\ndata = data - lr * velocity\n\\]\n\n\nCode\ndef SGD_Momentum(vector):\n    viscosity = 0.9\n    velocity = torch.zeros_like(vector)\n    vectors = [vector.squeeze(0).tolist()]\n\n    for i in range(EPOCHS):\n        z = LOSS_FUNC(vector)\n        z.backward()\n        with torch.no_grad():\n            velocity = viscosity * velocity + vector.grad\n            vector.data = vector.data - LR *  velocity\n            vector.grad.zero_()\n            vectors.append(vector.squeeze(0).tolist())\n    return np.array(vectors)\n\n\n\n\nCode\nanim = Animation(SGD,SGD_Momentum)(VECTOR)\nHTML(anim)\n\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/Optimization/optimisers_in_dl.html#sgd-with-nestrov-momentum",
    "href": "posts/Optimization/optimisers_in_dl.html#sgd-with-nestrov-momentum",
    "title": "Optimizers in - Deep Learning",
    "section": "SGD with Nestrov Momentum",
    "text": "SGD with Nestrov Momentum\n\nNesterov momentum. Instead of evaluating gradient at the current position (red circle), we know that our momentum is about to carry us to the tip of the green arrow. With Nesterov momentum we therefore instead evaluate the gradient at this “looked-ahead” position."
  },
  {
    "objectID": "posts/Optimization/optimisers_in_dl.html#adagrad",
    "href": "posts/Optimization/optimisers_in_dl.html#adagrad",
    "title": "Optimizers in - Deep Learning",
    "section": "AdaGrad",
    "text": "AdaGrad\n\\[\ncache = cache + (grad^2) \\\\\ndata = data - \\frac {(lr * grad)}{( \\sqrt {cache} + eps)}\n\\]\nIt is a adaptive learning method where we are constantly annealing the lr.\nThe eps is a extremely small value to smoothen the denominator.\n\n\nCode\n\ndef adaGrad(vector):\n    lr=0.5\n    cache = torch.zeros_like(vector)\n    vectors = [vector.squeeze(0).tolist()]\n    eps = 1e-10 # can be any small value\n\n    for i in range(EPOCHS):\n        z = LOSS_FUNC(vector)\n        z.backward()\n        with torch.no_grad():\n\n            cache +=  (vector.grad ** 2)  \n            vector.data = vector.data - (lr *  vector.grad / (torch.sqrt(cache) + eps)) \n            vector.grad.zero_()\n            vectors.append(vector.squeeze(0).tolist())\n    \n    return np.array(vectors)\n\n\n\n\nCode\nanim = Animation(SGD,SGD_Momentum,adaGrad)(VECTOR)\nHTML(anim)\n\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/Optimization/optimisers_in_dl.html#rmsprop",
    "href": "posts/Optimization/optimisers_in_dl.html#rmsprop",
    "title": "Optimizers in - Deep Learning",
    "section": "RMSProp",
    "text": "RMSProp\nIt solved AdaGrad problem of deminishing gradient. \\[\ncache = alpha *cache + (1-alpha) * grad^2 \\\\\ndata = data - \\frac {(lr * grad)} {(\\sqrt{cache} + eps)}\n\\]\n\n\nCode\ndef RMSprop(vector):\n    lr=0.5\n    alpha=0.99\n    cache = torch.zeros_like(vector)\n    vectors = [vector.squeeze(0).tolist()]\n    eps = 1e-10 # can be any small value\n\n    for i in range(EPOCHS):\n        z = LOSS_FUNC(vector)\n        z.backward()\n        with torch.no_grad():\n\n            cache = alpha * cache + ((1-alpha) * (vector.grad ** 2))  \n            vector.data = vector.data - (lr *  vector.grad / (torch.sqrt(cache) + eps)) \n            vector.grad.zero_()\n            vectors.append(vector.squeeze(0).tolist())\n    return np.array(vectors)\n\n\n\n\nCode\nanim = Animation(RMSprop,adaGrad)(VECTOR)\nHTML(anim)\n\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/Optimization/optimisers_in_dl.html#adadelta",
    "href": "posts/Optimization/optimisers_in_dl.html#adadelta",
    "title": "Optimizers in - Deep Learning",
    "section": "AdaDelta",
    "text": "AdaDelta\nIt is all similar to RMSProp with an additional delta attribute which just eliminates the use of lr away from the update parameter.\nThe authors note that the units in this update (as well as in SGD, Momentum, or Adagrad) do not match, i.e. the update should have the same hypothetical units as the parameter. To realize this, they first define another exponentially decaying average, this time not of squared gradients but of squared parameter updates:\n\\[\ncache = alpha *cache + (1-alpha) * grad^2 \\\\\ndelta = \\frac {\\sqrt {acc\\_delta + eps}*grad}{\\sqrt{cache+eps}}\\\\\ndata = data - delta\\\\\nacc\\_delta = alpha * acc\\_delta + (1-alpha) * delta^2 \\\\\n\\]\n\n\nCode\ndef AdaDelta(vector):\n    rho = 0.9\n    lr=1.0\n    cache = torch.zeros_like(vector)\n    acc_delta = torch.zeros_like(vector)\n    vectors = [vector.squeeze(0).tolist()]\n    eps = 1e-6 # can be any small value\n\n    for i in range(EPOCHS):\n        z = LOSS_FUNC(vector)\n        z.backward()\n        with torch.no_grad():\n            cache = rho * cache + ((1-rho) * (vector.grad ** 2))  \n            delta = torch.sqrt(acc_delta+eps) *vector.grad / torch.sqrt(cache+eps)\n            vector.data = vector.data - lr * delta\n            acc_delta = acc_delta * rho + ((1-rho) * (delta**2))\n            vector.grad.zero_()\n            vectors.append(vector.squeeze(0).tolist())\n    return np.array(vectors)\n\n\n\n\nCode\nanim = Animation(adaGrad,RMSprop,AdaDelta)(VECTOR)\nHTML(anim)\n\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/Optimization/optimisers_in_dl.html#adam",
    "href": "posts/Optimization/optimisers_in_dl.html#adam",
    "title": "Optimizers in - Deep Learning",
    "section": "Adam",
    "text": "Adam\nIt is basically RMSProp with Momentum\n\\[\nm = beta1*m + (1-beta1)*dx \\\\\nmt = \\frac{m} {(1-beta1^t)} \\\\\nv = beta2*v + (1-beta2)* dx^2 \\\\\nvt = \\frac{v} {(1-beta2^t)} \\\\\nx = x - \\frac{lr * mt}{np.sqrt(vt) + eps}\n\\]\nNotice that the update looks exactly as RMSProp update, except the “smooth” version of the gradient m is used instead of the raw (and perhaps noisy) gradient vector dx. Recommended values in the paper are eps = 1e-8, beta1 = 0.9, beta2 = 0.999. In practice Adam is currently recommended as the default algorithm to use, and often works slightly better than RMSProp. However, it is often also worth trying SGD+Nesterov Momentum as an alternative. The full Adam update also includes a bias correction mechanism, which compensates for the fact that in the first few time steps the vectors m,v are both initialized and therefore biased at zero, before they fully “warm up”. With the bias correction mechanism, the update looks as follows:\n\n\nCode\nimport math\ndef Adam(vector):\n    m = torch.zeros_like(vector)\n    v = torch.zeros_like(vector)\n    beta1 = 0.9\n    beta2 = 0.99\n    eps = 1e-8\n    lr=0.2\n    vectors = [vector.squeeze(0).tolist()]\n    for i in range(1 , EPOCHS+1):\n        z = LOSS_FUNC(vector)\n        z.backward()\n        with torch.no_grad():\n            m = m * beta1 + (1-beta1)*vector.grad\n            v = v * beta2 + (1-beta2)*(vector.grad**2)\n            denom = torch.sqrt(v)+eps\n            bias_correction_m = 1 - beta1**i\n            bias_correction_v = 1 - beta2**i\n            \n            step_size = lr * math.sqrt(bias_correction_v)/bias_correction_m\n            \n            \n            vector.data = vector.data - step_size*m/denom\n            vector.grad.zero_()\n            vectors.append(vector.squeeze(0).tolist())\n    return np.array(vectors)\n\n\n\n\nCode\nanim = Animation(adaGrad,RMSprop,Adam)(VECTOR)\nHTML(anim)\n\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/Python/functional_programming.html",
    "href": "posts/Python/functional_programming.html",
    "title": "Functional Programming capabilites of python",
    "section": "",
    "text": "Lambda and Comphresions\nFunctions in python are first class Objects that means you can assign them to variable, store them in data structure, pass them as a parameter to other functions and even return them from other function\n\n\nCode\n# addition function \ndef add(x, y):\n    return x+y\nprint (f\" function is add(5,6) = {add(5,6)}\")\n# you can assign them to other variable\nmyAdd = add\n# wait you can also delete the add function and  the myAdd still points to underlying function\ndel add\nprint (f\" function is myAdd(5,6) = {myAdd(5,6)}\")\n#functions have their own set of attributes \nprint(f\"{myAdd.__name__}\")\n# to see a complete list of attributes of a function type dir(myAdd) in console\n\n\n function is add(5,6) = 11\n function is myAdd(5,6) = 11\nadd\n\n\n\n\nCode\n# functions as data structures\nList_Funcs = [str.upper , str.lower , str.title]\nfor f in List_Funcs:\n    print (f , f(\"aI6-saturdays\"))\n\n\n&lt;method 'upper' of 'str' objects&gt; AI6-SATURDAYS\n&lt;method 'lower' of 'str' objects&gt; ai6-saturdays\n&lt;method 'title' of 'str' objects&gt; Ai6-Saturdays\n\n\nSo lambdas are a sweet Little anonymous Single-Expression functions\n\n\nCode\nadd_lambda = lambda x , y : x+y # lambda automaticaly returns the value after colon\nprint(f\"lambda value add_lambda(2,3)= {add_lambda(2,3)}\") #you call lambda function as normal functions\n\n\nlambda value add_lambda(2,3)= 5\n\n\nYou :- But Wait it’s an anonymous function and how can you give it a name\nStackOverflow :- Relax, Searching for another example\n\n\nCode\ndef someFunc(func):\n    quote = func(\"We will democratize AI \")\n    return quote\n# here the lambda function is passes to a normal function \n# the lambda here is anonymous and the parameter my_sentence = We will democratize AI \n# so we are adding some text of ours and returning the string\nsomeFunc(lambda my_sentence: my_sentence+\"by teaching everyone AI\")\n\n\n'We will democratize AI by teaching everyone AI'\n\n\n\n\nCode\n# here is one more example\ntuples = [(1, 'd'), (2, 'b'), (4, 'a'), (3, 'c')]\nsorted(tuples, key=lambda x: x[1])\n\n\n\n\nCode\n# list comphrensions\nl = [ x for x in range(20)]\neven_list = [x for x in l if x%2==0]\neven_list_with_Zero = [x if x%2==0 else 0 for x in l ]\nl , even_list ,even_list_with_Zero\n\n\n([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n [0, 2, 4, 6, 8, 10, 12, 14, 16, 18],\n [0, 0, 2, 0, 4, 0, 6, 0, 8, 0, 10, 0, 12, 0, 14, 0, 16, 0, 18, 0])\n\n\n\n\nCode\n# dictionary comphrension\nd = {x: x**2 for x in range(2,6)}\nflip_key_value = {value:key for key,value in d.items()}\nd , flip_key_value\n\n\n({2: 4, 3: 9, 4: 16, 5: 25}, {4: 2, 9: 3, 16: 4, 25: 5})\n\n\n\n\nDecorators\nPython’s decorators allow you to extend and modify the behavior of a callable (functions, methods, and classes) without permanently modifying the callable itself.\nAny sufficiently generic functionality you can tack on to an existing class or function’s behavior makes a great use case for decoration. This includes the following:\n\nlogging\nenforcing access control and authentication\ninstrumentation and timing functions\nrate-limiting\ncaching, and more\n\nImagine that you some 50 functions in your code. Now that all functions are working you being a great programmer thought of optimizing each function by checking the amount of time it takes and also you need to log the input/output of few functions. what are you gonna do ?\nWithout decorators you might be spending the next three days modifying each of those 50 functions and clutter them up with your manual logging calls. Fun times, right?\n\n\nCode\ndef my_decorator(func):\n    return func  # It's simple right it takes a function as it's parameter and returns it\ndef someFunc():\n    return \"Deep learning is fun\"\nsomeFunc = my_decorator(someFunc) # it is similar to i = i + 1\nprint(f\" someFunc value  = {someFunc()}\")\n\n\n someFunc value  = Deep learning is fun\n\n\n\n\nCode\n# now just to add syntatic sugar to the code so that we can brag how easy and terse python code is \n# we gonna write this way\ndef my_decorator(func):\n    return func \n\n@my_decorator  # the awesomeness of this block of code lies here which can be used as toggle switch\ndef someFunc():\n    return \"Deep learning is fun\"\n\nprint(f\" someFunc value  = {someFunc()}\")\n\n\n someFunc value  = Deep learning is fun\n\n\nStackoverflow :- Now that you got a little taste of Decorators let’s write another decorator that actually does something and modifies the behavior of the decorated function.\n\n\nCode\n# This blocks contains and actual implementation of decorator\nimport time\n#import functools\n\ndef myTimeItDeco(func):\n    #@functools.wraps(func)\n    def wrapper(*args,**kwargs):\n        starttime = time.time()\n        call_of_func = func(*args,**kwargs) # this works because you can function can be nested and they remember the state\n        function_modification = call_of_func.upper()\n        endtime = time.time()\n        return f\" Executed output is {function_modification} and time is {endtime-starttime} \"\n    return wrapper\n\n@myTimeItDeco\ndef myFunc(arg1,arg2,arg3): # some arguments of no use to show how to pass them in code\n    \"\"\"Documentation of a obfuscate function\"\"\"\n    time.sleep(2) # just to show some complex calculation\n    return \"You had me at Hello world\"\n\nmyFunc(1,2,3) , myFunc.__doc__ , myFunc.__name__ \n\n\n(' Executed output is YOU HAD ME AT HELLO WORLD and time is 2.0032527446746826 ',\n None,\n 'wrapper')\n\n\nYou :- Why didn’t I got the doc and the name of my function. Hmmm….\nStackOverflow :- Great Programmers use me as there debugging tool, so use It.\nHints functools.wrap\nStackOverflow : - Applying Multiple Decorators to a Function (This is really fascinating as it’s gonna confuse you)\n\n\nCode\ndef strong(func):\n    def wrapper():\n        return '&lt;strong&gt;' + func() + '&lt;/strong&gt;'\n    return wrapper\n\ndef emphasis(func):\n    def wrapper():\n        return '&lt;em&gt;' + func() + '&lt;/em&gt;'\n    return wrapper\n\n@strong\n@emphasis\ndef greet():\n    return 'Hello!'\n\n\ngreet()\n# this is your assignment to understand it hints strong(emphasis(greet))()\n\n\n'&lt;strong&gt;&lt;em&gt;Hello!&lt;/em&gt;&lt;/strong&gt;'\n\n\n\n\nCode\n#Disclaimer Execute this at your own risk \n#Only  80's kids will remember this\nimport dis\ndis.dis(greet)\n\n\n\n\nContext Managers\n\n\nCode\n# let's open  a file and write some thing into it\nfile = open('hello.txt', 'w')\ntry:\n    file.write('Some thing')\nfinally:\n    file.close()\n\n\nYou :- ok now that I have wrote something into the file I want to read it, but try and finally again, it suck’s. There should be some other way around\nStackOverflow :- Context manger for your Rescue\n\n\nCode\nwith open(\"hello.txt\") as file:\n    print(file.read())\n\n\nSome thing\n\n\nYou :- That’s pretty easy but what is with\nStackoverflow :- It helps python programmers like you to simplify some common resource management patterns by abstracting their functionality and allowing them to be factored out and reused.\nSo in this case you don’t have to open and close file all done for you automatically.\n\n\nCode\n# A good pattern for with use case is this \nsome_lock = threading.Lock()\n\n# Harmful:\nsome_lock.acquire()\n    try:\n        # Do something complicated because you are coder and you love to do so ...\n    finally:\n        some_lock.release()\n\n# Better :\nwith some_lock:\n    # Do something awesome Because you are a Data Scientist...\n\n\nYou :- But I want to use with for my own use case how do I do it?\nStackOverflow :- Use Data Models and relax\n\n\nCode\n# Python is language full of hooks and protocol\n# Here MyContextManger abides context manager protocol\nclass MyContextManger:\n    \n    def __init__(self, name):\n        self.name=name\n# with statement automatically calls __enter__ and __exit__ methods        \n    def __enter__(self):  ## Acquire the lock do the processing in this method\n        self.f = open(self.name,\"r\")\n        return self.f\n    \n    def __exit__(self,exc_type,exc_val,exc_tb): ## release the lock and free allocated resources in this method\n        if self.f:\n            self.f.close()\n            \nwith MyContextManger(\"hello.txt\") as f:\n    print(f.read())\n\n\nSome thing\n\n\nYou :- It works but what are those parameters in exit method\nStackoverflow :- Google It !\nYou :- But writing a class in python is hectic, I want to do functional Programming\nStackOverflow :- Use Decorators\n\n\nCode\nfrom contextlib import contextmanager\n\n@contextmanager\ndef mySimpleContextManager(name):\n    try:\n        f = open(name, 'r')\n        yield f\n    finally:\n        f.close()\n        \nwith mySimpleContextManager(\"hello.txt\")  as f:\n    print(f.read())\n\n\nSome thing\n\n\nYou :- Ok, that’s what I call a pythonic code but what is yeild\nStackoverflow :- Hang On!\n\n\nIterators and Generators\nAn iterator is an object representing a stream of data; this object returns the data one element at a time. A Python iterator must support a method called next() that takes no arguments and always returns the next element of the stream. If there are no more elements in the stream, next() must raise the StopIteration exception. Iterators don’t have to be finite, though; it’s perfectly reasonable to write an iterator that produces an infinite stream of data.\nThe built-in iter() function takes an arbitrary object and tries to return an iterator that will return the object’s contents or elements, raising TypeError if the object doesn’t support iteration. Several of Python’s built-in data types support iteration, the most common being lists and dictionaries. An object is called iterable if you can get an iterator for it.\n\n\nCode\nl = [1,2,3]\nit = l.__iter__()  ## same as iter(l)\nit.__next__() ## gives 1\nnext(it) ## gives 2\nnext(it) ## gives 3\nnext(it) ## gives error StopIteration\n\n\nStopIteration: \n\n\n\n\nCode\n#lets replicate the simple range method\nclass MyRange:\n    def __init__(self,start,stop):\n        self.start = start -1\n        self.stop = stop\n    def __iter__(self):\n        return self\n    \n    def __next__(self):\n        self.start = self.start+1\n        if self.start&lt;self.stop:\n            return self.start\n        else:\n            raise StopIteration()\n                \nfor i in MyRange(2,10):\n    print(i)\n\n\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nYou :- Again a class\nStackOverflow :- OK here’s a easy way Use Generators\nThey Simplify writing Iterators, kind of iterable you can only iterate over once. Generators do not store the values in memory, they generate the values on the fly so no storage is required. So you ask one value it will generate and spit it out\n\n\nCode\ndef myRange(start,stop):\n    while True:\n        if start&lt;stop:\n            yield start\n            start = start+1\n        else:\n            return \nfor i in myRange(2,10):\n    print(i)\n\n\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nYou’re doubtless familiar with how regular function calls work in Python or C. When you call a function, it gets a private namespace where its local variables are created. When the function reaches a return statement, the local variables are destroyed and the value is returned to the caller. A later call to the same function creates a new private namespace and a fresh set of local variables. But, what if the local variables weren’t thrown away on exiting a function? What if you could later resume the function where it left off? This is what generators provide; they can be thought of as resumable functions.\nAny function containing a yield keyword is a generator function; this is detected by Python’s bytecode compiler which compiles the function specially as a result.\nWhen you call a generator function, it doesn’t return a single value; instead it returns a generator object that supports the iterator protocol. On executing the yield expression, the generator outputs the value start , similar to a return statement. The big difference between yield and a return statement is that on reaching a yield the generator’s state of execution is suspended and local variables are preserved. On the next call to the generator’s next() method, the function will resume executing.\n\n\nCode\n# generator comphresnsive\nl = ( x for x in range(20))\nl\n\n\n&lt;generator object &lt;genexpr&gt; at 0x7f6f905e0fc0&gt;\n\n\n\n\nCode\n[*l]\n\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n\n\nLet’s look in more detail at built-in functions often used with iterators.\nTwo of Python’s built-in functions, map() and filter() duplicate the features of generator expressions:\nmap(f, iterA, iterB, …) returns an iterator over the sequence\nf(iterA[0], iterB[0]), f(iterA[1], iterB[1]), f(iterA[2], iterB[2]), ....\nfilter(predicate, iter) returns an iterator over all the sequence elements that meet a certain condition, and is similarly duplicated by list comprehensions. A predicate is a function that returns the truth value of some condition; for use with filter(), the predicate must take a single value.\n\n\nCode\n# why did it returned an empty list think?\n[*map(lambda x :x **2 , l)]\n\n\n[]\n\n\n\n\nCode\n[*filter(lambda x :x%2!=0,l)]\n\n\n&lt;filter at 0x7f6f9057f9e8&gt;\n\n\nzip(iterA, iterB, …) takes one element from each iterable and returns them in a tuple:\n\n\nCode\nz = zip(['a', 'b', 'c'], (1, 2, 3))\nfor x , y in z:\n    print (x,y)\n\n\na 1\nb 2\nc 3\n\n\n\n\nFunctools and Itertools (This is going to blow your mind)\nThese two python modules are super helpful in writing Efficient Functional Code\n\n\nCode\n# reduce\nfrom functools import reduce\nl = (x for x in range(1,10))\nreduce(lambda x,y : x+y , l)\n\n\n45\n\n\nFor programs written in a functional style, you’ll sometimes want to construct variants of existing functions that have some of the parameters filled in. Consider a Python function f(a, b, c); you may wish to create a new function g(b, c) that’s equivalent to f(1, b, c); you’re filling in a value for one of f()’s parameters. This is called “partial function application”.\nThe constructor for partial() takes the arguments (function, arg1, arg2, …, kwarg1=value1, kwarg2=value2). The resulting object is callable, so you can just call it to invoke function with the filled-in arguments.\n\n\nCode\nfrom functools import partial\n\ndef log(message, subsystem):\n    \"\"\"Write the contents of 'message' to the specified subsystem.\"\"\"\n    print('%s: %s' % (subsystem, message))\n    ...\n\nserver_log = partial(log, subsystem='server')\nserver_log('Unable to open socket')\n\n\nserver: Unable to open socket\n\n\n\n\nCode\nfrom itertools import islice ,takewhile,dropwhile\n\n# here is a very simple implementation of the fibonacci sequence \ndef fib(x=0 , y=1):\n    while True:\n        yield x\n        x , y = y , x+y\n\n\n\n\nCode\nlist(islice(fib(),10))\n\n\n[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n\n\n\n\nCode\nlist(takewhile(lambda x : x &lt; 5  , islice(fib(),10)))\n\n\n[0, 1, 1, 2, 3]\n\n\n\n\nCode\nlist(dropwhile(lambda x : x &lt; 5  , islice(fib(),10)))\n\n\n[5, 8, 13, 21, 34]\n\n\n\n\nCode\nlist(dropwhile(lambda x :x&lt;11 , takewhile(lambda x : x &lt; 211  , islice(fib(),15))))\n\n\n[13, 21, 34, 55, 89, 144]\n\n\nTo read more about itertools https://docs.python.org/3.6/howto/functional.html#creating-new-iterators\n\n\nSome Python Tricks\n\n\nCode\n#normal calculator prorgramm\ndef calculator(operator , x , y):\n    if operator==\"add\":\n        return x+y\n    elif operator==\"sub\":\n        return x-y\n    elif operator == \"div\":\n        return x/y\n    elif operator==\"mul\":\n        return x*y\n    else :\n        return \"unknow\"\ncalculator(\"add\",2,3)\n\n\n5\n\n\n\n\nCode\n#Pythonic way \ncalculatorDict = {\n    \"add\":lambda x,y:x+y,\n    \"sub\":lambda x,y:x-y,\n    \"mul\":lambda x,y:x*y,\n    \"div\":lambda x,y:x/y\n}\ncalculatorDict.get(\"add\",lambda x , y:None)(2,3)\n\n\n5\n\n\n\n\nCode\n# because we are repeating x,y in all lambda so better approach\ndef calculatorCorrected(operator,x,y):\n    return {\n    \"add\":lambda :x+y,\n    \"sub\":lambda :x-y,\n    \"mul\":lambda :x*y,\n    \"div\":lambda :x/y\n}.get(operator , lambda :\"None\")()\n\ncalculatorCorrected(\"add\",2,3)\n\n\n5\n\n\n\n\nCode\n# How to merge multiple dictionaries\nx = {1:2,3:4} \ny = {3:5,6:7}\n{**x,**y}\n\n\n{1: 2, 3: 5, 6: 7}\n\n\n\n\nCode\n# how to merge multiple list you gussed it correct \na = [1,2,3]\nb=[2,3,4,5]\n[*a,*b]\n\n\n[1, 2, 3, 2, 3, 4, 5]\n\n\n\n\nCode\n# Named tuple\nfrom collections import namedtuple\nfrom sys import getsizeof\nvector = namedtuple(\"Vector\" , [\"x\",\"y\",\"z\",\"k\"])(11,12,212,343)\nvector,vector[0], vector.y # can be accessed lke list and dic\n\n\n(Vector(x=11, y=12, z=212, k=343), 11, 12)\n\n\n\n\nCode\n# how to manage a dictionary with count\nfrom collections import Counter\npubg_level3_bag = Counter()\nkill = {\"kar98\":1 , \"7.76mm\":60}\npubg_level3_bag.update(kill)\nprint(pubg_level3_bag)\nmore_kill = {\"7.76mm\":30 , \"scarl\":1 , \"5.56mm\":30}\npubg_level3_bag.update(more_kill)\nprint(pubg_level3_bag)\n\n\nCounter({'7.76mm': 60, 'kar98': 1})\nCounter({'7.76mm': 90, '5.56mm': 30, 'kar98': 1, 'scarl': 1})\n\n\n\n\nCode\n# don't remove element from the front of a list in python use instead deque\nfrom collections import deque\n\n\n\n\nCode\n# for Datastructure with locking functionality use queue module in python\n\n\n\n\nCode\n# how to check if the data structure is iterable\nfrom collections import Iterable\nisinstance([1,2,3] , Iterable)\n\n\nTrue\n\n\n\n\nRefer these To be GREAT IN PYTHON\n\nPython Doc\n\n\nReal Python\n\n\nYoutube PyData\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Machine Learning/model_in_production.html",
    "href": "posts/Machine Learning/model_in_production.html",
    "title": "Data and Model Management | Model Monitoring and Logging.",
    "section": "",
    "text": "Whenever a ML model goes to production there are lot many things which has to be take care starting from,\nHere we assume that the model is already in production using some sort of deployment strategy, except how to deploy we talk about every other thing. For Deployment a seperate blog has to be written."
  },
  {
    "objectID": "posts/Machine Learning/model_in_production.html#code",
    "href": "posts/Machine Learning/model_in_production.html#code",
    "title": "Data and Model Management | Model Monitoring and Logging.",
    "section": "Code",
    "text": "Code\nWithout a single doubt every programmer knows why we should track our code and to be very precise , Tracking means having knowledge of what changed in the code at what time and by which person and how can we retrieve those changes. The hero of the code tracking is git. and various places where we can keep those changes on the cloud can be Github, Gitlab etc."
  },
  {
    "objectID": "posts/Machine Learning/model_in_production.html#data",
    "href": "posts/Machine Learning/model_in_production.html#data",
    "title": "Data and Model Management | Model Monitoring and Logging.",
    "section": "Data",
    "text": "Data\nIn the ML world we are dealing with lots and lots of data where the nature of the received data also sometime it is a streaming data sometimes new data is received in batches and other time may be like a single blob, In the context of ML experiements where with every new experiments we might be generating new data columns doing some different feature processing or with time in the same experiment adding new features.\nAlso when were are putting our models to production we keep updating our models and corresponding Data and feature from one version to another.\nAll of these different scenario asks for some way of tracking the changes in the data.\nWorst way of doing it is put it in git which is okay if data is of small size couple of MBs how about couple of GBs or TBs we need a sophisticated library for this.\nLet me introduce DVC (Data Version Control) dvc integrates very nicely with git and works in tandem and will be used to track changes in the data and storing in some sort of file server it can Amazon s3, Azure storage, GCP, or any self hosted place.\nDVC makes Data Management simplified and helps us maintain a Data Registry.\n\nIn general we see DVC can help us track any type of file or folder, it a generic tool for tracking everything apart from code, so using DVC we can not only track data but also our saved model and can maintain a Model Registry.\nWorking of DVC is quite simple also.\n\nIt uses similar flow as git and is a completely command line tool like git.\nWhenever we ask dvc to track a particular file it tracks it and creates a .dvc metafile. For example 20gb_finance_data.csv when tracked using dvc it will generate 20gb_finance_data.dvc\nNow this .dvc file has to be tracked using git.\nWhen ever at any point of time we checkout a git commit, with code this metafile will also come out and then using dvc we checkout the corresponding data for the .dvc file.\n\n\nso if we see we have not only versioned and tracked our data but also our models, from tracking models I mean tracking weights of a particular model"
  },
  {
    "objectID": "posts/Machine Learning/model_in_production.html#configuration-management-and-validation",
    "href": "posts/Machine Learning/model_in_production.html#configuration-management-and-validation",
    "title": "Data and Model Management | Model Monitoring and Logging.",
    "section": "Configuration Management and Validation",
    "text": "Configuration Management and Validation\nIn a production environment lot of values to program comes from the run time and these key value pair has to be stored some where various ways people store it are\n\nPut it in a regular file\nStore it in some database\nStore it in on cloud behind firewalls\nuse tools like Consul, Apache Zookeeper etc\n\nand every choice comes with its on pros and cons.\nUsually JSON or YAML is used for storing config files, YAML being more feature rich.\nPython comes with ConfigParser class which implements a basic configuration language which provides a structure similar to what’s found in Microsoft Windows INI files. You can use this to write Python programs which can be customized by end users easily.\nOne of the most feature rich config management though not much needed in production setting more useful when doing HyperParameter search or Experimenting with different configuration is OmegaConf. It is a hierarchical configuration system, with support for merging configurations from multiple sources (YAML config files, dataclasses/objects and CLI arguments) providing a consistent API regardless of how the configuration was created.\nAn important need of config management is config value validation in run time and there is no better library than Pydantic that is the most widely used data validation library for Python."
  },
  {
    "objectID": "posts/Machine Learning/model_in_production.html#model-monitoring",
    "href": "posts/Machine Learning/model_in_production.html#model-monitoring",
    "title": "Data and Model Management | Model Monitoring and Logging.",
    "section": "Model Monitoring",
    "text": "Model Monitoring\nIn terms of Model monitoring there are three aspects which needs to be measured all the time, and special emphasis has to be given to create a Feedback loop to collect user feedback and use it for model improvement.\n\nMeasuring the Resource Cosumption.\n\nAmount of resource consumption (CPU, GPU, Storage)\nSpike in model usage that leads to increase in nodes a.k.a Auto Scaling\nLatency and Throughput\n\nMeasuring the Models Prediction quality\n\nData Drift\nModel Prediction Quality\n\nMeasuring Model Meta Data\n\nFrequency of the training\nModel Versioning\n\n\nData points described in 1 and 3 is of time series nature hence we need a mechanism to store the data in a DB and then show it in a dashboard.\nTIG (Telegraf-InfluxDb-Grafana) comes to the resuce.\n\nTelegraf -&gt; Mechanism to send data to influxdb on specific time intervals. In python we can use Statsd library to send data.\nInflux DB -&gt; Stores the data.\nGrafana -&gt; Helps to visualize data stored in Influx DB.\nThe same can be done with Prometheus and Grafana\n\nPrometheus Server has exporters which pull metrics from different sources and stores it locally which can be then queried using PromQL. Grafana uses PromQL to show the data.\nTelegraf with InfluxDB provides flexibility in data collection and storage, making it well-suited for scalable, long-term retention of metrics data.\nWhen it comes to measuring Data Drift and Model prediction Quality, it is bit different.\nBased on the nature of the problem, Data Drift models that can captures Outliers, Anomalies has to be placed. Model selection is highly dependent on the business, here TIG can be used to measure things.\nIn terms Model predictive quality there is needs to be some ground truth against which we can measure those changes but it unlikely that there will be something like that present, given there is also problem of data drift. What we can do is measure the drop i.e drop in engagment, user count, purchase etc. These events can be measured by paid tools like Amplitude, Google Analytics, Firebase or with some custom solution."
  },
  {
    "objectID": "posts/Machine Learning/model_in_production.html#logging-and-alerts",
    "href": "posts/Machine Learning/model_in_production.html#logging-and-alerts",
    "title": "Data and Model Management | Model Monitoring and Logging.",
    "section": "Logging and Alerts",
    "text": "Logging and Alerts\nLogging is a necessity of production.\nPython comes with a feature rich way of generating logs in Logging Module and only in depth tutorial to know everything Logging How To and Logging Cookbook.\nWhen there are multiple services from where logs get generated, There needs to be way to efficiently collecting, aggregating and analyse that data.\nThere are multiple ways of doing it. I am listing a few here\n\nELK stack with an extra Filebeat\nAWS Cloudwatch\nApache Flume\nFluentd\n\nELK stack (Elastic Search , Log Stash, Kibana) -&gt; When the web servers generate logs those logs will get read by Log Stash and we can read those logs from Log Stash directly, but to take a step further we can filter and process those logs using Log Stash Grok and make the unstructed generate log into a structured Log which can be then stored in Elastic Search and visualize on Kibana.\nFilebeat uses a backpressure-sensitive protocol when sending data to Logstash or Elasticsearch to account for higher volumes of data. If Logstash is busy crunching data, it lets Filebeat know to slow down its read. Once the congestion is resolved, Filebeat will build back up to its original pace and keep on shippin’.\nIt is very common to using Elastic search for this purpose apart from using search tool.\n\n\nSame can be done using AWS Cloudwatch, if you entire infrastructure is on AWS. Amazon CloudWatch collects and visualizes real-time logs, metrics, and event data in automated dashboards to streamline your infrastructure and application maintenance.\n\nNow whether to go with ELK stack or fluentd or Cloudwatch or any other tool depends completely on the scale the logs will be generated, infrastructure needs, affordability, value generated out of those logs.\nAs an important measure it is not advised to delete old logs instantly rather archive it on Amazon S3 and then delete after certain amount of time pases.\nWhether we are logging or monitoring every other tools has a built in support for alerts and these alerts has to be generated based on certain criteria the frequency or time till the alerts has to be checked all of them can be customized. Grafana, Kibana, Cloudwatch, Fluentd all of them supports it. We can send alerts to email, slack, discord, sms there are tons of plugin which gives out of the box support for every other communication channel."
  },
  {
    "objectID": "posts/Machine Learning/model_in_production.html#thank-you",
    "href": "posts/Machine Learning/model_in_production.html#thank-you",
    "title": "Data and Model Management | Model Monitoring and Logging.",
    "section": "Thank you",
    "text": "Thank you"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html",
    "href": "posts/Machine Learning/metrics.html",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Example\nLet’s take an example of Binary classification where the task is to predict whether we have Dog or not. So in an Image if model predicts Dog that’s a positive class if it predicts no Dog that’s a negative class.\n\n\nA confusion matrix is a table that is used to describe the performance of a classification model.\n\nLet’s consider this example table where N denotes the total number of images our model will predict upon.\nN = 150 Total Number of Dog Images = 100 Total Number of No Dog Images = 50\nIn total 100 Dog Images 1. Model Predicted 60 Correct which is your True Positive. 2. Model Predicted 40 incorrect which is False Negative.(Since It has to predict Dog and It Predicted No Dog which is a False prediction to a Negative class.)\nSimilarly in 50 No Dog Images 1. Model Predicted 30 Correct which is True Negative. 2. Model Predicted 20 Incorrect which is False Positive. (Since It has to predict No Dog and It predicted Dog which is a False prediction to a Positive class.)\nTP-&gt;60 FN-&gt;40 TN-&gt;30 FP-&gt;20\n\n\n\nAccuracy is a basic metric which just tells models overall performance. How many predictions made by model is correct.\n\\[\nAccuracy = \\frac{True Positive + True Negative}{N}\n\\]\n\\[\nAccuracy = \\frac{60+30}{150} = 0.6 = 60\n\\]\nAccuracy gave us a idea about how 60 % prediction was correct. Accuracy can only be a good metric. if all the classes are balanced i.e No of positive sample is approximately equal to No of negative samples. Per class Accuracy can also be calculated to know for which classes model is behaving well.\n\n\n\nMisclassification Rate tells overall how poor model performance is. It just opposite of Accuracy.\n\\[\nmisclassification\\ rate = 1 - Accuracy\n\\]\n\\[\nmisclassifcation\\ rate = \\frac{False Postive + False Negative}{N}\n\\]\n\\[\nmisclassification\\ rate = \\frac{20+40}{150} = 0.4 = 40\\%\n\\]\n\n\n\nPrecision is another metric which tells while predicting how accurately can I predict positive classes .\n\\[\nPositive\\ class\\ prediction  = True\\ positive + False\\ Positive\n\\]\n\\[\nprecision = \\frac{True\\ Positive}{positive\\ class\\ prediction}\n\\]\n\\[\nprecision = \\frac {60}{60 + 20} = 0.75 = 75\\%\n\\]\nWe are only worried about the prediction of one class which is Dog and the positive prediction class will have (Positive class, Negative class) from Actuals. That’s why it is positive predictive value.\nLet’s consider two example where\n\nSpam detection -&gt; Classify whether an email is Spam or Not Spam.\nHere the goal is to accurately classify spam emails. It’s okay to classify a Spam mail as Not Spam mail as it will come in our inbox it does no harm to use. But if we classify as Not Spam Mail as Spam mail then there is a problem because we generally do not open our Spam Box.\nIf you think about it the first case is False Negative and Second case is False Positive and we are okay with False Negative’s but we are not okay with False Positives and our goal is to reduce False Positive.\nSo in Spam detection task precision is a good metric. since it is inversely proportionate to False Positives.\nCancer detection -&gt; Classify whether a person has a cancer or not.\nHere the goal is to accurately classify whether a person has a cancer or Not. It’s okay to classify a person Not having cancer as cancer. But it’s not okay to predict a person having cancer as Not cancer.\nIf you think the first case is False Positive and Second case is False Negative and we are okay with False Positive but not okay with False Negative.\nHence in this particular task Precision plays no role.\n\nHence to reduce False Positives Precision is used. Precision can easily be effected by class Imbalance.\n\n\n\nNegative Predictive Value is another metric which tells while predicting how accurately can I predict Negative classes .\n\\[\nNegative\\ class\\ prediction  = True\\ Negative + False\\ Negative\n\\]\n\\[\nNegative\\ Prediction\\ Value = \\frac{True\\ Negative}{Negative\\ class\\ prediction}\n\\]\n\\[\nnegative\\ prediction\\ value = \\frac {60}{60 + 20} = 0.75 = 75\\%\n\\]\nWe are only worried about the prediction of one class which is Dog and the positive prediction class will have (Positive class, Negative class) from Actuals. That’s why it is positive predictive value.\nLet Suppose we don’t want to have any additional process for screening patients checked as healthy (not cancer) then we may want to make sure that our negative predictive value is high.\n\n\n\nRecall is another metric which tells us while predicting how accurately can it predict positive classes given a set of Actual Positives.\n\\[\nActual\\ positive\\ class = True\\ Positive + False\\ Negative\n\\]\n\\[\nrecall = \\frac{True\\ Positive}{Actual\\ positive\\ class}\n\\]\n\\[\nrecall = \\frac{60}{100} = 0.6 = 60\\%\n\\]\nHere our concern is about Given a set of Positive Samples which is like giving all the Dog images and then making prediction on it. The prediction will have (positive class , Negative class ) from Actual positives. That’s why recall is also True Positive Rate .\nThe reason why it’s called recall is given all the positive sample knowledge how well the model can recall that knowledge to predicted accurately by decreasing the error rate for the Actual positive class.\nGoing back to the Cancer Example it is very clear know that for Cancer Detection we will use Recall as our metric. Recall is good metric to be used for class Imbalance problem.\n\n\n\nSimilar to True positive rate, True Negative rate tells us while predicting how accurately can it predict Negative classes given a set of Actual Negatives.\n\\[\nActual\\ negative\\ class = True\\ Negative + False\\ Positive\n\\]\n\\[\nTrue\\ Negative\\ Rate = \\frac{True\\ Negative}{Actual\\ negative\\ class}\n\\]\n\\[\nTrue\\ Negative\\ Rate = \\frac{60}{100} = 0.6 = 60\\%\n\\]\nHere our concern is about Given a set of Negative Samples which is like giving all the No Dog images and then making prediction on it. The prediction will have (positive class , Negative class ) from Actual Negatives.\nFor the same cancer example True Negative rate will show how many non cancer people are identified as not having cancer.\n\n\n\n\nn many tests, including diagnostic medical tests, sensitivity is the extent to which actual positives are not overlooked (so false negatives are few), and specificity is the extent to which actual negatives are classified as such (so false positives are few). Thus, a highly sensitive test rarely overlooks an actual positive (for example, showing “nothing bad” despite something bad existing); a highly specific test rarely registers a positive classification for anything that is not the target of testing (for example, finding one bacterial species and mistaking it for another closely related one that is the true target); and a test that is highly sensitive and highly specific does both, so it “rarely overlooks a thing that it is looking for” and it “rarely mistakes anything else for that thing.” Because most medical tests do not have sensitivity and specificity values above 99%, “rarely” does not equate to certainty. But for practical reasons, tests with sensitivity and specificity values above 90% have high credibility, albeit usually no certainty, in differential diagnosis.\nSensitivity, therefore, quantifies the avoidance of false negatives and specificity does the same for false positives.\n\n\n\nWhen the model predicts something Falsely to the positive class which then it is contributing to the False Positive rate. we can think of it as False alert. For example if in a production house based on certain machine parameters the model has to predict whether the situation insider the production house is dangerous or not and it has to raise alarm if its dangerous. Now if everything is fine and still the model predicts as dangerous situation then that’s a False alarm which you can say a False Positive Rate.\nFalse Positive Rate is just opposite of True Negative Rate\n\\[\nActual\\ negative\\ class = True\\ Negative + False\\ Positive\n\\]\n\\[\nFalse\\ positive\\ Rate = \\frac{False\\ Positive}{Actual\\ negative\\ class}\n\\]\n\\[\nFalse\\ Positive\\ Rate = 1 - True\\ Negative\\ Rate\n\\]\nThe lower the False Positive Rate the better the model.\n\n\n\nWhen the model doesn’t predict something which it should then it is contributing to the False Negative Rate. We can think it as Miss Rate. For example in Online fraud transaction if the model classifies a Fraud Transaction as a Non Fraud Transaction then the model basically missed to catch that Fraud transaction.\nFalse Negative Rate is just of True Positive Rate\n\\[\nActual\\ positive\\ class = True\\ Positive + False\\ Negative\n\\]\n\\[\nFalse\\ Negative\\ Rate = \\frac{False\\ Negative}{Actual\\ positive\\ class}\n\\]\n\\[\nFalse\\ Negative\\ Rate = 1 - True\\ Positive\\ Rate\n\\]\n\n\n\nFalse Discovery Rate is just opposite of Precision It measures how many predictions out of all positive predictions were incorrect.\n\\[\nPositive\\ class\\ prediction  = True\\ positive + False\\ Positive\n\\]\n\\[\nFalse\\ Discovery\\ Rate = \\frac{False\\ Positive}{positive\\ class\\ prediction}\n\\]\n\\[\nFalse\\ Discovery\\ Rate = 1 - Precision\n\\]\nWhen raising False alert is expensive it is worth looking every Positive prediction then we should optimize for precision.\n\n\n\nFalse Omission Rate is just opposite of Negative Predictive Value\n\\[\nFalse\\ Omission\\ Rate = 1 - Negative\\ Predictive\\ Value\n\\]\n\n\n\nNow that two important metric which is used often is precision and recall and rather then having too look two number F1 score combines precision and recall.\nThe score lies in the range [0,1] with 1 being ideal and 0 being the worst. The two ways to combine Precision and recall is\n\\[\nArithmetic\\ Mean\\\\\nF1\\ score = \\frac{precision + recall}{2}\n\\]\n\\[\nHarmonic\\ Mean\\\\\nF1\\ Score = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}}\n\\]\nThe reason to choose Harmonic mean over Arithmetic mean is precision and recall both have same numerator but different denominators so it makes no sense to average two different things as, fractions are more sensible to average by arithmetic mean when they have the same denominator. Rather we take reciprocal so that the average makes sense.\n\n\n\nIt’s a metric that combines precision and recall, putting 2x emphasis on recall.\n\\[\nF2\\ score = \\frac{1+2}{\\frac{2}{precision} + \\frac{1}{recall}}\n\\]\n\n\n\nF beta score is a general formula for F1 score and F2 score\nWhen choosing beta in your F-beta score the more you care about recall over precision the higher beta you should choose. For example, with F1 score we care equally about recall and precision with F2 score, recall is twice as important to us.\nWith 0&lt;beta&lt;1 we care more about precision\n\\[\nF beta \\ score = \\frac{1+\\beta}{\\frac{\\beta}{precision} + \\frac{1}{recall}}\n\\]\n\n\n\nmicro\nCalculate metrics globally by counting the total number of times each class was correctly predicted and incorrectly predicted. Micro Average captures class-imbalance and will bring down the precision\nFor example in Iris Dataset the model prediction result is given in the table\n\n\n\n\nTP\nFP\n\n\n\n\nSetosa\n45\n5\n\n\nVirgnica\n10\n60\n\n\nVersicolor\n40\n10\n\n\n\n\\[\nmicro\\ precision = \\frac{45 + 10 + 40}{45+ 10 + 40 + 5+60+10 }  = 0.55\n\\]\nmacro\nCalculate metrics for each “class” independently, and find their unweighted mean. This does not take label imbalance into account. In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class.\n\\[\nSetosa\\ precision = \\frac{45}{45+5} =0.9\\\\\nvirgnica\\ precision = \\frac{10}{10 + 60} =0.14\\\\\nversicolor\\ precision = \\frac{40}{40+10} = 0.8\\\\\n\\]\n\\[\nMacro\\ Precision = \\frac{0.9+0.14+0.8}{3} = 0.613\n\\]\nweighted accounts for class imbalance by computing the average of binary metrics in which each class’s score is weighted by its presence in the true data sample.\n\n\n\nPrecision-Recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.\nThe precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\nA system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly.\n\n\n\n\nA receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.\nThe top left corner of the plot is the “ideal” point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better.\nThe “steepness” of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate.\nIt can also be used for Mutli label classification problem.\n\n\n\n\nThe function cohen_kappa_score computes Cohen’s kappa statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.\nThe kappa score is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).\nKappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators.\nFor Kappa score formulae and calculation refer Cohen’s kappa\n\n\n\nThe Hamming loss is the fraction of labels that are incorrectly predicted.\nEvaluation metrics for multi-label classification performance are inherently different from those used in multi-class (or binary) classification, due to the inherent differences of the classification problem. If T denotes the true set of labels for a given sample, and P the predicted set of labels, then the following metrics can be defined on that sample:\nHamming loss: the fraction of the wrong labels to the total number of labels, i.e.\n\\[\nhamming\\ loss  = {\\frac {1}{|N|\\ . |L|}}\\sum_{i=1}^{|N|}\\sum_{j=1}^{|L|}xor (y_{i,j},z_{i,j})\n\\]\nwhere y_ij target and z_ij is the prediction. This is a loss function, so the optimal value is zero.\nHamming Loss computes Hamming distance and In information theory, the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other. In a more general context, the Hamming distance is one of several string metrics for measuring the edit distance between two sequences. It is named after the American mathematician Richard Hamming.\n\n\n\nTill Now For Binary Classification Problem we haven’t encountered any metric which incorporates all 4 parts of the confusion matrix and works good either we have balanced dataset or a Imbalanced one.\nMatthews Correlation Coefficient is the answer It is a more reliable statistical rate which produces high score only if the prediction obtained good results in all 4 parts of the confusion matrix.\nIt computes correlation coefficient between the true class and the predicted class the higher the correlation coefficient the better the model is at prediction.\nThe MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient.\n\\[\nMCC = \\frac{TP \\times TN - FP \\times FN }{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n\\]\nIf there is no False prediction, then the model has +1 as a correlation coefficient since (FP x FN = 0) vice-versa if (TP x TN = 0) then the model has -1 as a correlation coefficient.\nAdvantages of MCC over accuracy and F1 score\n\n\n\nAP summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight:\n\\[\nAP = \\sum_n{(R_n-R_{n-1}) P_n}\n\\]\nwhere Pn and Rn denotes the nth threshold.\nThis metric is also used in Object Detection.\n\nIntution Behind Average Precision\nWikipedia Average Precision\n\n\n\n\nBalanced Accuracy is metric used to deal with Imbalanced dataset. It is the average of Sensitivity and Specificity . In more generic term averaging recall of all classes.\nSensitivity covers the True Positive part and Specificity covers True Negative Part.\n\\[\nBalanced\\ Accuracy = \\frac{sensitivity + specificity}{2}\n\\]\n\n\n\nIn an ideal model, the probability scores of all true 1’s should be greater than the probability scores of ALL true 0’s. Such a model is said to be perfectly concordant and this phenomenon can be measured by Concordance and Discordance.\nSo how to calculate Concordance?\nLet’s consider the following 4 observation’s actual class and predicted probability scores.\n\n\n\nPatient No\nTrue Class\nProbability Score\n\n\n\n\nP1\n1\n0.9\n\n\nP2\n0\n0.42\n\n\nP3\n1\n0.30\n\n\nP4\n1\n0.80\n\n\n\nFrom the above 4 observations, there are 3 possible pairs of 1’s and 0’s. That is, P1-P2, P3-P2 and P4-P2.\nA pair is said to be concordant if the probability score of True 1 is greater than the probability score of True 0.\nP1-P2 =&gt; 0.9 &gt; 0.42 =&gt; Concordant! P3-P2 =&gt; 0.3 &lt; 0.42 =&gt; Discordant! P4-P2 =&gt; 0.8 &gt; 0.42 =&gt; Concordant!\nOut of the 3 pairs, only 2 are concordant. So, the concordance is 2/3 = 0.66 and discordance is 1 - 0.66 = 0.33.\nIn simpler words, we take all possible combinations of true events and non-events. Concordance is the percentage of pairs, where true event’s probability scores are greater than the scores of true non-events.\nFor a perfect model, this will be 100%. So, the higher the concordance, the better is the quality of the model."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#confusion-matrix",
    "href": "posts/Machine Learning/metrics.html#confusion-matrix",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "A confusion matrix is a table that is used to describe the performance of a classification model.\n\nLet’s consider this example table where N denotes the total number of images our model will predict upon.\nN = 150 Total Number of Dog Images = 100 Total Number of No Dog Images = 50\nIn total 100 Dog Images 1. Model Predicted 60 Correct which is your True Positive. 2. Model Predicted 40 incorrect which is False Negative.(Since It has to predict Dog and It Predicted No Dog which is a False prediction to a Negative class.)\nSimilarly in 50 No Dog Images 1. Model Predicted 30 Correct which is True Negative. 2. Model Predicted 20 Incorrect which is False Positive. (Since It has to predict No Dog and It predicted Dog which is a False prediction to a Positive class.)\nTP-&gt;60 FN-&gt;40 TN-&gt;30 FP-&gt;20"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#accuracy",
    "href": "posts/Machine Learning/metrics.html#accuracy",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Accuracy is a basic metric which just tells models overall performance. How many predictions made by model is correct.\n\\[\nAccuracy = \\frac{True Positive + True Negative}{N}\n\\]\n\\[\nAccuracy = \\frac{60+30}{150} = 0.6 = 60\n\\]\nAccuracy gave us a idea about how 60 % prediction was correct. Accuracy can only be a good metric. if all the classes are balanced i.e No of positive sample is approximately equal to No of negative samples. Per class Accuracy can also be calculated to know for which classes model is behaving well."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#misclassification-rate",
    "href": "posts/Machine Learning/metrics.html#misclassification-rate",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Misclassification Rate tells overall how poor model performance is. It just opposite of Accuracy.\n\\[\nmisclassification\\ rate = 1 - Accuracy\n\\]\n\\[\nmisclassifcation\\ rate = \\frac{False Postive + False Negative}{N}\n\\]\n\\[\nmisclassification\\ rate = \\frac{20+40}{150} = 0.4 = 40\\%\n\\]"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#precision-positive-predictive-value",
    "href": "posts/Machine Learning/metrics.html#precision-positive-predictive-value",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Precision is another metric which tells while predicting how accurately can I predict positive classes .\n\\[\nPositive\\ class\\ prediction  = True\\ positive + False\\ Positive\n\\]\n\\[\nprecision = \\frac{True\\ Positive}{positive\\ class\\ prediction}\n\\]\n\\[\nprecision = \\frac {60}{60 + 20} = 0.75 = 75\\%\n\\]\nWe are only worried about the prediction of one class which is Dog and the positive prediction class will have (Positive class, Negative class) from Actuals. That’s why it is positive predictive value.\nLet’s consider two example where\n\nSpam detection -&gt; Classify whether an email is Spam or Not Spam.\nHere the goal is to accurately classify spam emails. It’s okay to classify a Spam mail as Not Spam mail as it will come in our inbox it does no harm to use. But if we classify as Not Spam Mail as Spam mail then there is a problem because we generally do not open our Spam Box.\nIf you think about it the first case is False Negative and Second case is False Positive and we are okay with False Negative’s but we are not okay with False Positives and our goal is to reduce False Positive.\nSo in Spam detection task precision is a good metric. since it is inversely proportionate to False Positives.\nCancer detection -&gt; Classify whether a person has a cancer or not.\nHere the goal is to accurately classify whether a person has a cancer or Not. It’s okay to classify a person Not having cancer as cancer. But it’s not okay to predict a person having cancer as Not cancer.\nIf you think the first case is False Positive and Second case is False Negative and we are okay with False Positive but not okay with False Negative.\nHence in this particular task Precision plays no role.\n\nHence to reduce False Positives Precision is used. Precision can easily be effected by class Imbalance."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#negative-predictive-value",
    "href": "posts/Machine Learning/metrics.html#negative-predictive-value",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Negative Predictive Value is another metric which tells while predicting how accurately can I predict Negative classes .\n\\[\nNegative\\ class\\ prediction  = True\\ Negative + False\\ Negative\n\\]\n\\[\nNegative\\ Prediction\\ Value = \\frac{True\\ Negative}{Negative\\ class\\ prediction}\n\\]\n\\[\nnegative\\ prediction\\ value = \\frac {60}{60 + 20} = 0.75 = 75\\%\n\\]\nWe are only worried about the prediction of one class which is Dog and the positive prediction class will have (Positive class, Negative class) from Actuals. That’s why it is positive predictive value.\nLet Suppose we don’t want to have any additional process for screening patients checked as healthy (not cancer) then we may want to make sure that our negative predictive value is high."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#recall-true-positive-rate-sensitivity",
    "href": "posts/Machine Learning/metrics.html#recall-true-positive-rate-sensitivity",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Recall is another metric which tells us while predicting how accurately can it predict positive classes given a set of Actual Positives.\n\\[\nActual\\ positive\\ class = True\\ Positive + False\\ Negative\n\\]\n\\[\nrecall = \\frac{True\\ Positive}{Actual\\ positive\\ class}\n\\]\n\\[\nrecall = \\frac{60}{100} = 0.6 = 60\\%\n\\]\nHere our concern is about Given a set of Positive Samples which is like giving all the Dog images and then making prediction on it. The prediction will have (positive class , Negative class ) from Actual positives. That’s why recall is also True Positive Rate .\nThe reason why it’s called recall is given all the positive sample knowledge how well the model can recall that knowledge to predicted accurately by decreasing the error rate for the Actual positive class.\nGoing back to the Cancer Example it is very clear know that for Cancer Detection we will use Recall as our metric. Recall is good metric to be used for class Imbalance problem."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#selectivity-true-negative-rate-specificity",
    "href": "posts/Machine Learning/metrics.html#selectivity-true-negative-rate-specificity",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Similar to True positive rate, True Negative rate tells us while predicting how accurately can it predict Negative classes given a set of Actual Negatives.\n\\[\nActual\\ negative\\ class = True\\ Negative + False\\ Positive\n\\]\n\\[\nTrue\\ Negative\\ Rate = \\frac{True\\ Negative}{Actual\\ negative\\ class}\n\\]\n\\[\nTrue\\ Negative\\ Rate = \\frac{60}{100} = 0.6 = 60\\%\n\\]\nHere our concern is about Given a set of Negative Samples which is like giving all the No Dog images and then making prediction on it. The prediction will have (positive class , Negative class ) from Actual Negatives.\nFor the same cancer example True Negative rate will show how many non cancer people are identified as not having cancer."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#sensitivity-vs-specificity",
    "href": "posts/Machine Learning/metrics.html#sensitivity-vs-specificity",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "n many tests, including diagnostic medical tests, sensitivity is the extent to which actual positives are not overlooked (so false negatives are few), and specificity is the extent to which actual negatives are classified as such (so false positives are few). Thus, a highly sensitive test rarely overlooks an actual positive (for example, showing “nothing bad” despite something bad existing); a highly specific test rarely registers a positive classification for anything that is not the target of testing (for example, finding one bacterial species and mistaking it for another closely related one that is the true target); and a test that is highly sensitive and highly specific does both, so it “rarely overlooks a thing that it is looking for” and it “rarely mistakes anything else for that thing.” Because most medical tests do not have sensitivity and specificity values above 99%, “rarely” does not equate to certainty. But for practical reasons, tests with sensitivity and specificity values above 90% have high credibility, albeit usually no certainty, in differential diagnosis.\nSensitivity, therefore, quantifies the avoidance of false negatives and specificity does the same for false positives."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#false-positive-rate-type-i-error",
    "href": "posts/Machine Learning/metrics.html#false-positive-rate-type-i-error",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "When the model predicts something Falsely to the positive class which then it is contributing to the False Positive rate. we can think of it as False alert. For example if in a production house based on certain machine parameters the model has to predict whether the situation insider the production house is dangerous or not and it has to raise alarm if its dangerous. Now if everything is fine and still the model predicts as dangerous situation then that’s a False alarm which you can say a False Positive Rate.\nFalse Positive Rate is just opposite of True Negative Rate\n\\[\nActual\\ negative\\ class = True\\ Negative + False\\ Positive\n\\]\n\\[\nFalse\\ positive\\ Rate = \\frac{False\\ Positive}{Actual\\ negative\\ class}\n\\]\n\\[\nFalse\\ Positive\\ Rate = 1 - True\\ Negative\\ Rate\n\\]\nThe lower the False Positive Rate the better the model."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#false-negative-rate-type---ii-error",
    "href": "posts/Machine Learning/metrics.html#false-negative-rate-type---ii-error",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "When the model doesn’t predict something which it should then it is contributing to the False Negative Rate. We can think it as Miss Rate. For example in Online fraud transaction if the model classifies a Fraud Transaction as a Non Fraud Transaction then the model basically missed to catch that Fraud transaction.\nFalse Negative Rate is just of True Positive Rate\n\\[\nActual\\ positive\\ class = True\\ Positive + False\\ Negative\n\\]\n\\[\nFalse\\ Negative\\ Rate = \\frac{False\\ Negative}{Actual\\ positive\\ class}\n\\]\n\\[\nFalse\\ Negative\\ Rate = 1 - True\\ Positive\\ Rate\n\\]"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#false-discovery-rate",
    "href": "posts/Machine Learning/metrics.html#false-discovery-rate",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "False Discovery Rate is just opposite of Precision It measures how many predictions out of all positive predictions were incorrect.\n\\[\nPositive\\ class\\ prediction  = True\\ positive + False\\ Positive\n\\]\n\\[\nFalse\\ Discovery\\ Rate = \\frac{False\\ Positive}{positive\\ class\\ prediction}\n\\]\n\\[\nFalse\\ Discovery\\ Rate = 1 - Precision\n\\]\nWhen raising False alert is expensive it is worth looking every Positive prediction then we should optimize for precision."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#false-omission-rate",
    "href": "posts/Machine Learning/metrics.html#false-omission-rate",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "False Omission Rate is just opposite of Negative Predictive Value\n\\[\nFalse\\ Omission\\ Rate = 1 - Negative\\ Predictive\\ Value\n\\]"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#f-1-score-beta-1",
    "href": "posts/Machine Learning/metrics.html#f-1-score-beta-1",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Now that two important metric which is used often is precision and recall and rather then having too look two number F1 score combines precision and recall.\nThe score lies in the range [0,1] with 1 being ideal and 0 being the worst. The two ways to combine Precision and recall is\n\\[\nArithmetic\\ Mean\\\\\nF1\\ score = \\frac{precision + recall}{2}\n\\]\n\\[\nHarmonic\\ Mean\\\\\nF1\\ Score = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}}\n\\]\nThe reason to choose Harmonic mean over Arithmetic mean is precision and recall both have same numerator but different denominators so it makes no sense to average two different things as, fractions are more sensible to average by arithmetic mean when they have the same denominator. Rather we take reciprocal so that the average makes sense."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#f-2-score-beta-2",
    "href": "posts/Machine Learning/metrics.html#f-2-score-beta-2",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "It’s a metric that combines precision and recall, putting 2x emphasis on recall.\n\\[\nF2\\ score = \\frac{1+2}{\\frac{2}{precision} + \\frac{1}{recall}}\n\\]"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#f-beta-score",
    "href": "posts/Machine Learning/metrics.html#f-beta-score",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "F beta score is a general formula for F1 score and F2 score\nWhen choosing beta in your F-beta score the more you care about recall over precision the higher beta you should choose. For example, with F1 score we care equally about recall and precision with F2 score, recall is twice as important to us.\nWith 0&lt;beta&lt;1 we care more about precision\n\\[\nF beta \\ score = \\frac{1+\\beta}{\\frac{\\beta}{precision} + \\frac{1}{recall}}\n\\]"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#averaging-parameter",
    "href": "posts/Machine Learning/metrics.html#averaging-parameter",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "micro\nCalculate metrics globally by counting the total number of times each class was correctly predicted and incorrectly predicted. Micro Average captures class-imbalance and will bring down the precision\nFor example in Iris Dataset the model prediction result is given in the table\n\n\n\n\nTP\nFP\n\n\n\n\nSetosa\n45\n5\n\n\nVirgnica\n10\n60\n\n\nVersicolor\n40\n10\n\n\n\n\\[\nmicro\\ precision = \\frac{45 + 10 + 40}{45+ 10 + 40 + 5+60+10 }  = 0.55\n\\]\nmacro\nCalculate metrics for each “class” independently, and find their unweighted mean. This does not take label imbalance into account. In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class.\n\\[\nSetosa\\ precision = \\frac{45}{45+5} =0.9\\\\\nvirgnica\\ precision = \\frac{10}{10 + 60} =0.14\\\\\nversicolor\\ precision = \\frac{40}{40+10} = 0.8\\\\\n\\]\n\\[\nMacro\\ Precision = \\frac{0.9+0.14+0.8}{3} = 0.613\n\\]\nweighted accounts for class imbalance by computing the average of binary metrics in which each class’s score is weighted by its presence in the true data sample."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#precision-recall-curve",
    "href": "posts/Machine Learning/metrics.html#precision-recall-curve",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.\nThe precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\nA system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#roc-auc-curve",
    "href": "posts/Machine Learning/metrics.html#roc-auc-curve",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.\nThe top left corner of the plot is the “ideal” point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better.\nThe “steepness” of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate.\nIt can also be used for Mutli label classification problem."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#cohens-kappa",
    "href": "posts/Machine Learning/metrics.html#cohens-kappa",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "The function cohen_kappa_score computes Cohen’s kappa statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.\nThe kappa score is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).\nKappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators.\nFor Kappa score formulae and calculation refer Cohen’s kappa"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#hamming-loss",
    "href": "posts/Machine Learning/metrics.html#hamming-loss",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "The Hamming loss is the fraction of labels that are incorrectly predicted.\nEvaluation metrics for multi-label classification performance are inherently different from those used in multi-class (or binary) classification, due to the inherent differences of the classification problem. If T denotes the true set of labels for a given sample, and P the predicted set of labels, then the following metrics can be defined on that sample:\nHamming loss: the fraction of the wrong labels to the total number of labels, i.e.\n\\[\nhamming\\ loss  = {\\frac {1}{|N|\\ . |L|}}\\sum_{i=1}^{|N|}\\sum_{j=1}^{|L|}xor (y_{i,j},z_{i,j})\n\\]\nwhere y_ij target and z_ij is the prediction. This is a loss function, so the optimal value is zero.\nHamming Loss computes Hamming distance and In information theory, the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other. In a more general context, the Hamming distance is one of several string metrics for measuring the edit distance between two sequences. It is named after the American mathematician Richard Hamming."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#matthews-correlation-coefficient",
    "href": "posts/Machine Learning/metrics.html#matthews-correlation-coefficient",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Till Now For Binary Classification Problem we haven’t encountered any metric which incorporates all 4 parts of the confusion matrix and works good either we have balanced dataset or a Imbalanced one.\nMatthews Correlation Coefficient is the answer It is a more reliable statistical rate which produces high score only if the prediction obtained good results in all 4 parts of the confusion matrix.\nIt computes correlation coefficient between the true class and the predicted class the higher the correlation coefficient the better the model is at prediction.\nThe MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient.\n\\[\nMCC = \\frac{TP \\times TN - FP \\times FN }{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n\\]\nIf there is no False prediction, then the model has +1 as a correlation coefficient since (FP x FN = 0) vice-versa if (TP x TN = 0) then the model has -1 as a correlation coefficient.\nAdvantages of MCC over accuracy and F1 score"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#average-precision-score",
    "href": "posts/Machine Learning/metrics.html#average-precision-score",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "AP summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight:\n\\[\nAP = \\sum_n{(R_n-R_{n-1}) P_n}\n\\]\nwhere Pn and Rn denotes the nth threshold.\nThis metric is also used in Object Detection.\n\nIntution Behind Average Precision\nWikipedia Average Precision"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#balanced-accuracy",
    "href": "posts/Machine Learning/metrics.html#balanced-accuracy",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Balanced Accuracy is metric used to deal with Imbalanced dataset. It is the average of Sensitivity and Specificity . In more generic term averaging recall of all classes.\nSensitivity covers the True Positive part and Specificity covers True Negative Part.\n\\[\nBalanced\\ Accuracy = \\frac{sensitivity + specificity}{2}\n\\]"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#concordance-and-discordance",
    "href": "posts/Machine Learning/metrics.html#concordance-and-discordance",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "In an ideal model, the probability scores of all true 1’s should be greater than the probability scores of ALL true 0’s. Such a model is said to be perfectly concordant and this phenomenon can be measured by Concordance and Discordance.\nSo how to calculate Concordance?\nLet’s consider the following 4 observation’s actual class and predicted probability scores.\n\n\n\nPatient No\nTrue Class\nProbability Score\n\n\n\n\nP1\n1\n0.9\n\n\nP2\n0\n0.42\n\n\nP3\n1\n0.30\n\n\nP4\n1\n0.80\n\n\n\nFrom the above 4 observations, there are 3 possible pairs of 1’s and 0’s. That is, P1-P2, P3-P2 and P4-P2.\nA pair is said to be concordant if the probability score of True 1 is greater than the probability score of True 0.\nP1-P2 =&gt; 0.9 &gt; 0.42 =&gt; Concordant! P3-P2 =&gt; 0.3 &lt; 0.42 =&gt; Discordant! P4-P2 =&gt; 0.8 &gt; 0.42 =&gt; Concordant!\nOut of the 3 pairs, only 2 are concordant. So, the concordance is 2/3 = 0.66 and discordance is 1 - 0.66 = 0.33.\nIn simpler words, we take all possible combinations of true events and non-events. Concordance is the percentage of pairs, where true event’s probability scores are greater than the scores of true non-events.\nFor a perfect model, this will be 100%. So, the higher the concordance, the better is the quality of the model."
  },
  {
    "objectID": "posts/Machine Learning/Weight_Initializer.html",
    "href": "posts/Machine Learning/Weight_Initializer.html",
    "title": "Weight Intialization",
    "section": "",
    "text": "Weight and bias intialization is one of the important factor responsible for today’s state of the art algorithm.\nWeights intialization is done in random fashion but that randomness is to be tuned in various ways to get optimum result.\n\nPitfall: all zero initialization. Lets start with what we should not do. Note that we do not know what the final value of every weight should be in the trained network, but with proper data normalization it is reasonable to assume that approximately half of the weights will be positive and half of them will be negative. A reasonable-sounding idea then might be to set all the initial weights to zero, which we expect to be the “best guess” in expectation. This turns out to be a mistake, because if every neuron in the network computes the same output, then they will also all compute the same gradients during backpropagation and undergo the exact same parameter updates. In other words, there is no source of asymmetry between neurons if their weights are initialized to be the same.\n\n\nSmall random numbers. Therefore, we still want the weights to be very close to zero, but as we have argued above, not identically zero. As a solution, it is common to initialize the weights of the neurons to small numbers and refer to doing so as symmetry breaking. The idea is that the neurons are all random and unique in the beginning, so they will compute distinct updates and integrate themselves as diverse parts of the full network. The implementation for one weight matrix might look like W = 0.01* np.random.randn(D,H), where randn samples from a zero mean, unit standard deviation gaussian. With this formulation, every neuron’s weight vector is initialized as a random vector sampled from a multi-dimensional gaussian, so the neurons point in random direction in the input space. It is also possible to use small numbers drawn from a uniform distribution, but this seems to have relatively little impact on the final performance in practice.\n\n\nWarning: It’s not necessarily the case that smaller numbers will work strictly better. For example, a Neural Network layer that has very small weights will during backpropagation compute very small gradients on its data (since this gradient is proportional to the value of the weights). This could greatly diminish the “gradient signal” flowing backward through a network, and could become a concern for deep networks.\n\n\nBradley (2009) found that back-propagated gradients were smaller as one moves from the output layer towards the input layer, just after initialization. He studied networks with linear activation at each layer, finding that the variance of the back-propagated gradients decreases as we go back- wards in the network. We will also start by studying the linear regime.\n\n\nCalibrating the variances with 1/sqrt(n) Xavier/Glorot Initialization. One problem with the above suggestion is that the distribution of the outputs from a randomly initialized neuron has a variance that grows with the number of inputs. It turns out that we can normalize the variance of each neuron’s output to 1 by scaling its weight vector by the square root of its fan-in (i.e. its number of inputs). That is, the recommended heuristic is to initialize each neuron’s weight vector as: w = np.random.randn(n) / sqrt(n), where n is the number of its inputs. This ensures that all neurons in the network initially have approximately the same output distribution and empirically improves the rate of convergence.\n\n\nThe sketch of the derivation is as follows: Consider the inner product (s = _i^n w_i x_i) between the weights (w) and input (x), which gives the raw activation of a neuron before the non-linearity. We can examine the variance of (s):\n\n\\[\\begin{align}\n\\text{Var}(s) &= \\text{Var}(\\sum_i^n w_ix_i) \\\\\\\\\n&= \\sum_i^n \\text{Var}(w_ix_i) \\\\\\\\\n&= \\sum_i^n [E(w_i)]^2\\text{Var}(x_i) + E[(x_i)]^2\\text{Var}(w_i) + \\text{Var}(x_i)\\text{Var}(w_i) \\\\\\\\\n&= \\sum_i^n \\text{Var}(x_i)\\text{Var}(w_i) \\\\\\\\\n&= \\left( n \\text{Var}(w) \\right) \\text{Var}(x)\n\\end{align}\\]\n\nwhere in the first 2 steps we have used properties of variance. In third step we assumed zero mean inputs and weights, so (E[x_i] = E[w_i] = 0). Note that this is not generally the case: For example ReLU units will have a positive mean. In the last step we assumed that all (w_i, x_i) are identically distributed. From this derivation we can see that if we want (s) to have the same variance as all of its inputs (x), then during initialization we should make sure that the variance of every weight (w) is (1/n). And since ((aX) = a^2(X)) for a random variable (X) and a scalar (a), this implies that we should draw from unit gaussian and then scale it by (a = ), to make its variance (1/n). This gives the initialization w = np.random.randn(n) / sqrt(nin+nout).\n\n\nSparse initialization. Another way to address the uncalibrated variances problem is to set all weight matrices to zero, but to break symmetry every neuron is randomly connected (with weights sampled from a small gaussian as above) to a fixed number of neurons below it. A typical number of neurons to connect to may be as small as 10.\n\n\nInitializing the biases. It is possible and common to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights. For ReLU non-linearities, some people like to use small constant value such as 0.01 for all biases because this ensures that all ReLU units fire in the beginning and therefore obtain and propagate some gradient. However, it is not clear if this provides a consistent improvement (in fact some results seem to indicate that this performs worse) and it is more common to simply use 0 bias initialization.\n\n\nIn practice, the current recommendation is to use ReLU units and use the w = np.random.randn(n) * sqrt(2.0/n), as discussed in He et al..\n\nWhy is initialization essential to deep networks? It turns out that if you do it wrong, it can lead to exploding or vanishing weights and gradients.\n\nVanishing Gradient In deep nueral network if sigmoid neuron is used there will be a problem of vanishing gradient because the value of sigmoid is between 0 and 1 and derivative of sigmoid is \\[sigmoid(x)*(1-sigmoid(x))\\] and when gradient flows backward in the network it is multiplied by subsequent wights which decreases the gradient value so the gradient to the beginning layers is very close to zero that means there is no learning. Using ReLU solves this problem.\n\nExploding Gradient:- Vanishing gradient is not much of a problem but in the contrary Exploding gradient is something on which we can ponder. This problem occours when weight are intialized with huge number like 10 or 100 and input to the network is not normalized. In this scenario either of these three can happen.\n1. The model is unable to get traction on your training data (e.g. poor loss).\n2. The model is unstable, resulting in large changes in loss from update to update.\n3. The model loss goes to NaN during training.\nTo Solve this problem the solution given are: 1. Redesign the network In deep neural networks, exploding gradients may be addressed by redesigning the network to have fewer layers. There may also be some benefit in using a smaller batch size while training the network.\n\nWeight Regularizers Another approach, if exploding gradients are still occurring, is to check the size of network weights and apply a penalty to the networks loss function for large weight values.\n\nThis is called weight regularization and often an L1 (absolute weights) or an L2 (squared weights) penalty can be used.\n\nGradient Clipping Clip the gradient before updating the weights. Either the gradient can be clipped by using l2 norm or the easiest way to clip is by values. we define a threshold value with range [-limit ,limit] if the grad value exceeds the limit we replace it with the limit value\nPytorch Supports various popular intialization type which can seen here -&gt; https://pytorch.org/docs/stable/nn.init.html\nGradient and Weights can be tracked using TensorBoard distribution Tracking.\nRandom sampling using different distributions -&gt; https://pytorch.org/docs/stable/torch.html#in-place-random-sampling\n\n# Random sampling Techniques\n    torch.Tensor.bernoulli_() - in-place version of torch.bernoulli()\n\n    torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution\n\n    torch.Tensor.exponential_() - numbers drawn from the exponential distribution\n\n    torch.Tensor.geometric_() - elements drawn from the geometric distribution\n\n    torch.Tensor.log_normal_() - samples from the log-normal distribution\n\n    torch.Tensor.normal_() - in-place version of torch.normal()\n\n    torch.Tensor.random_() - numbers sampled from the discrete uniform distribution\n\n    torch.Tensor.uniform_() - numbers sampled from the continuous uniform distribution\n# one example \ntorch.randn(32,32).uniform_()\n\n\nCode\nfrom torch import nn\n\n# a simple network\nmodel=nn.Sequential(nn.Linear(2, 5),\n                         nn.ReLU(),)\nnext(model.parameters())\n\n\nParameter containing:\ntensor([[-0.4624,  0.1237],\n        [ 0.3361,  0.3213],\n        [ 0.0601, -0.5251],\n        [ 0.0932, -0.6454],\n        [-0.1530,  0.3163]], requires_grad=True)\n\n\n\n\nCode\n\n# initialization function, first checks the module type,\n# then applies the desired changes to the weights\ndef init_normal(m):\n    if type(m) == nn.Linear:\n        nn.init.uniform_(m.weight)\n    elif isinstance(m , nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight)\n\n# use the modules apply function to recursively apply the initialization to every submodule.\nmodel.apply(init_normal)\nnext(model.parameters())\n\n\nParameter containing:\ntensor([[0.9828, 0.0986],\n        [0.8059, 0.4303],\n        [0.2424, 0.9537],\n        [0.5189, 0.4018],\n        [0.1175, 0.8017]], requires_grad=True)\n\n\nWeight initialization happens once, when a model is created and before it trains. Having good initial weights can place the neural network close to the optimalsolution. This allows the neural network to come to the best solution quicker.\n\n\n\nTo see how different weights perform, we’ll test on the same dataset and neural network. That way, we know that any changes in model behavior are due to the weights and not any changing data or model structure. &gt; We’ll instantiate at least two of the same models, with different initial weights and see how the training loss decreases over time, such as in the example below.\n\nSometimes the differences in training loss, over time, will be large and other times, certain weights offer only small improvements.\n\n\nWe’ll train an MLP to classify images from the Fashion-MNIST database to demonstrate the effect of different initial weights. As a reminder, the FashionMNIST dataset contains images of clothing types; classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']. The images are normalized so that their pixel values are in a range [0.0 - 1.0). Run the cell below to download and load the dataset.\n\n\n\n\n\nCode\nimport torch\nimport numpy as np\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n# number of subprocesses to use for data loading\nnum_workers = 0\n# how many samples per batch to load\nbatch_size = 100\n# percentage of training set to use as validation\nvalid_size = 0.2\n\n# convert data to torch.FloatTensor\ntransform = transforms.ToTensor()\n\n# choose the training and test datasets\ntrain_data = datasets.FashionMNIST(root='data', train=True,\n                                   download=True, transform=transform)\ntest_data = datasets.FashionMNIST(root='data', train=False,\n                                  download=True, transform=transform)\n\n# obtain training indices that will be used for validation\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n# prepare data loaders (combine dataset and sampler)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n    sampler=train_sampler, num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n    sampler=valid_sampler, num_workers=num_workers)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n    num_workers=num_workers)\n\n# specify the image classes\nclasses = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n%matplotlib inline\n    \n# obtain one batch of training images\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nimages = images.numpy()\n\n# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(25, 4))\nfor idx in np.arange(20):\n    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n    ax.set_title(classes[labels[idx]])\n\n\n\n\n\n\n\n\n\nWe’ve defined the MLP that we’ll use for classifying the dataset.\n\n\n\n\nA 3 layer MLP with hidden dimensions of 256 and 128.\nThis MLP accepts a flattened image (784-value long vector) as input and produces 10 class scores as output. We’ll test the effect of different initial weights on this 3 layer neural network with ReLU activations and an Adam optimizer.\n\nThe lessons you learn apply to other neural networks, including different activations and optimizers.\n\n\n\n\nLet’s start looking at some initial weights. ### All Zeros or Ones If you follow the principle of Occam’s razor, you might think setting all the weights to 0 or 1 would be the best solution. This is not the case.\nWith every weight the same, all the neurons at each layer are producing the same output. This makes it hard to decide which weights to adjust.\nLet’s compare the loss with all ones and all zero weights by defining two models with those constant weights.\nBelow, we are using PyTorch’s nn.init to initialize each Linear layer with a constant weight. The init library provides a number of weight initialization functions that give you the ability to initialize the weights of each layer according to layer type.\nIn the case below, we look at every layer/module in our model. If it is a Linear layer (as all three layers are for this MLP), then we initialize those layer weights to be a constant_weight with bias=0 using the following code: &gt;if isinstance(m, nn.Linear):     nn.init.constant_(m.weight, constant_weight)     nn.init.constant_(m.bias, 0)\nThe constant_weight is a value that you can pass in when you instantiate the model.\n\n\nCode\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# define the NN architecture\nclass Net(nn.Module):\n    def __init__(self, hidden_1=256, hidden_2=128, constant_weight=None):\n        super(Net, self).__init__()\n        # linear layer (784 -&gt; hidden_1)\n        self.fc1 = nn.Linear(28 * 28, hidden_1)\n        # linear layer (hidden_1 -&gt; hidden_2)\n        self.fc2 = nn.Linear(hidden_1, hidden_2)\n        # linear layer (hidden_2 -&gt; 10)\n        self.fc3 = nn.Linear(hidden_2, 10)\n        # dropout layer (p=0.2)\n        self.dropout = nn.Dropout(0.2)\n        \n        # initialize the weights to a specified, constant value\n        if(constant_weight is not None):\n            for m in self.modules():\n                if isinstance(m, nn.Linear):\n                    nn.init.constant_(m.weight, constant_weight)\n                    nn.init.constant_(m.bias, 0)\n    \n            \n    def forward(self, x):\n        # flatten image input\n        x = x.view(-1, 28 * 28)\n        # add hidden layer, with relu activation function\n        x = F.relu(self.fc1(x))\n        # add dropout layer\n        x = self.dropout(x)\n        # add hidden layer, with relu activation function\n        x = F.relu(self.fc2(x))\n        # add dropout layer\n        x = self.dropout(x)\n        # add output layer\n        x = self.fc3(x)\n        return x\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim\n\n\ndef _get_loss_acc(model, train_loader, valid_loader):\n    \"\"\"\n    Get losses and validation accuracy of example neural network\n    \"\"\"\n    n_epochs = 2\n    learning_rate = 0.001\n    \n    # Training loss\n    criterion = nn.CrossEntropyLoss()\n\n    # Optimizer\n    optimizer = optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n\n    # Measurements used for graphing loss\n    loss_batch = []\n\n    for epoch in range(1, n_epochs+1):\n        # initialize var to monitor training loss\n        train_loss = 0.0\n        ###################\n        # train the model #\n        ###################\n        for data, target in train_loader:\n            # clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data)\n            # calculate the batch loss\n            loss = criterion(output, target)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            # record average batch loss \n            loss_batch.append(loss.item())\n             \n    # after training for 2 epochs, check validation accuracy \n    correct = 0\n    total = 0\n    for data, target in valid_loader:\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # get the predicted class from the maximum class score\n        _, predicted = torch.max(output.data, 1)\n        # count up total number of correct labels\n        # for which the predicted and true labels are equal\n\n\n\n\nBelow, we are using compare_init_weights to compare the training and validation loss for the two models we defined above, model_0 and model_1. This function takes in a list of models (each with different initial weights), the name of the plot to produce, and the training and validation dataset loaders. For each given model, it will plot the training loss for the first 100 batches and print out the validation accuracy after 2 training epochs. Note: if you’ve used a small batch_size, you may want to increase the number of epochs here to better compare how models behave after seeing a few hundred images.\nWe plot the loss over the first 100 batches to better judge which model weights performed better at the start of training.\nRun the cell below to see the difference between weights of all zeros against all ones.\n\n\nCode\n# initialize two NN's with 0 and 1 constant weights\nmodel_0 = Net(constant_weight=0)\nmodel_1 = Net(constant_weight=1)\n\n\n\n\nCode\n# put them in list form to compare\nmodel_list = [(model_0, 'All Zeros'),\n              (model_1, 'All Ones')]\n\n\n# plot the loss over the first 100 batches\ncompare_init_weights(model_list, \n                             'All Zeros vs All Ones', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n    9.675% -- All Zeros\n    9.675% -- All Ones\nTraining Loss\n    2.305  -- All Zeros\n  478.745  -- All Ones\n\n\nAs you can see the accuracy is close to guessing for both zeros and ones, around 10%.\nThe neural network is having a hard time determining which weights need to be changed, since the neurons have the same output for each layer. To avoid neurons with the same output, let’s use unique weights. We can also randomly select these weights to avoid being stuck in a local minimum for each run.\nA good solution for getting these random weights is to sample from a uniform distribution.\n\n\n\nA uniform distribution has the equal probability of picking any number from a set of numbers. We’ll be picking from a continuous distribution, so the chance of picking the same number is low. We’ll use NumPy’s np.random.uniform function to pick random numbers from a uniform distribution.\n\n\nOutputs random values from a uniform distribution.\n\n\nThe generated values follow a uniform distribution in the range [low, high). The lower bound minval is included in the range, while the upper bound maxval is excluded.\n\n\n\nlow: The lower bound on the range of random values to generate. Defaults to 0.\nhigh: The upper bound on the range of random values to generate. Defaults to 1.\nsize: An int or tuple of ints that specify the shape of the output array.\n\n\nWe can visualize the uniform distribution by using a histogram. Let’s map the values from np.random_uniform(-3, 3, [1000]) to a histogram using the hist_dist function. This will be 1000 random float values from -3 to 3, excluding the value 3.\n\n\nCode\nhist_dist('Random Uniform (low=-3, high=3)', np.random.uniform(-3, 3, [1000]))\n\n\n\n\n\nThe histogram used 500 buckets for the 1000 values. Since the chance for any single bucket is the same, there should be around 2 values for each bucket. That’s exactly what we see with the histogram. Some buckets have more and some have less, but they trend around 2.\nNow that you understand the uniform function, let’s use PyTorch’s nn.init to apply it to a model’s initial weights.\n\n\n\nLet’s see how well the neural network trains using a uniform weight initialization, where low=0.0 and high=1.0. Below, I’ll show you another way (besides in the Net class code) to initialize the weights of a network. To define weights outside of the model definition, you can: &gt;1. Define a function that assigns weights by the type of network layer, then 2. Apply those weights to an initialized model using model.apply(fn), which applies a function to each model layer.\nThis time, we’ll use weight.data.uniform_ to initialize the weights of our model, directly.\n\n\nCode\n# takes in a module and applies the specified weight initialization\ndef weights_init_uniform(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # apply a uniform distribution to the weights and a bias=0\n        m.weight.data.uniform_(0.0, 1.0)\n        m.bias.data.fill_(0)\n\n\n\n\nCode\n# create a new model with these weights\nmodel_uniform = Net()\nmodel_uniform.apply(weights_init_uniform)\n\n\nNet(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\n  (dropout): Dropout(p=0.2)\n)\n\n\n\n\nCode\n# evaluate behavior \ncompare_init_weights([(model_uniform, 'Uniform Weights')], \n                             'Uniform Baseline', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n   33.933% -- Uniform Weights\nTraining Loss\n    4.697  -- Uniform Weights\n\n\nThe loss graph is showing the neural network is learning, which it didn’t with all zeros or all ones. We’re headed in the right direction!\n\n\n\n\nThe general rule for setting the weights in a neural network is to set them to be close to zero without being too small. &gt;Good practice is to start your weights in the range of \\([-y, y]\\) where \\(y=1/\\sqrt{n}\\)\n(\\(n\\) is the number of inputs to a given neuron).\nLet’s see if this holds true; let’s create a baseline to compare with and center our uniform range over zero by shifting it over by 0.5. This will give us the range [-0.5, 0.5).\n\n\nCode\n# takes in a module and applies the specified weight initialization\ndef weights_init_uniform_center(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # apply a centered, uniform distribution to the weights\n        m.weight.data.uniform_(-0.5, 0.5)\n        m.bias.data.fill_(0)\n\n# create a new model with these weights\nmodel_centered = Net()\nmodel_centered.apply(weights_init_uniform_center)\n\n\nNet(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\n  (dropout): Dropout(p=0.2)\n)\n\n\nThen let’s create a distribution and model that uses the general rule for weight initialization; using the range \\([-y, y]\\), where \\(y=1/\\sqrt{n}\\) .\nAnd finally, we’ll compare the two models.\n\n\nCode\n# takes in a module and applies the specified weight initialization\ndef weights_init_uniform_rule(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # get the number of the inputs\n        n = m.in_features\n        y = 1.0/np.sqrt(n)\n        m.weight.data.uniform_(-y, y)\n        m.bias.data.fill_(0)\n\n# create a new model with these weights\nmodel_rule = Net()\nmodel_rule.apply(weights_init_uniform_rule)\n\n\nNet(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\n  (dropout): Dropout(p=0.2)\n)\n\n\n\n\nCode\n# compare these two models\nmodel_list = [(model_centered, 'Centered Weights [-0.5, 0.5)'), \n              (model_rule, 'General Rule [-y, y)')]\n\n# evaluate behavior \ncompare_init_weights(model_list, \n                             '[-0.5, 0.5) vs [-y, y)', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n   74.875% -- Centered Weights [-0.5, 0.5)\n   84.833% -- General Rule [-y, y)\nTraining Loss\n    0.789  -- Centered Weights [-0.5, 0.5)\n    0.487  -- General Rule [-y, y)\n\n\nThis behavior is really promising! Not only is the loss decreasing, but it seems to do so very quickly for our uniform weights that follow the general rule; after only two epochs we get a fairly high validation accuracy and this should give you some intuition for why starting out with the right initial weights can really help your training process!\n\nSince the uniform distribution has the same chance to pick any value in a range, what if we used a distribution that had a higher chance of picking numbers closer to 0? Let’s look at the normal distribution.\n\n\nUnlike the uniform distribution, the normal distribution has a higher likelihood of picking number close to it’s mean. To visualize it, let’s plot values from NumPy’s np.random.normal function to a histogram.\n\nnp.random.normal(loc=0.0, scale=1.0, size=None)\n\n\nOutputs random values from a normal distribution.\n\n\n\nloc: The mean of the normal distribution.\nscale: The standard deviation of the normal distribution.\nshape: The shape of the output array.\n\n\n\n\nCode\nhist_dist('Random Normal (mean=0.0, stddev=1.0)', np.random.normal(size=[1000]))\n\n\n\n\n\nLet’s compare the normal distribution against the previous, rule-based, uniform distribution.\nBelow, we define a normal distribution that has a mean of 0 and a standard deviation of \\(y=1/\\sqrt{n}\\).\n\n\nCode\n# takes in a module and applies the specified weight initialization\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # get the number of the inputs\n        n = m.in_features\n        y = (1.0/np.sqrt(n))\n        m.weight.data.normal_(0, y)\n        m.bias.data.fill_(0)\n\n\n\n\nCode\n# create a new model with the rule-based, uniform weights\nmodel_uniform_rule = Net()\nmodel_uniform_rule.apply(weights_init_uniform_rule)\n\n# create a new model with the rule-based, NORMAL weights\nmodel_normal_rule = Net()\nmodel_normal_rule.apply(weights_init_normal)\n\n\nNet(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\n  (dropout): Dropout(p=0.2)\n)\n\n\n\n\nCode\n# compare the two models\nmodel_list = [(model_uniform_rule, 'Uniform Rule [-y, y)'), \n              (model_normal_rule, 'Normal Distribution')]\n\n# evaluate behavior \ncompare_init_weights(model_list, \n                             'Uniform vs Normal', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n   85.442% -- Uniform Rule [-y, y)\n   85.233% -- Normal Distribution\nTraining Loss\n    0.318  -- Uniform Rule [-y, y)\n    0.333  -- Normal Distribution\n\n\nThe normal distribution gives us pretty similar behavior compared to the uniform distribution, in this case. This is likely because our network is so small; a larger neural network will pick more weight values from each of these distributions, magnifying the effect of both initialization styles. In general, a normal distribution will result in better performance for a model.\n\n\n\n\nLet’s quickly take a look at what happens without any explicit weight initialization.\n\n\nCode\nmodel_no_initialization = Net()\n\n\n\n\nCode\nmodel_list = [(model_no_initialization, 'No Weights')]\n\n# evaluate behavior \ncompare_init_weights(model_list, \n                             'No Weight Initialization', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n   84.458% -- No Weights\nTraining Loss\n    0.530  -- No Weights\n\n\n\n\n\nSomething really interesting is happening here. You may notice that the red line “no weights” looks a lot like our uniformly initialized weights. It turns out that PyTorch has default weight initialization behavior for every kind of layer. You can see that linear layers are initialized with a uniform distribution (uniform weights and biases) in the module source code.\nHowever, you can also see that the weights taken from a normal distribution are comparable, perhaps even a little better! So, it may still be useful, especially if you are trying to train the best models, to initialize the weights of a model according to rules that you define.\nAnd, this is not the end ! You’re encouraged to look at the different types of common initialization distributions.\n\n\n\n\nhttp://cs231n.github.io/neural-networks-2/#init\nhttps://github.com/udacity/deep-learning"
  },
  {
    "objectID": "posts/Machine Learning/Weight_Initializer.html#initial-weights-and-observing-training-loss",
    "href": "posts/Machine Learning/Weight_Initializer.html#initial-weights-and-observing-training-loss",
    "title": "Weight Intialization",
    "section": "",
    "text": "To see how different weights perform, we’ll test on the same dataset and neural network. That way, we know that any changes in model behavior are due to the weights and not any changing data or model structure. &gt; We’ll instantiate at least two of the same models, with different initial weights and see how the training loss decreases over time, such as in the example below.\n\nSometimes the differences in training loss, over time, will be large and other times, certain weights offer only small improvements.\n\n\nWe’ll train an MLP to classify images from the Fashion-MNIST database to demonstrate the effect of different initial weights. As a reminder, the FashionMNIST dataset contains images of clothing types; classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']. The images are normalized so that their pixel values are in a range [0.0 - 1.0). Run the cell below to download and load the dataset.\n\n\n\n\n\nCode\nimport torch\nimport numpy as np\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n# number of subprocesses to use for data loading\nnum_workers = 0\n# how many samples per batch to load\nbatch_size = 100\n# percentage of training set to use as validation\nvalid_size = 0.2\n\n# convert data to torch.FloatTensor\ntransform = transforms.ToTensor()\n\n# choose the training and test datasets\ntrain_data = datasets.FashionMNIST(root='data', train=True,\n                                   download=True, transform=transform)\ntest_data = datasets.FashionMNIST(root='data', train=False,\n                                  download=True, transform=transform)\n\n# obtain training indices that will be used for validation\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n# prepare data loaders (combine dataset and sampler)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n    sampler=train_sampler, num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n    sampler=valid_sampler, num_workers=num_workers)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n    num_workers=num_workers)\n\n# specify the image classes\nclasses = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n%matplotlib inline\n    \n# obtain one batch of training images\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nimages = images.numpy()\n\n# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(25, 4))\nfor idx in np.arange(20):\n    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n    ax.set_title(classes[labels[idx]])"
  },
  {
    "objectID": "posts/Machine Learning/Weight_Initializer.html#define-the-model-architecture",
    "href": "posts/Machine Learning/Weight_Initializer.html#define-the-model-architecture",
    "title": "Weight Intialization",
    "section": "",
    "text": "We’ve defined the MLP that we’ll use for classifying the dataset.\n\n\n\n\nA 3 layer MLP with hidden dimensions of 256 and 128.\nThis MLP accepts a flattened image (784-value long vector) as input and produces 10 class scores as output. We’ll test the effect of different initial weights on this 3 layer neural network with ReLU activations and an Adam optimizer.\n\nThe lessons you learn apply to other neural networks, including different activations and optimizers."
  },
  {
    "objectID": "posts/Machine Learning/Weight_Initializer.html#initialize-weights",
    "href": "posts/Machine Learning/Weight_Initializer.html#initialize-weights",
    "title": "Weight Intialization",
    "section": "",
    "text": "Let’s start looking at some initial weights. ### All Zeros or Ones If you follow the principle of Occam’s razor, you might think setting all the weights to 0 or 1 would be the best solution. This is not the case.\nWith every weight the same, all the neurons at each layer are producing the same output. This makes it hard to decide which weights to adjust.\nLet’s compare the loss with all ones and all zero weights by defining two models with those constant weights.\nBelow, we are using PyTorch’s nn.init to initialize each Linear layer with a constant weight. The init library provides a number of weight initialization functions that give you the ability to initialize the weights of each layer according to layer type.\nIn the case below, we look at every layer/module in our model. If it is a Linear layer (as all three layers are for this MLP), then we initialize those layer weights to be a constant_weight with bias=0 using the following code: &gt;if isinstance(m, nn.Linear):     nn.init.constant_(m.weight, constant_weight)     nn.init.constant_(m.bias, 0)\nThe constant_weight is a value that you can pass in when you instantiate the model.\n\n\nCode\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# define the NN architecture\nclass Net(nn.Module):\n    def __init__(self, hidden_1=256, hidden_2=128, constant_weight=None):\n        super(Net, self).__init__()\n        # linear layer (784 -&gt; hidden_1)\n        self.fc1 = nn.Linear(28 * 28, hidden_1)\n        # linear layer (hidden_1 -&gt; hidden_2)\n        self.fc2 = nn.Linear(hidden_1, hidden_2)\n        # linear layer (hidden_2 -&gt; 10)\n        self.fc3 = nn.Linear(hidden_2, 10)\n        # dropout layer (p=0.2)\n        self.dropout = nn.Dropout(0.2)\n        \n        # initialize the weights to a specified, constant value\n        if(constant_weight is not None):\n            for m in self.modules():\n                if isinstance(m, nn.Linear):\n                    nn.init.constant_(m.weight, constant_weight)\n                    nn.init.constant_(m.bias, 0)\n    \n            \n    def forward(self, x):\n        # flatten image input\n        x = x.view(-1, 28 * 28)\n        # add hidden layer, with relu activation function\n        x = F.relu(self.fc1(x))\n        # add dropout layer\n        x = self.dropout(x)\n        # add hidden layer, with relu activation function\n        x = F.relu(self.fc2(x))\n        # add dropout layer\n        x = self.dropout(x)\n        # add output layer\n        x = self.fc3(x)\n        return x\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim\n\n\ndef _get_loss_acc(model, train_loader, valid_loader):\n    \"\"\"\n    Get losses and validation accuracy of example neural network\n    \"\"\"\n    n_epochs = 2\n    learning_rate = 0.001\n    \n    # Training loss\n    criterion = nn.CrossEntropyLoss()\n\n    # Optimizer\n    optimizer = optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n\n    # Measurements used for graphing loss\n    loss_batch = []\n\n    for epoch in range(1, n_epochs+1):\n        # initialize var to monitor training loss\n        train_loss = 0.0\n        ###################\n        # train the model #\n        ###################\n        for data, target in train_loader:\n            # clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data)\n            # calculate the batch loss\n            loss = criterion(output, target)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            # record average batch loss \n            loss_batch.append(loss.item())\n             \n    # after training for 2 epochs, check validation accuracy \n    correct = 0\n    total = 0\n    for data, target in valid_loader:\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # get the predicted class from the maximum class score\n        _, predicted = torch.max(output.data, 1)\n        # count up total number of correct labels\n        # for which the predicted and true labels are equal\n\n\n\n\nBelow, we are using compare_init_weights to compare the training and validation loss for the two models we defined above, model_0 and model_1. This function takes in a list of models (each with different initial weights), the name of the plot to produce, and the training and validation dataset loaders. For each given model, it will plot the training loss for the first 100 batches and print out the validation accuracy after 2 training epochs. Note: if you’ve used a small batch_size, you may want to increase the number of epochs here to better compare how models behave after seeing a few hundred images.\nWe plot the loss over the first 100 batches to better judge which model weights performed better at the start of training.\nRun the cell below to see the difference between weights of all zeros against all ones.\n\n\nCode\n# initialize two NN's with 0 and 1 constant weights\nmodel_0 = Net(constant_weight=0)\nmodel_1 = Net(constant_weight=1)\n\n\n\n\nCode\n# put them in list form to compare\nmodel_list = [(model_0, 'All Zeros'),\n              (model_1, 'All Ones')]\n\n\n# plot the loss over the first 100 batches\ncompare_init_weights(model_list, \n                             'All Zeros vs All Ones', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n    9.675% -- All Zeros\n    9.675% -- All Ones\nTraining Loss\n    2.305  -- All Zeros\n  478.745  -- All Ones\n\n\nAs you can see the accuracy is close to guessing for both zeros and ones, around 10%.\nThe neural network is having a hard time determining which weights need to be changed, since the neurons have the same output for each layer. To avoid neurons with the same output, let’s use unique weights. We can also randomly select these weights to avoid being stuck in a local minimum for each run.\nA good solution for getting these random weights is to sample from a uniform distribution.\n\n\n\nA uniform distribution has the equal probability of picking any number from a set of numbers. We’ll be picking from a continuous distribution, so the chance of picking the same number is low. We’ll use NumPy’s np.random.uniform function to pick random numbers from a uniform distribution.\n\n\nOutputs random values from a uniform distribution.\n\n\nThe generated values follow a uniform distribution in the range [low, high). The lower bound minval is included in the range, while the upper bound maxval is excluded.\n\n\n\nlow: The lower bound on the range of random values to generate. Defaults to 0.\nhigh: The upper bound on the range of random values to generate. Defaults to 1.\nsize: An int or tuple of ints that specify the shape of the output array.\n\n\nWe can visualize the uniform distribution by using a histogram. Let’s map the values from np.random_uniform(-3, 3, [1000]) to a histogram using the hist_dist function. This will be 1000 random float values from -3 to 3, excluding the value 3.\n\n\nCode\nhist_dist('Random Uniform (low=-3, high=3)', np.random.uniform(-3, 3, [1000]))\n\n\n\n\n\nThe histogram used 500 buckets for the 1000 values. Since the chance for any single bucket is the same, there should be around 2 values for each bucket. That’s exactly what we see with the histogram. Some buckets have more and some have less, but they trend around 2.\nNow that you understand the uniform function, let’s use PyTorch’s nn.init to apply it to a model’s initial weights.\n\n\n\nLet’s see how well the neural network trains using a uniform weight initialization, where low=0.0 and high=1.0. Below, I’ll show you another way (besides in the Net class code) to initialize the weights of a network. To define weights outside of the model definition, you can: &gt;1. Define a function that assigns weights by the type of network layer, then 2. Apply those weights to an initialized model using model.apply(fn), which applies a function to each model layer.\nThis time, we’ll use weight.data.uniform_ to initialize the weights of our model, directly.\n\n\nCode\n# takes in a module and applies the specified weight initialization\ndef weights_init_uniform(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # apply a uniform distribution to the weights and a bias=0\n        m.weight.data.uniform_(0.0, 1.0)\n        m.bias.data.fill_(0)\n\n\n\n\nCode\n# create a new model with these weights\nmodel_uniform = Net()\nmodel_uniform.apply(weights_init_uniform)\n\n\nNet(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\n  (dropout): Dropout(p=0.2)\n)\n\n\n\n\nCode\n# evaluate behavior \ncompare_init_weights([(model_uniform, 'Uniform Weights')], \n                             'Uniform Baseline', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n   33.933% -- Uniform Weights\nTraining Loss\n    4.697  -- Uniform Weights\n\n\nThe loss graph is showing the neural network is learning, which it didn’t with all zeros or all ones. We’re headed in the right direction!"
  },
  {
    "objectID": "posts/Machine Learning/Weight_Initializer.html#general-rule-for-setting-weights",
    "href": "posts/Machine Learning/Weight_Initializer.html#general-rule-for-setting-weights",
    "title": "Weight Intialization",
    "section": "",
    "text": "The general rule for setting the weights in a neural network is to set them to be close to zero without being too small. &gt;Good practice is to start your weights in the range of \\([-y, y]\\) where \\(y=1/\\sqrt{n}\\)\n(\\(n\\) is the number of inputs to a given neuron).\nLet’s see if this holds true; let’s create a baseline to compare with and center our uniform range over zero by shifting it over by 0.5. This will give us the range [-0.5, 0.5).\n\n\nCode\n# takes in a module and applies the specified weight initialization\ndef weights_init_uniform_center(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # apply a centered, uniform distribution to the weights\n        m.weight.data.uniform_(-0.5, 0.5)\n        m.bias.data.fill_(0)\n\n# create a new model with these weights\nmodel_centered = Net()\nmodel_centered.apply(weights_init_uniform_center)\n\n\nNet(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\n  (dropout): Dropout(p=0.2)\n)\n\n\nThen let’s create a distribution and model that uses the general rule for weight initialization; using the range \\([-y, y]\\), where \\(y=1/\\sqrt{n}\\) .\nAnd finally, we’ll compare the two models.\n\n\nCode\n# takes in a module and applies the specified weight initialization\ndef weights_init_uniform_rule(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # get the number of the inputs\n        n = m.in_features\n        y = 1.0/np.sqrt(n)\n        m.weight.data.uniform_(-y, y)\n        m.bias.data.fill_(0)\n\n# create a new model with these weights\nmodel_rule = Net()\nmodel_rule.apply(weights_init_uniform_rule)\n\n\nNet(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\n  (dropout): Dropout(p=0.2)\n)\n\n\n\n\nCode\n# compare these two models\nmodel_list = [(model_centered, 'Centered Weights [-0.5, 0.5)'), \n              (model_rule, 'General Rule [-y, y)')]\n\n# evaluate behavior \ncompare_init_weights(model_list, \n                             '[-0.5, 0.5) vs [-y, y)', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n   74.875% -- Centered Weights [-0.5, 0.5)\n   84.833% -- General Rule [-y, y)\nTraining Loss\n    0.789  -- Centered Weights [-0.5, 0.5)\n    0.487  -- General Rule [-y, y)\n\n\nThis behavior is really promising! Not only is the loss decreasing, but it seems to do so very quickly for our uniform weights that follow the general rule; after only two epochs we get a fairly high validation accuracy and this should give you some intuition for why starting out with the right initial weights can really help your training process!\n\nSince the uniform distribution has the same chance to pick any value in a range, what if we used a distribution that had a higher chance of picking numbers closer to 0? Let’s look at the normal distribution.\n\n\nUnlike the uniform distribution, the normal distribution has a higher likelihood of picking number close to it’s mean. To visualize it, let’s plot values from NumPy’s np.random.normal function to a histogram.\n\nnp.random.normal(loc=0.0, scale=1.0, size=None)\n\n\nOutputs random values from a normal distribution.\n\n\n\nloc: The mean of the normal distribution.\nscale: The standard deviation of the normal distribution.\nshape: The shape of the output array.\n\n\n\n\nCode\nhist_dist('Random Normal (mean=0.0, stddev=1.0)', np.random.normal(size=[1000]))\n\n\n\n\n\nLet’s compare the normal distribution against the previous, rule-based, uniform distribution.\nBelow, we define a normal distribution that has a mean of 0 and a standard deviation of \\(y=1/\\sqrt{n}\\).\n\n\nCode\n# takes in a module and applies the specified weight initialization\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # get the number of the inputs\n        n = m.in_features\n        y = (1.0/np.sqrt(n))\n        m.weight.data.normal_(0, y)\n        m.bias.data.fill_(0)\n\n\n\n\nCode\n# create a new model with the rule-based, uniform weights\nmodel_uniform_rule = Net()\nmodel_uniform_rule.apply(weights_init_uniform_rule)\n\n# create a new model with the rule-based, NORMAL weights\nmodel_normal_rule = Net()\nmodel_normal_rule.apply(weights_init_normal)\n\n\nNet(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\n  (dropout): Dropout(p=0.2)\n)\n\n\n\n\nCode\n# compare the two models\nmodel_list = [(model_uniform_rule, 'Uniform Rule [-y, y)'), \n              (model_normal_rule, 'Normal Distribution')]\n\n# evaluate behavior \ncompare_init_weights(model_list, \n                             'Uniform vs Normal', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n   85.442% -- Uniform Rule [-y, y)\n   85.233% -- Normal Distribution\nTraining Loss\n    0.318  -- Uniform Rule [-y, y)\n    0.333  -- Normal Distribution\n\n\nThe normal distribution gives us pretty similar behavior compared to the uniform distribution, in this case. This is likely because our network is so small; a larger neural network will pick more weight values from each of these distributions, magnifying the effect of both initialization styles. In general, a normal distribution will result in better performance for a model.\n\n\n\n\nLet’s quickly take a look at what happens without any explicit weight initialization.\n\n\nCode\nmodel_no_initialization = Net()\n\n\n\n\nCode\nmodel_list = [(model_no_initialization, 'No Weights')]\n\n# evaluate behavior \ncompare_init_weights(model_list, \n                             'No Weight Initialization', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n   84.458% -- No Weights\nTraining Loss\n    0.530  -- No Weights\n\n\n\n\n\nSomething really interesting is happening here. You may notice that the red line “no weights” looks a lot like our uniformly initialized weights. It turns out that PyTorch has default weight initialization behavior for every kind of layer. You can see that linear layers are initialized with a uniform distribution (uniform weights and biases) in the module source code.\nHowever, you can also see that the weights taken from a normal distribution are comparable, perhaps even a little better! So, it may still be useful, especially if you are trying to train the best models, to initialize the weights of a model according to rules that you define.\nAnd, this is not the end ! You’re encouraged to look at the different types of common initialization distributions."
  },
  {
    "objectID": "posts/Machine Learning/Weight_Initializer.html#additional-resources",
    "href": "posts/Machine Learning/Weight_Initializer.html#additional-resources",
    "title": "Weight Intialization",
    "section": "",
    "text": "http://cs231n.github.io/neural-networks-2/#init\nhttps://github.com/udacity/deep-learning"
  },
  {
    "objectID": "temp/parallel_processing.html",
    "href": "temp/parallel_processing.html",
    "title": "Aman Pandey | Data Scientist",
    "section": "",
    "text": "Parallel Programming\n\nMulti Threading\nMutli-Processing\nAsync\n\n\n\n\n Back to top"
  },
  {
    "objectID": "temp/experimentation_management.html",
    "href": "temp/experimentation_management.html",
    "title": "Aman Pandey | Data Scientist",
    "section": "",
    "text": "“Every development begins with a theoretical foundation, but it is experiments that lend empirical understanding, making them indispensable for the inquisitive theoretical mind.”\nThe needs for doing experimentation whether it is for finding the perfect architecture, or finding the perfect hyper-parameter is always required for a given data.\nVisualization leads to in depth understanding of an experiment.\nEvery process generates data, the one on which we train our models are also generated with in a real world system and every bit of it is important from meta-data to actual data. Similarly ,when we train our models there are lot of data points which gets generated and it is important that we store all of them and if we track the current data points truth will be revelad, Hence, Visualization is needed if we know track and visualize we are one step closer to truth and a deeper understanding of the experiment."
  },
  {
    "objectID": "temp/experimentation_management.html#parallel-execution-of-different-configs",
    "href": "temp/experimentation_management.html#parallel-execution-of-different-configs",
    "title": "Aman Pandey | Data Scientist",
    "section": "Parallel Execution of different Configs",
    "text": "Parallel Execution of different Configs\nWhen there are so many configuration and each one them has to be executed either Synchronously or Parallely. There is a need for configuration Management.\nAnd there is no better tool for configuration Management then Hydra which is built on top on OmegaConf It is a hierarchical configuration system, with support for merging configurations from multiple sources (YAML config files, dataclasses/objects and CLI arguments) providing a consistent API regardless of how the configuration was created.\nHydra also comes with a --multirun feature which can be used run the code through all configurations synchronously or Parallely with a Joblib or Ray Launcher\nFor example if there is my model and I have 10 different configuration each one of them with may different features, or different lr, and other hyperparams and I need to execute all of the them parallely, hydra can do it."
  },
  {
    "objectID": "temp/experimentation_management.html#hyper-parameter-optimization",
    "href": "temp/experimentation_management.html#hyper-parameter-optimization",
    "title": "Aman Pandey | Data Scientist",
    "section": "Hyper Parameter Optimization",
    "text": "Hyper Parameter Optimization\nThe effectiveness of any machine learning model hinges on its hyperparameters, which are essential settings that can significantly impact its performance. In practical scenarios, there is no one-size-fits-all method for determining these hyperparameters. Instead, practitioners often resort to a trial-and-error approach or simply rely on default values, which can result in suboptimal model performance.\nHyperparameter optimization presents a structured solution to this challenge. It treats the task of hyperparameter selection as an optimization problem, aiming to identify hyperparameters that minimize validation errors.\nLet’s Start how many ways someone can do hyper-parameter optimization\n\nStart with Random i.e choose a parameter, let’s say lr and generate n numbers of random values and train the model with all those lr values and see which gives the lowest of errror.\nhttps://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide\nhttps://docs.ray.io/en/latest/tune/index.html\nhttps://www.determined.ai/\nhttps://mesos.apache.org/\n\nHyperparameter\nVisualization\n\nwhat needs to be visulaized\n\nDistribution Gradients and weights\nMetrics/losses\nHyperparameters\nmodel architecture\n\nTools\n\nTensorboard\nhttps://github.com/aimhubio/aim\n\n\ntalk more about using hydra and different conf file\ntalk more about tensor board\n\n\nConfigparser - https://docs.python.org/3/library/configparser.html\nPydantic -"
  },
  {
    "objectID": "posts/Machine Learning/Attention.html",
    "href": "posts/Machine Learning/Attention.html",
    "title": "Different types of Attentions",
    "section": "",
    "text": "A way to selectively focus on important subset of input from the bigger set.\n\n\n\n\\[\n(context\\ Vector)\\ c_t = \\sum_{i=1}^{j} \\alpha_{t,i}* h_i ;\\ where\\ \\alpha\\ is\\ attention\\ vector\\ and\\ h_i\\ is\\ RNN\\ output\\ at\\ i\\ time\\ step\n\\] \\[\n\\alpha_{t,i} = align(y_t,x_i)\\ ;\\ how\\ well\\ two\\ words\\ y_t\\ and\\ x_i\\ are\\ aligned\n\\] \\[\nE_t = v_t^T * tanh(W_a*S_{t-1} + U_a * h_i)\\ ;\\      W_a, U_a, v_t^T are\\ all\\ weight\\ parameters\n\\] \\[\n\\alpha_{t,i} = softmax(E_t)\\ ;\\ Location\\ Based\n\\]\n\nInitially we pass the entire sequence and generate two things the combined Hidden weights called output which in this case concat(h1 ,h2, h3, h4) let refer this as encoder_output and the decoder hidden state which z .\nWe generate a the attention based context vector as\n1. Add encoder_output, decoder_hidden_state\n2. Apply tanh and then multiply transpose of vt\n3. Apply mask to the Et vector if you don't want to put attention on padding\n4. Apply softmax to Et which gives the attention vector\n5. Multiply attention vector with encoder_output to get context vector\n\nIn decoder step\n\nConcat the input embedding for decoder sequence with context vector and pass as the input to Decoder RNN\nConcat the embedding , Decoder RNN output and Context Vector and pass it to linear layer to get the output.\nAt each time the context vector is updated using above steps with new hidden state for next time step\n\nimport torch.nn as nn\nimport torch\n\nclass BahdanauAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.w = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.u = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.v = nn.Linear(hidden_dim, 1)\n\n    def forward(self, decoder_hidden_state, encoder_output):\n        # decoder_hidden_state, encoder_output -&gt; (batch_size, seq_len, hidden_dim)\n\n        energy = self.v(torch.tanh(self.u(decoder_hidden_state) + self.v(encoder_output))).squeeze(-1)\n        #energy = (batch_size, seq_len)\n  \n        attn = nn.functional.softmax(energy, dim=-1).unsqueeze(1)\n        # attn = (batch_size, 1, seq_len)\n        context = torch.bmm(attn, encoder_output)\n        # context (batch_size, 1,hidden_dim)\n        return context, attn\n \n\n\n\n\\[\nscore(h_t , h_s^-)\\  ;\\ where\\ h_t\\ is\\ currect\\ decoder\\ hidden\\ state\\ and\\ h_s^-\\ is\\ encoder\\ output\n\\]\nIn loung paper there are basically 3 types of scoring function\n\\[\nscore(h_t , h_s^-) = h_t^T h_s^-\\ ; dot\\ product\n\\]\nThe Intuition behind this is dot product is a cosine similarity measure.\nIn this scoring function we are making an assumption that both the hidden states share the same embedding space so this will work for text summarization but on machine translation it will fail so we add weight matrix Wa\n\nAfter calculating scoring function which are then given as input to the softmax function to generate a attention vector.\nand then calculate the context vector using the same formulae\n\\[\n(context\\ Vector)\\ c_t = \\sum_{i=1}^{j} \\alpha_{t,i}* h_i\n\\]\nThe after the context vector is generated in the decoder step\n1. we concat the the context vector with current decoder hidden state which is then passed to linear layer with tanh acctivation to generate the output.\n\\[\nh_t^- = tanh(W_c[c_t;h_t])\n\\]\n1. The next step whatever output we get is input for decoder RNN for next time step with new hidden state, calucate again context vector and repeat the step.\nThe difference between the bahdanau paper is the simplicity in the steps\n\\[\nfrom\\ h_t -&gt; a_t -&gt; c_t-&gt;h_t^- \\\\\nbut\\ in\\ bahdanau\\ paper\\\\\nfrom\\ h_{t-1} -&gt; a_t -&gt; c_t-&gt;h_t\n\\]\nNote in the bahdanau paper\n\\[\nh_{t-1}\n\\]\nwhich means before feeding to decoder RNN calculate context vector but in loung paper\n\\[\nh_t\n\\]\nmeans attention vector is compute on current decoder RNN outputted hidden state.\nLoung Attention is all about matrix Multiplication so it is fast to implement but Additive Attention works well with larger sequence when compared to loung attention and also a little computation expensive.\nclass LoungAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.wa = nn.Linear(hidden_dim,hidden_dim,bias=False)\n\n    def forward(self, decoder_hidden_state, encoder_output) :\n        # decoder_hidden_state, encoder_output -&gt; (batch_size, seq_len, hidden_dim)\n\n        batch_size,input_size, hidden_dim,  = encoder_output.size()\n        score = torch.bmm(decoder_hidden_state, self.wa(encoder_output).transpose(1,2))\n        # score -&gt;  (batch_size, seq_len, seq_len)\n   \n        attn = nn.functional.softmax(score.view(-1, input_size), dim=1).view(batch_size, -1, input_size)\n        # score -&gt;  (batch_size, seq_len, seq_len)\n        context = torch.bmm(attn, encoder_output)\n        # context -&gt; (batch_size, seq_len, hidden_dim)\n        return context, attn\nNow Both the Attentions are Global attention because both uses a global constant encoder output to make decision and only the changing part is decoder hidden state. The global attention has a drawback that it has to attend to all words on the source side for each tar-get word, which is expensive and can potentially render it impractical to translate longer sequences,e.g., paragraphs or documents. To address this deficiency, A local attentional mechanism that chooses to focus only on a small subset of the source positions per target word Self Attention.\n\nThe “soft” vs “hard” attention is another way to categorize how attention is defined. The original idea was proposed in the show, attend and tell paper. Based on whether the attention has access to the entire image or only a patch:\n\nSoft\nAttention: the alignment weights are learned and placed “softly” over all patches in the source image; essentially the same idea as in\n\nPro: the model is smooth and differentiable.\nCon: expensive when the source input is large.\n\nHard\nAttention: only selects one patch of the image to attend to at a time.\n\nPro: less calculation at the inference time.\nCon: the model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train.\n\n\n\n\n\n\n\n\nSay the following sentence is an input sentence we want to translate:\n”The animal didn't cross the street because it was too tired”\nWhat does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.\nWhen the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.\nAs the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\nSelf-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.\n\nAs we are encoding the word “it” in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on “The Animal”, and baked a part of its representation into the encoding of “it”.\nWithin self attention there are three Matrices Query Matrix (Q), Key Matrix (K), Value Matrix(V) .\nLet’s say we have source sentence src = India is a great country and after tokenizng this sentence and passing it down by an embedding layer we have a matrix of\n\\[\n[5,256]\n\\]\nwhere 256 is the hidden dimension.\n\\[\nsrc=embedding(src)\n\\]\nNow with self attention we have a special property where the shape of\n\\[\nQ,K,V = [256,256]\\ all\\ of\\ them\\ have\\ same\\ shape\n\\]\nThe formulae for self attention is.\n\n\\[\nAttention(Q,K,V) = softmax(\\frac {Q.K^T} {\\sqrt{d_{model}}})V\n\\]\nLet’s see the first step where Q and K are multiplied we know when two vectors are multiplied there similarity is governed by cos function where\n\\[\n\\theta = 0\n\\]\nmeans two vectors are similar and\n\\[\n\\theta = -1\n\\]\nmeans two vectors are opposite \nSo when we are multiplying two matrices we found out where we want to put out attention. (Note because of linear layer we will putting attentions at different vectors.) By putting Softmax we are just making sure similar vectors have a high probability and dissimilar vectors are have a minuscule value.\nNow when we again multiply with V we get a matrix were the vectors were we want to put attention has a high value compared to the vector where we don’t want to put attention.\nThe\n\\[\n\\sqrt{d_{model}}\n\\]\nis just a scaling factor used in Multi Head Attention. where\n\\[\nd_{model} =\\frac {hid\\_dim}{n\\_heads}\n\\]\n\n\n\n\nTo Boost the self Attention Mechanism Multi Head Attention was introduced where we have multiple heads. We have a source sentence src and if n_heads=8 then 8 separate query, key, value Matrices are present.\nwe calculate self Attention on all 8 heads and then we concat it and pass it to a linear Layer.\nThe Reason for Multi Head Attention is\n\nIt expands the model’s ability to focus on different positions.\nIt gives the attention layer multiple “representation subspaces”.\n\n\nPytorch Multi Head Attention\n \n\n\n\nFlash attention is Fast and Memory-Efficient Exact Attention with IO-Awareness\nNote - Read This Before moving forward - Making Deep Learning Go Brrrr From First Principles\nTransformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length.\nTo see the problem\nQuery -&gt; Batch_size, seq_len,hidden_dim\nKey -&gt; Batch_size, seq_len, hidden_dim\nUsually the batch_size and hidden_dim are parameters with small numbers.\nWhen we Batch matrix Multiplication the result is Batch_size, seq_len, seq_len.\nThe bigger the seq_len the bigger the bigger the memory complexity.\nVarious past methods tried to reduce the FLOP but their implementation was not that successful here in flash attention the intention is reduce the memory access(IO).\nThe excerpt from the paper which states things very clearly.\n ” We propose FlashAttention, a new attention algorithm that computes exact attention with far fewer memory accesses.\nOur main goal is to avoid reading and writing the attention matrix to and from HBM.\nThis requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large intermediate attention matrix for the backward pass.\nWe apply two well-established techniques to address these challenges. (i) We restructure the attention computation to split the input into blocks and make several passes over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We store the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the backward pass, which is faster than the standard approach of reading the intermediate attention matrix from HBM.\nNow the problem would be clear that most of the time is spent in Loading the data from HBM and Saving the data to HBM. Calculation of softmax over a large input. ”\n\nIf we try to understand from this picture itself without looking the algo it says, 1. HBM is storage is big. SRAM is smaller, hence the entire Q or K or V can’t be loaded fully. 2. Break it into smaller chunks of respectable size which can fit on SRAM. here d is no of heads and N is seq_len 3. There is an outer loop which loads K,V in chunks to SRAM. 1. There is an inner loop which loads Q in chunks to SRAM. 2. The entire self-attention computation with softmax happens. 3. The output block is written to HBM.\nThe IO complexity of flash attention -\n\n\nThey save softmax statistics over blocks which over many iteration approximates the correct softmax values which in their algo they call it tilling.\nDuring backward pass softmax inputs and outputs are needed they recompute those inputs and outputs from the final output and softmax normalization statistics from blocks of Q K V in SRAM. This can be seen as a form of selective gradient checkpointing.\nLet suppose there is bigger transformer architecture which has got lot of activations in forward pass we compute these activations and save each of the intermediary results. During backward pass when we access those intermediary results their is huge memory footprint to load these value from HBM to SRAM, to in gradient checkpointing rather then storing all of the intermediary, we keep only a few and rest we compute on fly, because compute is faster, much faster then loading the data too and fro from HBM."
  },
  {
    "objectID": "posts/Machine Learning/Attention.html#bahdanau-additive-attention",
    "href": "posts/Machine Learning/Attention.html#bahdanau-additive-attention",
    "title": "Different types of Attentions",
    "section": "",
    "text": "\\[\n(context\\ Vector)\\ c_t = \\sum_{i=1}^{j} \\alpha_{t,i}* h_i ;\\ where\\ \\alpha\\ is\\ attention\\ vector\\ and\\ h_i\\ is\\ RNN\\ output\\ at\\ i\\ time\\ step\n\\] \\[\n\\alpha_{t,i} = align(y_t,x_i)\\ ;\\ how\\ well\\ two\\ words\\ y_t\\ and\\ x_i\\ are\\ aligned\n\\] \\[\nE_t = v_t^T * tanh(W_a*S_{t-1} + U_a * h_i)\\ ;\\      W_a, U_a, v_t^T are\\ all\\ weight\\ parameters\n\\] \\[\n\\alpha_{t,i} = softmax(E_t)\\ ;\\ Location\\ Based\n\\]\n\nInitially we pass the entire sequence and generate two things the combined Hidden weights called output which in this case concat(h1 ,h2, h3, h4) let refer this as encoder_output and the decoder hidden state which z .\nWe generate a the attention based context vector as\n1. Add encoder_output, decoder_hidden_state\n2. Apply tanh and then multiply transpose of vt\n3. Apply mask to the Et vector if you don't want to put attention on padding\n4. Apply softmax to Et which gives the attention vector\n5. Multiply attention vector with encoder_output to get context vector\n\nIn decoder step\n\nConcat the input embedding for decoder sequence with context vector and pass as the input to Decoder RNN\nConcat the embedding , Decoder RNN output and Context Vector and pass it to linear layer to get the output.\nAt each time the context vector is updated using above steps with new hidden state for next time step\n\nimport torch.nn as nn\nimport torch\n\nclass BahdanauAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.w = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.u = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.v = nn.Linear(hidden_dim, 1)\n\n    def forward(self, decoder_hidden_state, encoder_output):\n        # decoder_hidden_state, encoder_output -&gt; (batch_size, seq_len, hidden_dim)\n\n        energy = self.v(torch.tanh(self.u(decoder_hidden_state) + self.v(encoder_output))).squeeze(-1)\n        #energy = (batch_size, seq_len)\n  \n        attn = nn.functional.softmax(energy, dim=-1).unsqueeze(1)\n        # attn = (batch_size, 1, seq_len)\n        context = torch.bmm(attn, encoder_output)\n        # context (batch_size, 1,hidden_dim)\n        return context, attn"
  },
  {
    "objectID": "posts/Machine Learning/Attention.html#loung-multiplicative-attention",
    "href": "posts/Machine Learning/Attention.html#loung-multiplicative-attention",
    "title": "Different types of Attentions",
    "section": "",
    "text": "\\[\nscore(h_t , h_s^-)\\  ;\\ where\\ h_t\\ is\\ currect\\ decoder\\ hidden\\ state\\ and\\ h_s^-\\ is\\ encoder\\ output\n\\]\nIn loung paper there are basically 3 types of scoring function\n\\[\nscore(h_t , h_s^-) = h_t^T h_s^-\\ ; dot\\ product\n\\]\nThe Intuition behind this is dot product is a cosine similarity measure.\nIn this scoring function we are making an assumption that both the hidden states share the same embedding space so this will work for text summarization but on machine translation it will fail so we add weight matrix Wa\n\nAfter calculating scoring function which are then given as input to the softmax function to generate a attention vector.\nand then calculate the context vector using the same formulae\n\\[\n(context\\ Vector)\\ c_t = \\sum_{i=1}^{j} \\alpha_{t,i}* h_i\n\\]\nThe after the context vector is generated in the decoder step\n1. we concat the the context vector with current decoder hidden state which is then passed to linear layer with tanh acctivation to generate the output.\n\\[\nh_t^- = tanh(W_c[c_t;h_t])\n\\]\n1. The next step whatever output we get is input for decoder RNN for next time step with new hidden state, calucate again context vector and repeat the step.\nThe difference between the bahdanau paper is the simplicity in the steps\n\\[\nfrom\\ h_t -&gt; a_t -&gt; c_t-&gt;h_t^- \\\\\nbut\\ in\\ bahdanau\\ paper\\\\\nfrom\\ h_{t-1} -&gt; a_t -&gt; c_t-&gt;h_t\n\\]\nNote in the bahdanau paper\n\\[\nh_{t-1}\n\\]\nwhich means before feeding to decoder RNN calculate context vector but in loung paper\n\\[\nh_t\n\\]\nmeans attention vector is compute on current decoder RNN outputted hidden state.\nLoung Attention is all about matrix Multiplication so it is fast to implement but Additive Attention works well with larger sequence when compared to loung attention and also a little computation expensive.\nclass LoungAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.wa = nn.Linear(hidden_dim,hidden_dim,bias=False)\n\n    def forward(self, decoder_hidden_state, encoder_output) :\n        # decoder_hidden_state, encoder_output -&gt; (batch_size, seq_len, hidden_dim)\n\n        batch_size,input_size, hidden_dim,  = encoder_output.size()\n        score = torch.bmm(decoder_hidden_state, self.wa(encoder_output).transpose(1,2))\n        # score -&gt;  (batch_size, seq_len, seq_len)\n   \n        attn = nn.functional.softmax(score.view(-1, input_size), dim=1).view(batch_size, -1, input_size)\n        # score -&gt;  (batch_size, seq_len, seq_len)\n        context = torch.bmm(attn, encoder_output)\n        # context -&gt; (batch_size, seq_len, hidden_dim)\n        return context, attn\nNow Both the Attentions are Global attention because both uses a global constant encoder output to make decision and only the changing part is decoder hidden state. The global attention has a drawback that it has to attend to all words on the source side for each tar-get word, which is expensive and can potentially render it impractical to translate longer sequences,e.g., paragraphs or documents. To address this deficiency, A local attentional mechanism that chooses to focus only on a small subset of the source positions per target word Self Attention.\n\nThe “soft” vs “hard” attention is another way to categorize how attention is defined. The original idea was proposed in the show, attend and tell paper. Based on whether the attention has access to the entire image or only a patch:\n\nSoft\nAttention: the alignment weights are learned and placed “softly” over all patches in the source image; essentially the same idea as in\n\nPro: the model is smooth and differentiable.\nCon: expensive when the source input is large.\n\nHard\nAttention: only selects one patch of the image to attend to at a time.\n\nPro: less calculation at the inference time.\nCon: the model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train."
  },
  {
    "objectID": "posts/Machine Learning/Attention.html#self-attention",
    "href": "posts/Machine Learning/Attention.html#self-attention",
    "title": "Different types of Attentions",
    "section": "",
    "text": "Say the following sentence is an input sentence we want to translate:\n”The animal didn't cross the street because it was too tired”\nWhat does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.\nWhen the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.\nAs the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\nSelf-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.\n\nAs we are encoding the word “it” in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on “The Animal”, and baked a part of its representation into the encoding of “it”.\nWithin self attention there are three Matrices Query Matrix (Q), Key Matrix (K), Value Matrix(V) .\nLet’s say we have source sentence src = India is a great country and after tokenizng this sentence and passing it down by an embedding layer we have a matrix of\n\\[\n[5,256]\n\\]\nwhere 256 is the hidden dimension.\n\\[\nsrc=embedding(src)\n\\]\nNow with self attention we have a special property where the shape of\n\\[\nQ,K,V = [256,256]\\ all\\ of\\ them\\ have\\ same\\ shape\n\\]\nThe formulae for self attention is.\n\n\\[\nAttention(Q,K,V) = softmax(\\frac {Q.K^T} {\\sqrt{d_{model}}})V\n\\]\nLet’s see the first step where Q and K are multiplied we know when two vectors are multiplied there similarity is governed by cos function where\n\\[\n\\theta = 0\n\\]\nmeans two vectors are similar and\n\\[\n\\theta = -1\n\\]\nmeans two vectors are opposite \nSo when we are multiplying two matrices we found out where we want to put out attention. (Note because of linear layer we will putting attentions at different vectors.) By putting Softmax we are just making sure similar vectors have a high probability and dissimilar vectors are have a minuscule value.\nNow when we again multiply with V we get a matrix were the vectors were we want to put attention has a high value compared to the vector where we don’t want to put attention.\nThe\n\\[\n\\sqrt{d_{model}}\n\\]\nis just a scaling factor used in Multi Head Attention. where\n\\[\nd_{model} =\\frac {hid\\_dim}{n\\_heads}\n\\]"
  },
  {
    "objectID": "posts/Machine Learning/Attention.html#multi-head-attention",
    "href": "posts/Machine Learning/Attention.html#multi-head-attention",
    "title": "Different types of Attentions",
    "section": "",
    "text": "To Boost the self Attention Mechanism Multi Head Attention was introduced where we have multiple heads. We have a source sentence src and if n_heads=8 then 8 separate query, key, value Matrices are present.\nwe calculate self Attention on all 8 heads and then we concat it and pass it to a linear Layer.\nThe Reason for Multi Head Attention is\n\nIt expands the model’s ability to focus on different positions.\nIt gives the attention layer multiple “representation subspaces”.\n\n\nPytorch Multi Head Attention"
  },
  {
    "objectID": "posts/Machine Learning/Attention.html#flash-attention",
    "href": "posts/Machine Learning/Attention.html#flash-attention",
    "title": "Different types of Attentions",
    "section": "",
    "text": "Flash attention is Fast and Memory-Efficient Exact Attention with IO-Awareness\nNote - Read This Before moving forward - Making Deep Learning Go Brrrr From First Principles\nTransformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length.\nTo see the problem\nQuery -&gt; Batch_size, seq_len,hidden_dim\nKey -&gt; Batch_size, seq_len, hidden_dim\nUsually the batch_size and hidden_dim are parameters with small numbers.\nWhen we Batch matrix Multiplication the result is Batch_size, seq_len, seq_len.\nThe bigger the seq_len the bigger the bigger the memory complexity.\nVarious past methods tried to reduce the FLOP but their implementation was not that successful here in flash attention the intention is reduce the memory access(IO).\nThe excerpt from the paper which states things very clearly.\n ” We propose FlashAttention, a new attention algorithm that computes exact attention with far fewer memory accesses.\nOur main goal is to avoid reading and writing the attention matrix to and from HBM.\nThis requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large intermediate attention matrix for the backward pass.\nWe apply two well-established techniques to address these challenges. (i) We restructure the attention computation to split the input into blocks and make several passes over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We store the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the backward pass, which is faster than the standard approach of reading the intermediate attention matrix from HBM.\nNow the problem would be clear that most of the time is spent in Loading the data from HBM and Saving the data to HBM. Calculation of softmax over a large input. ”\n\nIf we try to understand from this picture itself without looking the algo it says, 1. HBM is storage is big. SRAM is smaller, hence the entire Q or K or V can’t be loaded fully. 2. Break it into smaller chunks of respectable size which can fit on SRAM. here d is no of heads and N is seq_len 3. There is an outer loop which loads K,V in chunks to SRAM. 1. There is an inner loop which loads Q in chunks to SRAM. 2. The entire self-attention computation with softmax happens. 3. The output block is written to HBM.\nThe IO complexity of flash attention -\n\n\nThey save softmax statistics over blocks which over many iteration approximates the correct softmax values which in their algo they call it tilling.\nDuring backward pass softmax inputs and outputs are needed they recompute those inputs and outputs from the final output and softmax normalization statistics from blocks of Q K V in SRAM. This can be seen as a form of selective gradient checkpointing.\nLet suppose there is bigger transformer architecture which has got lot of activations in forward pass we compute these activations and save each of the intermediary results. During backward pass when we access those intermediary results their is huge memory footprint to load these value from HBM to SRAM, to in gradient checkpointing rather then storing all of the intermediary, we keep only a few and rest we compute on fly, because compute is faster, much faster then loading the data too and fro from HBM."
  },
  {
    "objectID": "posts/Machine Learning/Regularization.html",
    "href": "posts/Machine Learning/Regularization.html",
    "title": "Improve Model’s Prediction power using Regularization",
    "section": "",
    "text": "A central problem in machine learning is how to make an algorithm that will perform well not just on the training data, but also on new inputs. The great OverFitting Problem.\nMany strategies used in machine learning are explicitly designed to reduce the test error, possibly at the expense of increased training error. These strategies are known collectively as regularization.\nOne of the easiet to say but harder to do things is to increase the amount of training data.\nRegularization increases training error but reduces generalization error hence more no of epochs are needed to get the desired result. Regularization helps to reduce overfitting of the model.\nThere are many regularization techniques used some but extra term in objective function and some but extra constraint on the model.\n\nL1/L2 regularizers\nDropOut\nLabel Smoothing\nData Augmentation\nEarly Stopping\nWeight Clipping and Gradient Clipping\nPruning\nNormalization\n\n\n\nL1 and L2 regularizers are some time known as weight decay.\nL1 Regularization works by adding an l1 norm to the cost function.\n\\[\nL1\\ Norm :\n||X||_1 = \\sum_i |x_i|\n\\] L2 Regularization works by adding an l2 norm to the cost function.\n\\[\nL2\\ Norm :\n||X||_2 = \\sqrt {\\sum_i |x_i^2|}\n\\]\nThe idea behind l1 and l2 norm is smaller weight generalizes the model better so both of these norm perform some kind of weight decay.\n\n\n\\[\n    C = any\\ loss\\ function  + \\frac{\\lambda}{2n}\\sum w^2\n\\]\nHere λ is a regularization parameter and n is the size of training data w is the weight.we are adding a sum of squares of all weights to the cost function which is scaled by λ/2n where λ &gt; 0.\nThe intitution behind the l2 reguarization is to make it so the network prefers to learn small weights. Large weights will only be allowed if they considerably improve the first part of the cost function.\nPut another way, regularization can be viewed as a way of compromising between finding small weights and minimizing the original cost function.\nThe relative importance of the two elements of the compromise depends on the value of λ: when λ is small we prefer to minimize the original cost function, but when λ is large we prefer small weights.\nUpdating weight formulae while backprop \\[\nw = w - {lr} \\frac{\\partial C}{\\partial w} - \\frac {{lr} \\lambda} {n} w\n\\]\n\\[\nw = \\left( 1 - \\frac{{lr}\\lambda } {n} \\right) w - {lr} \\frac{\\partial C}{\\partial w}\n\\]\nHere \\[\n\\left( 1 - \\frac{{lr} \\lambda } {n} \\right)\n\\] is the rescaling factor for weights or the weight decay factor.For very small λ value it is allowing big weights and if λ value is big it is penealizing the weights.\nWhy is this going on? Heuristically, if the cost function is unregularized, then the length of the weight vector is likely to grow, all other things being equal. Over time this can lead to the weight vector being very large indeed. This can cause the weight vector to get stuck pointing in more or less the same direction, since changes due to gradient descent only make tiny changes to the direction, when the length is long. I believe this phenomenon is making it hard for our learning algorithm to properly explore the weight space, and consequently harder to find good minima of the cost function.\n\n\n\n\\[\nC = any\\ loss\\ function  + \\frac{\\lambda}{n}\\sum_w |w|\n\\]\nL1 regularization is similar to l2 just the norm formulae changes from sum of squares to absolute value.\nUpdating weight formulae while backprop \\[\nw = w - {lr} \\frac{\\partial C}{\\partial w} - \\frac {{lr} \\lambda} {n} sign(w)\n\\]\nsign(w) is just the sign of the weight vector +1 for positive weights and -1 for negative weights\n\n\nIn both expressions the effect of regularization is to shrink the weights. This accords with our intuition that both kinds of regularization penalize large weights. But the way the weights shrink is different.\nIn L1 regularization, the weights shrink by a constant amount toward 0.\nIn L2 regularization, the weights shrink by an amount which is proportional to w. And so when a particular weight has a large magnitude, |w|, L1 regularization shrinks the weight much less than L2 regularization does.\nBy contrast, when |w| is small, L1 regularization shrinks the weight much more than L2 regularization. The net result is that L1 regularization tends to concentrate the weight of the network in a relatively small number of high-importance connections, while the other weights are driven toward zero.\nHence L1 regularization makes the Network Spare.\n\n\n\n\n\nDropout is another regularization techniques which is very simple to understand.\n\nSo it takes a probability p and based on the value of p it randomly disables that percentage of neuron.\nFor example if the dropout value is 0.3 on a layer. It will disable 30% neuron in the layer i.e zero the value of those neuron.\nWhile training with every batch a different set on neurons are disabled which is completely random.\nSo why does dropout increases the robustness of the model? Heuristically, when we dropout different sets of neurons, it’s rather like we’re training different neural networks. And so the dropout procedure is like averaging the effects of a very large number of different networks. The different networks will overfit in different ways, and so, hopefully, the net effect of dropout will be to reduce overfitting.\nFor example In cnn if the model is trained on dogs vs cats example and few particular neurons having higher weight, everytime the model witnesses the whiskers in the image it activates those neurons and we get cat. But what if those whiskers are no there then model fails significantly. so dropout forces the model to learn more attributes of the training data while training.\nwhen p = 0.5\nBy repeating dropout over and over, our network will learn a set of weights and biases. Of course, those weights and biases will have been learnt under conditions in which half the hidden neurons were dropped out. When we actually run the full network that means that twice as many hidden neurons will be active. To compensate for that, we halve the weights outgoing from the hidden neurons.\nif we Pytorch implementation of Dropout their , the outputs are scaled by a factor of \\[ 1/(1-p)​\\] during training. This means that during evaluation the module simply computes an identity function.\nThere is also DropConnect which is on similar lines as Dropout\n\n\n\n\n\n\n\n\n\n\n\nWhen we apply the cross-entropy loss to a classification task, we’re expecting true labels to have 1, while the others 0. In other words, we have no doubts that the true labels are true, and the others are not. Is that always true? Maybe not. Many manual annotations are the results of multiple participants. They might have different criteria. They might make some mistakes. They are human, after all. As a result, the ground truth labels we have had perfect beliefs on are possible wrong.\nThe impact of this on model is First, it may result in over-fitting: if the model learns to assign full probability to the ground truth label for each training example, it is not guaranteed to generalize. Second, it encourages the differences between the largest logit and all others to become large, and this, combined with the bounded gradient reduces the ability of the model to adapt. Intuitively, this happens because the model becomes too confident about its predictions.\nOne possibile solution to this is to relax our confidence on the labels. For instance, we can slighly lower the loss target values from 1 to, say, 0.9. And naturally we increase the target value of 0 for the others slightly as such. This idea is called label smoothing.\nCheck the result of imagenet model after applying on imagenet\nPytorch Supports it in Cross Entropy Loss"
  },
  {
    "objectID": "posts/Machine Learning/Regularization.html#l1l2-regularizers",
    "href": "posts/Machine Learning/Regularization.html#l1l2-regularizers",
    "title": "Improve Model’s Prediction power using Regularization",
    "section": "",
    "text": "L1 and L2 regularizers are some time known as weight decay.\nL1 Regularization works by adding an l1 norm to the cost function.\n\\[\nL1\\ Norm :\n||X||_1 = \\sum_i |x_i|\n\\] L2 Regularization works by adding an l2 norm to the cost function.\n\\[\nL2\\ Norm :\n||X||_2 = \\sqrt {\\sum_i |x_i^2|}\n\\]\nThe idea behind l1 and l2 norm is smaller weight generalizes the model better so both of these norm perform some kind of weight decay.\n\n\n\\[\n    C = any\\ loss\\ function  + \\frac{\\lambda}{2n}\\sum w^2\n\\]\nHere λ is a regularization parameter and n is the size of training data w is the weight.we are adding a sum of squares of all weights to the cost function which is scaled by λ/2n where λ &gt; 0.\nThe intitution behind the l2 reguarization is to make it so the network prefers to learn small weights. Large weights will only be allowed if they considerably improve the first part of the cost function.\nPut another way, regularization can be viewed as a way of compromising between finding small weights and minimizing the original cost function.\nThe relative importance of the two elements of the compromise depends on the value of λ: when λ is small we prefer to minimize the original cost function, but when λ is large we prefer small weights.\nUpdating weight formulae while backprop \\[\nw = w - {lr} \\frac{\\partial C}{\\partial w} - \\frac {{lr} \\lambda} {n} w\n\\]\n\\[\nw = \\left( 1 - \\frac{{lr}\\lambda } {n} \\right) w - {lr} \\frac{\\partial C}{\\partial w}\n\\]\nHere \\[\n\\left( 1 - \\frac{{lr} \\lambda } {n} \\right)\n\\] is the rescaling factor for weights or the weight decay factor.For very small λ value it is allowing big weights and if λ value is big it is penealizing the weights.\nWhy is this going on? Heuristically, if the cost function is unregularized, then the length of the weight vector is likely to grow, all other things being equal. Over time this can lead to the weight vector being very large indeed. This can cause the weight vector to get stuck pointing in more or less the same direction, since changes due to gradient descent only make tiny changes to the direction, when the length is long. I believe this phenomenon is making it hard for our learning algorithm to properly explore the weight space, and consequently harder to find good minima of the cost function.\n\n\n\n\\[\nC = any\\ loss\\ function  + \\frac{\\lambda}{n}\\sum_w |w|\n\\]\nL1 regularization is similar to l2 just the norm formulae changes from sum of squares to absolute value.\nUpdating weight formulae while backprop \\[\nw = w - {lr} \\frac{\\partial C}{\\partial w} - \\frac {{lr} \\lambda} {n} sign(w)\n\\]\nsign(w) is just the sign of the weight vector +1 for positive weights and -1 for negative weights\n\n\nIn both expressions the effect of regularization is to shrink the weights. This accords with our intuition that both kinds of regularization penalize large weights. But the way the weights shrink is different.\nIn L1 regularization, the weights shrink by a constant amount toward 0.\nIn L2 regularization, the weights shrink by an amount which is proportional to w. And so when a particular weight has a large magnitude, |w|, L1 regularization shrinks the weight much less than L2 regularization does.\nBy contrast, when |w| is small, L1 regularization shrinks the weight much more than L2 regularization. The net result is that L1 regularization tends to concentrate the weight of the network in a relatively small number of high-importance connections, while the other weights are driven toward zero.\nHence L1 regularization makes the Network Spare."
  },
  {
    "objectID": "posts/Machine Learning/Regularization.html#dropout",
    "href": "posts/Machine Learning/Regularization.html#dropout",
    "title": "Improve Model’s Prediction power using Regularization",
    "section": "",
    "text": "Dropout is another regularization techniques which is very simple to understand.\n\nSo it takes a probability p and based on the value of p it randomly disables that percentage of neuron.\nFor example if the dropout value is 0.3 on a layer. It will disable 30% neuron in the layer i.e zero the value of those neuron.\nWhile training with every batch a different set on neurons are disabled which is completely random.\nSo why does dropout increases the robustness of the model? Heuristically, when we dropout different sets of neurons, it’s rather like we’re training different neural networks. And so the dropout procedure is like averaging the effects of a very large number of different networks. The different networks will overfit in different ways, and so, hopefully, the net effect of dropout will be to reduce overfitting.\nFor example In cnn if the model is trained on dogs vs cats example and few particular neurons having higher weight, everytime the model witnesses the whiskers in the image it activates those neurons and we get cat. But what if those whiskers are no there then model fails significantly. so dropout forces the model to learn more attributes of the training data while training.\nwhen p = 0.5\nBy repeating dropout over and over, our network will learn a set of weights and biases. Of course, those weights and biases will have been learnt under conditions in which half the hidden neurons were dropped out. When we actually run the full network that means that twice as many hidden neurons will be active. To compensate for that, we halve the weights outgoing from the hidden neurons.\nif we Pytorch implementation of Dropout their , the outputs are scaled by a factor of \\[ 1/(1-p)​\\] during training. This means that during evaluation the module simply computes an identity function.\nThere is also DropConnect which is on similar lines as Dropout"
  },
  {
    "objectID": "posts/Machine Learning/Regularization.html#label-smoothing",
    "href": "posts/Machine Learning/Regularization.html#label-smoothing",
    "title": "Improve Model’s Prediction power using Regularization",
    "section": "",
    "text": "When we apply the cross-entropy loss to a classification task, we’re expecting true labels to have 1, while the others 0. In other words, we have no doubts that the true labels are true, and the others are not. Is that always true? Maybe not. Many manual annotations are the results of multiple participants. They might have different criteria. They might make some mistakes. They are human, after all. As a result, the ground truth labels we have had perfect beliefs on are possible wrong.\nThe impact of this on model is First, it may result in over-fitting: if the model learns to assign full probability to the ground truth label for each training example, it is not guaranteed to generalize. Second, it encourages the differences between the largest logit and all others to become large, and this, combined with the bounded gradient reduces the ability of the model to adapt. Intuitively, this happens because the model becomes too confident about its predictions.\nOne possibile solution to this is to relax our confidence on the labels. For instance, we can slighly lower the loss target values from 1 to, say, 0.9. And naturally we increase the target value of 0 for the others slightly as such. This idea is called label smoothing.\nCheck the result of imagenet model after applying on imagenet\nPytorch Supports it in Cross Entropy Loss"
  },
  {
    "objectID": "posts/Machine Learning/Regularization.html#early-stopping",
    "href": "posts/Machine Learning/Regularization.html#early-stopping",
    "title": "Improve Model’s Prediction power using Regularization",
    "section": "Early Stopping",
    "text": "Early Stopping\nMonitor the model’s performance on a validation set during training and stop when the performance starts degrading. This prevents the model from overfitting the training data.\nHere you can look at the code to implement the same link"
  },
  {
    "objectID": "posts/Machine Learning/Regularization.html#weight-clipping-and-gradient-clipping",
    "href": "posts/Machine Learning/Regularization.html#weight-clipping-and-gradient-clipping",
    "title": "Improve Model’s Prediction power using Regularization",
    "section": "Weight Clipping and Gradient Clipping",
    "text": "Weight Clipping and Gradient Clipping\n\nLimit the magnitude of weights in the network to prevent them from becoming too large. This can be achieved using techniques like weight clipping.\nLimit the gradients during training to prevent exploding gradients. This is especially useful in recurrent neural networks (RNNs).\n\nopt = optim.SGD(model.parameters(), lr=0.1)\nfor i in range(1000):\n    out = model(inputs)\n    loss = loss_fn(out, labels)\n    print(i, loss.item())\n    opt.zero_grad()\n    loss.backward()\n    # standard way of clipping the gradient\n    torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n    \n    # another way of doing it\n    # https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html#clip_grad_value_\n    for param in model.parameters():\n        param.grad.clamp_(-1, 1)  # weight clipping in range of -1 to 1\n\n    opt.step()\n    with torch.no_grad():\n        for param in model.parameters():\n            param.clamp_(-1, 1)  # weight clipping in range of -1 to 1"
  },
  {
    "objectID": "posts/Machine Learning/Regularization.html#pruning",
    "href": "posts/Machine Learning/Regularization.html#pruning",
    "title": "Improve Model’s Prediction power using Regularization",
    "section": "Pruning",
    "text": "Pruning\n Pruning is a technique that removes weights or biases (parameters) from a neural network model. Now there are many ways of doing it based on different criteria and what the need is overall if done properly , makes the model training/inference fast, better generalization, resource friendly.\nFollow few tutorials\nTutorial\nDoc"
  },
  {
    "objectID": "posts/Machine Learning/Regularization.html#thank-you.",
    "href": "posts/Machine Learning/Regularization.html#thank-you.",
    "title": "Improve Model’s Prediction power using Regularization",
    "section": "Thank you.",
    "text": "Thank you."
  },
  {
    "objectID": "posts/Python/profiling.html",
    "href": "posts/Python/profiling.html",
    "title": "Memory and Time Profiling",
    "section": "",
    "text": "Introduction\nEvery Developer desires to make their code optimzied and efficient. A dream where the developers want that their code to execute faster, with no memory leakage on the production system.\nLet’s make this dream true…\nCreating Data-processing pipeline, writing new algorithms, Deploying Machine Learning models to server million users, Scientific calculation in astropyhsics, these are few areas where when we write code we want to profile every single line of code for two things 1. The amount of time it is taking to execute where our goal is to reduce the time taken a.k.a Time Complexity. 2. The memory consumption for execution of that code where our goal is to reduce the memory usage a.k.a Space complexity.\nThere always a trade-off between both of them some time we are fine with memory consumption but not with the time it takes and vice-versa based on the needs we check for the trade-off, but the best system is where we can reduce both space and time complexity.\n\n\nPremature Optimization is evil.\nEarly in developing Algorithms we should think less about these things because it can be counter-productive which can lead to premature optimization and its the root cause of all evil.\nSo first make it work then optimize it.\n\n\nMagic functions and tools\nWhile most of the data science experiments starts in Ipython Notebook. The Ipython enviroment gives us some magic functions which can be utilized to profile our code.\n\n%%timeit: Measuring time taken for the codeblock to run\n%lprun: Run code with the line-by-line profiler\n%mprun: Run code with the line-by-line memory profiler\n\nFor Tracing Memory Leakage we can use Pympler.\n\n\nCode\nimport numpy as np\n\n\n\n\nTimeit\nThe usage of timeit is very simple just put the magic method on the top of the cell and it will calculate the time taken to execute the cell.\nLet’s compare vectorized vs non-vectorized version of numpy code.\n\n\nCode\nnumber = np.random.randint(0,100,10000)\n\n\n\n\nCode\n%%timeit\ntotal = 0\nfor i in number:\n    total+=i\n\n\n3.11 ms ± 86.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\nCode\n%%timeit\nnumber.sum()\n\n\n14.9 µs ± 74.3 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n\n\nThe difference in the execution time is evident one.\nNon-vectorized code in Milliseconds, 10-3. Vectorized code in Microseconds, 10-6.\nVectorized code is the winner here.\n\n\nTiming Profiling with Lprun\nline_profiler is a package for doing line-by-line timing profiling of functions.\nInstall using\npip install line_profiler\nPython provides a builtin profiler, but we will be using Line profiler for reasons stated below.\nThe current profiling tools supported in Python 2.7 and later only time function calls. This is a good first step for locating hotspots in one’s program and is frequently all one needs to do to optimize the program. However, sometimes the cause of the hotspot is actually a single line in the function, and that line may not be obvious from just reading the source code. These cases are particularly frequent in scientific computing. Functions tend to be larger (sometimes because of legitimate algorithmic complexity, sometimes because the programmer is still trying to write FORTRAN code), and a single statement without function calls can trigger lots of computation when using libraries like numpy. cProfile only times explicit function calls, not special methods called because of syntax. Consequently, a relatively slow numpy operation on large arrays like this,\na[large_index_array] = some_other_large_array\nis a hotspot that never gets broken out by cProfile because there is no explicit function call in that statement.\nLineProfiler can be given functions to profile, and it will time the execution of each individual line inside those functions. In a typical workflow, one only cares about line timings of a few functions because wading through the results of timing every single line of code would be overwhelming. However, LineProfiler does need to be explicitly told what functions to profile.\n\n\nCode\n# once installed we have load the extension\n%load_ext line_profiler\n\n\n\n\nCode\ndef some_operation(x):\n    x = x **2\n    x = x +2\n    x = np.concatenate([x,x,x],axis=0)\n    return x\n\n\nNow the %lprun command will do a line-by-line profiling of any function–in this case, we need to tell it explicitly which functions we’re interested in profiling:\n\n\nCode\n%lprun -f some_operation some_operation(np.random.randn(100))\n\n\nTimer unit: 1e-06 s\n\nTotal time: 7.7e-05 s\nFile: &lt;ipython-input-30-80aca4fcfa96&gt;\nFunction: some_operation at line 1\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     1                                           def some_operation(x):\n     2         1         24.0     24.0     31.2      x = x **2\n     3         1         22.0     22.0     28.6      x = x +2\n     4         1         30.0     30.0     39.0      x = np.concatenate([x,x,x],axis=0)\n     5         1          1.0      1.0      1.3      return x\n\nThe source code of the function is printed with the timing information for each line. There are six columns of information.\n\nLine : The line number in the file.\nHits: The number of times that line was executed.\nTime: The total amount of time spent executing the line in the timer’s units. In the header information before the tables, you will see a line “Timer unit:” giving the conversion factor to seconds. It may be different on different systems.\nPer Hit: The average amount of time spent executing the line once in the timer’s units.\n% Time: The percentage of time spent on that line relative to the total amount of recorded time spent in the function.\nLine Contents: The actual source code. Note that this is always read from disk when the formatted results are viewed, not when the code was executed. If you have edited the file in the meantime, the lines will not match up, and the formatter may not even be able to locate the function for display.\n\n\n\nMemory Profiling with mprun\nThis is a python module for monitoring memory consumption of a process as well as line-by-line analysis of memory consumption for python programs.\nInstall\npip install -U memory_profiler\nThe only issue mprun doesn’t work on notebook rather on a python file so we will write the code in notebook %%file magic function we will write that into a file and execute mprun on it\n\n\nCode\n%load_ext memory_profiler\n\n\n\n\nCode\n%%file mprun.py\nimport numpy as np\ndef some_operation(x):\n    y = x **2\n    z = y +2\n    result = np.concatenate([x,y,z],axis=0)\n    return result\n\n\nOverwriting mprun.py\n\n\n\n\nCode\nfrom mprun import some_operation\n%mprun -f some_operation some_operation(np.random.randn(100000))\n\n\n\n\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     2     62.5 MiB     62.5 MiB   def some_operation(x):\n     3     63.3 MiB      0.8 MiB       y = x **2\n     4     64.0 MiB      0.8 MiB       z = y +2\n     5     66.3 MiB      2.3 MiB       result = np.concatenate([x,y,z],axis=0)\n     6     66.3 MiB      0.0 MiB       return result\n\nThe first column represents the line number of the code that has been profiled.\nThe second column (Mem usage) the memory usage of the Python interpreter after that line has been executed.\nThe third column (Increment) represents the difference in memory of the current line with respect to the last one.\nThe last column (Line Contents) prints the code that has been profiled.\n\n\n\nMemory Leakage using pympler\nPympler is a development tool to measure, monitor and analyze the memory behavior of Python objects in a running Python application.\nBy pympling a Python application, detailed insight in the size and the lifetime of Python objects can be obtained. Undesirable or unexpected runtime behavior like memory bloat and other “pymples” can easily be identified.\nPympler integrates three previously separate modules into a single, comprehensive profiling tool. The asizeof module provides basic size information for one or several Python objects, module muppy is used for on-line monitoring of a Python application and module Class Tracker provides off-line analysis of the lifetime of selected Python objects.\nA web profiling frontend exposes process statistics, garbage visualisation and class tracker statistics.\nHit table of content for tutorial\n\n\nRead More\nUnderstanding Python Memory Managment\nPython Garbage Collector\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Python/big_data_processing.html",
    "href": "posts/Python/big_data_processing.html",
    "title": "Big Data Processing",
    "section": "",
    "text": "My day to day work revolves around understanding the data, analysis, Visualization and eventually Training model on it.\nMost of the time the data which I am dealing with is huge in size and one of characteristics of any kind of data is\nNow the frequency part can be from couple of seconds to days to weeks.\nAs a Data Scientist I never have to be bothered about how exactly the data that is getting generated on the client side is to be stored, updated, processed and it comes to me a ready, for me to train my model upon.\nSo in the Blog we will talking about some data engineering stuff."
  },
  {
    "objectID": "posts/Python/big_data_processing.html#basic-terminologies",
    "href": "posts/Python/big_data_processing.html#basic-terminologies",
    "title": "Big Data Processing",
    "section": "Basic Terminologies",
    "text": "Basic Terminologies\nBefore we get into the core part there are some terminologies which are important to understand also I will using AWS services to give my examples.\n\nData Lake -&gt; Any place where you can dump any type of data, video dump to s3(simple storage service), text dump to s3, files dump to s3, images dump to s3, blobs. Anything type data that can be stored at a place that place is called data lake.\nData Warehouse -&gt; The Data which is stored in data lake has to be processed for some purpose may be training model, reporting, analysis it could be anything. Processing the unstructured data from data lake and storing it in a structured way at a particular place that place is Data Warehouse.\n\nNow keen observers can say can’t we store the process data to data lake rather than in data warehouse in that case you are absolutely correct, we can store it there.\nFor ex - Taking bunch of user queries stored in S3 process it create a single csv file and again store it in s3.\nAlso sometimes we want to store that processed data in Database which is absolutely fine.\n\nBatch Processing -&gt; When data processing happens in batches or in scheduled interval for example If we take all the user queries for a particular day and on night if we process all of it that is called batch processing.\nStream Processing -&gt; When data gets generated at that same time or near real time we try to process it. for example the moment user submits a query we do some processing.\nEvent -&gt; when a user clicks a button, when a user writes a query, when a user scrolls , everything is an event and it depends completely on business on fundamental level what do they want to call an event."
  },
  {
    "objectID": "posts/Python/big_data_processing.html#data-streaming-ingestion-and-processing.",
    "href": "posts/Python/big_data_processing.html#data-streaming-ingestion-and-processing.",
    "title": "Big Data Processing",
    "section": "Data Streaming, Ingestion and Processing.",
    "text": "Data Streaming, Ingestion and Processing.\nWhen a event happens how to process that event for example - when on instagram we do share, like and comment, couple of things need to be done, first the UI has to be updated with an increement, and the data has to stored in some type of database.\nso when a event happens there can be n things which needs to be updated on the same time.\nHere comes Apache Kafka to Rescue\nApache Kafka is a open-source distributed event streaming platform. i.e in simple terms when you million of users sending different types of queries now how exactly are suppose to handle each query which dashboard, database, UI component has to be update all is handled by Apache Kafka. It helps integrate data from a variety of sources and sinks\nKafka follows a publish-subscribe model, where data producers (publishers) send messages to Kafka topics, and data consumers (subscribers) can subscribe to those topics to receive and process the messages.\n\nTo draw an analogy we can take example of google map as we enter source and drop location map shows us the way but we have to go by ourself by driving or walking.\nSo, Kafka is often used in conjunction with stream processing frameworks like Apache Flink, Apache Beam, Kafka Streams etc to perform real-time data processing and store the processed data for something.\nNow either we deploy apache kafka and manage by our own self our use AWS Kinesis. They both serve the same purpose.\nApache Flink - It is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.\n\nFor example -\n\nTake example of any Algo Trading bot for every news and fluctuation that happens inside the market the data has to be quickly processed and understand, flink would be a suitable candidate.\nCalculate metrics like click-through rates (CTR) for products.\nDetect patterns, such as identifying when users abandon their shopping carts.\nPerform sessionization to group user interactions into sessions for personalized recommendations.\nDetect anomalies or fraud in real-time, such as unusual purchase patterns.\n\nTo draw a similarity Kinesis Data Streams does the exact same thing.\nNote:- Neither Flink nor Kafka provide its own data-storage system, but provides data-source and sink connectors to systems.\nIf the data needs to stored some where directly may be Data Lake, Database, files.we can use Kafka Connect which connects Kafka with external data sources and sinks. It helps to ingest data.\n\nTo draw a similarity Amazon Kinesis Firehose provides the same functionality.\nAmazon Kinesis Firehose is a fully managed AWS service that simplifies the process of ingesting, transforming, and delivering real-time streaming data to other AWS services, such as Amazon S3, Amazon Redshift, or Amazon Elasticsearch. Kinesis Firehose is primarily used for data ingestion and data loading into AWS services. It’s particularly useful for situations where you want to move data from a streaming source to AWS data storage or analytics services without complex processing.\nDebezium - It is an open-source platform for change data capture (CDC) that captures and streams database changes in real-time. It allows you to monitor and react to changes in your databases as they occur, making it a valuable tool for building real-time data pipelines, event-driven microservices, and other applications that require access to fresh, up-to-date data.\nJust to sum it up we have seen how to stream events using kafka store it using kafka connect, some stream processing using Flink and react to certain changes in the database using debezium.\nNow when it comes selection of the Database we have plathora of option with pros and cons of each and we will look the pros and cons of each one of them some other day."
  },
  {
    "objectID": "posts/Python/big_data_processing.html#data-storage-and-querying",
    "href": "posts/Python/big_data_processing.html#data-storage-and-querying",
    "title": "Big Data Processing",
    "section": "Data Storage and Querying",
    "text": "Data Storage and Querying\nStoring the processed data in Data Lake vs. Data Warehouse, often depends on factors like data volume, query performance requirements, and cost considerations.\nNow let suppose their is an example scenario where some data from data lake is processed\n\nEither we can store the processed data back to data lake.\nStore the processed data onto some Database.\n\nFor case 1 - we can have a big csv file or parquet file which has all the processed data, now what all things we can do\n\nUse directly in model training, analysis.\nOr there is need to query that particular data which is stored in the data lake may be run a SQL query to fetch a subset of the data in that case use Presto a distributed SQL query engine.\n\n\nPresto acts a single query engine for data engineers who struggle with managing multiple query languages and interfaces to siloed databases and storage, Presto is the fast and reliable engine that provides one simple ANSI SQL interface for all your data analytics and your open lakehouse.\non AWS Athena gives the same functionality, actually Athena is built on top of presto.\nExample - let suppose we store some processed data into TB scale into S3 now what we can do is use Athena to query that data using SQL syntax.\nFor case 2- It is simple that we directly dump the data on to a Database which is designed for business use case some one is looking for. Amazon RedShift is a good option.\nAlso many times we just need data integration service where we just want to move data from different sources to one single source for different application. for example we want to perform some ETL on data stored in 5 different DBs to one single source in that AWS provide Glue\n We now know how to store the processed data and query it."
  },
  {
    "objectID": "posts/Python/big_data_processing.html#batch-processing",
    "href": "posts/Python/big_data_processing.html#batch-processing",
    "title": "Big Data Processing",
    "section": "Batch Processing",
    "text": "Batch Processing\nWe looked that we can use Flink to do stream processing now when it comes to batch processing of data we can use all time fav tool Apache Spark.\nBased on the data size you might need processing power from 1 node to multiples clusters.\nIn case we have a single computer with lot of computer power we can do parallel processing by our selves, without requiring any special library but when it comes to distributed processing using a matured library is always recommended.\nNow most of the distributed system works in a Master-Slave configuration where there is one master node and multiple slave nodes. the master schedules and monitors the job and slave nodes actually perform the job.\nSince the work is distributed across cluster we need a cluster manager also, Apache Mesos or Kubernetes is used to manage the allocation of resources (CPU and memory) for Spark applications.\n\nOn AWS we can run spark jobs on Amazon EMR(Elastic Map Reduce)"
  },
  {
    "objectID": "posts/Python/big_data_processing.html#data-visualization",
    "href": "posts/Python/big_data_processing.html#data-visualization",
    "title": "Big Data Processing",
    "section": "Data visualization",
    "text": "Data visualization\nThe data is processed now stored in some place now there is need to visualize now before visualizing there is we need to query the data and the most important part of querying is 1. how quickly we need the data. 2. how many people are querying it at a given moment. 3. what is the size of the database we are querying.\nTake a step back on to data processing system, when a database is designed either it is designed for heavy read operation OLAP or Write operation OLTP\nOLTP (Online Transaction Processing):\n\nOLTP databases are designed for transactional processing and day-to-day operations.\nThey handle tasks like order processing, inventory management, and customer record updates.\nOLTP systems are optimized for write-heavy operations and maintaining data integrity.\nData consistency is critical in OLTP databases, and transactions are ACID-compliant (Atomicity, Consistency, Isolation, Durability).\nOLTP queries are relatively simple and involve tasks like INSERT, UPDATE, DELETE, and SELECT of individual records. Response times are typically low to ensure efficient transaction processing.\nSo banks, atms, ecommerce websites are using OLTP for many write operations.\n\nOLAP (Online Analytical Processing): 1. OLAP databases are designed for complex queries and reporting. 2. They are used for business intelligence, data analysis, and decision support systems. 3. OLAP systems are optimized for read-heavy operations and are well-suited for data analysis tasks. 4. The data is denormalized, meaning redundant data is stored to optimize query performance.\nNow that we know about data processing databases Let’s take two OLAP scenario- 1. Every morning some manager at linkedin wants to tracks some numbers may be Active users, interaction etc. 2. Uber Eats wants to show real time analytics to the restaurant owner, for the owner to take quick decision.\nThe difference in both the use case is given a query how much time it has before it returns a result.\nIn case 1 - query take 5hrs, 6hrs hell even 10hrs we don’t care because it once everyday, here we can use any query engine or database we have Athena, Redshift, Postgres, MongoDb it doesn’t matter.\nIn case 2 - query has to execute within milliseconds. Here something different is needed we can’t rely on let say redshift to quickly query PB scale data and give result in milliseconds.\nLet me introduce Apache Pinot - Realtime distributed OLAP datastore, designed to answer OLAP queries with low latency\n\nHow do they do it to make any read operation fast we have to index the data, if we slice and dice the data to maintain 100’s of indexes across different dimension we can make the read of different queries faster.\nApache Pinot allows RealTime user facing analytics.\nOnce the query is executed use any data visualization tool to visualize the data we have many options Apache SuperSet, Amazon QuickSight, Tableau"
  },
  {
    "objectID": "posts/Python/big_data_processing.html#job-scheduling",
    "href": "posts/Python/big_data_processing.html#job-scheduling",
    "title": "Big Data Processing",
    "section": "Job Scheduling",
    "text": "Job Scheduling\nHow do we schedule our ETL(Extract Transform Load) jobs, we saw how spark can do batch processing, superset can do visualization for us but the question now is how do we schedule the job based on conditions, triggers and at particular time.\nApache Airflow is a full blown platform for orchestrating, scheduling, and monitoring complex workflows of data pipelines, ETL (Extract, Transform, Load) processes, and other data-related tasks. It provides a framework for defining, scheduling, and executing workflows as a directed acyclic graph (DAG), making it a powerful tool for automating and managing data workflows.\nIt also comes with parallel and distributed execution engine, error handling capabilites, monitoring and logging.\nApache Airflow can be said as Cron on steroids."
  },
  {
    "objectID": "posts/Python/big_data_processing.html#bakup-and-archival-of-data",
    "href": "posts/Python/big_data_processing.html#bakup-and-archival-of-data",
    "title": "Big Data Processing",
    "section": "Bakup and Archival of Data",
    "text": "Bakup and Archival of Data\nBackup and Archival are also important parts of Data Management\nAWS Backup is a centralized, fully managed backup service that streamlines the process of protecting your data across various AWS resources, including EC2 instances, RDS databases, EBS volumes, and more.\nAWS S3 Glacier complements AWS Backup by offering a cost-effective solution for long-term data archiving. Here are the key features of AWS S3 Glacier:\nLow-Cost Storage: S3 Glacier offers significantly lower storage costs compared to standard AWS S3 storage classes. It’s ideal for archiving data that is rarely accessed but needs to be retained for compliance or historical purposes.\nTiered Storage Options: Glacier provides different storage tiers to meet specific retrieval time requirements. You can choose between three retrieval options: Expedited, Standard, and Bulk, depending on the urgency of accessing archived data.\nData Lifecycle Policies: Automate data lifecycle management with policies that transition data from frequently accessed S3 storage classes to Glacier after a certain period. This helps optimize storage costs without manual intervention.\nVaults and Archives: S3 Glacier organizes data into “vaults,” and each vault can contain multiple “archives.” Archives are individual objects or files that can be stored, retrieved, and managed as needed.\nThis below picture can give insights on to how different tools interact with each other."
  },
  {
    "objectID": "posts/Python/big_data_processing.html#thank-you.",
    "href": "posts/Python/big_data_processing.html#thank-you.",
    "title": "Big Data Processing",
    "section": "Thank you.",
    "text": "Thank you."
  },
  {
    "objectID": "posts/Convolutions/convolution.html",
    "href": "posts/Convolutions/convolution.html",
    "title": "Convolution and Common architectures",
    "section": "",
    "text": "1. Regular Neural Nets don’t scale well to full images\nIn MNIST dataset,images are only of size 28x28x1 (28 wide, 28 high, 1 color channels), so a single fully-connected neuron in a first hidden layer of a regular Neural Network would have 28x28x1 = 786 weights. This amount still seems manageable,\nBut what if we move to larger images.\nFor example, an image of more respectable size, e.g. 200x200x3, would lead to neurons that have 200x200x3 = 120,000 weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting.\n2.Parameter Sharing  A feature detector that is useful in one part of the image is probably useful in another part of the image.Thus CNN are good in capturing translation invariance.\nSparsity of connections In each layer,each output value depends only on a small number of inputs.This makes CNN networks easy to train on smaller training datasets and is less prone to overfitting.\n2.3D volumes of neurons. Convolutional Neural Networks take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular, unlike a regular Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth."
  },
  {
    "objectID": "posts/Convolutions/convolution.html#inception-v1",
    "href": "posts/Convolutions/convolution.html#inception-v1",
    "title": "Convolution and Common architectures",
    "section": "Inception V1",
    "text": "Inception V1\nInception v1\n Problems this network tried to solve: 1. What is the right kernel size for convolution  A larger kernel is preferred for information that is distributed more globally, and a smaller kernel is preferred for information that is distributed more locally.  Ans- Filters with multiple sizes.The network essentially would get a bit “wider” rather than “deeper”   3. How to stack convolution which can be less computationally expensive  Stacking them naively computationally expensive.  Ans-Limit the number of input channels by adding an extra 1x1 convolution before the 3x3 and 5x5 convolutions   2. How to avoid overfitting in a very deep network  Very deep networks are prone to overfitting. It also hard to pass gradient updates through the entire network.  Ans-Introduce two auxiliary classifiers (The purple boxes in the image). They essentially applied softmax to the outputs of two of the inception modules, and computed an auxiliary loss over the same labels. The total loss function is a weighted sum of the auxiliary loss and the real loss.\nThe total loss used by the inception net during training.  total_loss = real_loss + 0.3 * aux_loss_1 + 0.3 * aux_loss_2  \n\nPoints to note\n\nUsed 9 Inception modules in the whole architecture, with over 100 layers in total! Now that is deep…\nNo use of fully connected layers! They use an average pool instead, to go from a 7x7x1024 volume to a 1x1x1024 volume. This saves a huge number of parameters.\nUses 12x fewer parameters than AlexNet.\nTrained on “a few high-end GPUs within a week”.\nIt achieved a top-5 error rate of 6.67%"
  },
  {
    "objectID": "posts/Convolutions/convolution.html#inception-v2",
    "href": "posts/Convolutions/convolution.html#inception-v2",
    "title": "Convolution and Common architectures",
    "section": "Inception V2",
    "text": "Inception V2\nRethinking the Inception Architecture for Computer Vision\nUpgrades were targeted towards: 1. Reducing representational bottleneck by replacing 5x5 convolution to two 3x3 convolution operations which further improves computational speed  The intuition was that, neural networks perform better when convolutions didn’t alter the dimensions of the input drastically. Reducing the dimensions too much may cause loss of information, known as a “representational bottleneck”   2. Using smart factorization method where they factorize convolutions of filter size nxn to a combination of 1xn and nx1 convolutions.  For example, a 3x3 convolution is equivalent to first performing a 1x3 convolution, and then performing a 3x1 convolution on its output. They found this method to be 33% more cheaper than the single 3x3 convolution."
  },
  {
    "objectID": "posts/Convolutions/convolution.html#spatial-seperable-convolution",
    "href": "posts/Convolutions/convolution.html#spatial-seperable-convolution",
    "title": "Convolution and Common architectures",
    "section": "Spatial Seperable Convolution",
    "text": "Spatial Seperable Convolution\n\nDivides a kernel into two, smaller kernels\n\nInstead of doing one convolution with 9 multiplications(parameters), we do two convolutions with 3 multiplications(parameters) each (6 in total) to achieve the same effect\n\nWith less multiplications, computational complexity goes down, and the network is able to run faster.\nThis was used in an architecture called Effnet showing promising results.\nThe main issue with the spatial separable convolution is that not all kernels can be “separated” into two, smaller kernels. This becomes particularly bothersome during training, since of all the possible kernels the network could have adopted, it can only end up using one of the tiny portion that can be separated into two smaller kernels."
  },
  {
    "objectID": "posts/Convolutions/convolution.html#depthwise-convolution",
    "href": "posts/Convolutions/convolution.html#depthwise-convolution",
    "title": "Convolution and Common architectures",
    "section": "Depthwise Convolution",
    "text": "Depthwise Convolution\n\nSay we need to increase the number of channels from 16 to 32 using 3x3 kernel. \nNormal Convolution  Total No of Parameters = 3 x 3 x 16 x 32 = 4608\n\nDepthwise Convolution\n\nDepthWise Convolution = 16 x [3 x 3 x 1]\nPointWise Convolution = 32 x [1 x 1 x 16]\n\nTotal Number of Parameters = 656\nMobile net uses depthwise seperable convolution to reduce the number of parameters"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Aman Pandey",
    "section": "",
    "text": "Data Scientist with expertise in Natural Language Processing and Recommendation Engines.\nEnthusiastic about Math, Programming and Free Software. \n\nBioSkillsNon Job Pursuits\n\n\nI am Data Scientist, Computer Programmer and Mathematician.\nI have worked majorly in Natual Language Processing, Natural Language Generation and Recommendation Engines in different Buisness domain.\nOver the years in companies I have worked all of them demanded innovations and ingenuity to be added to solve the complex problems in hand which has kept me ahead in this fast moving AI Research Landscape.\nMy love for human speaking languages lead me to dwell deeper into the world of NLP.\nOn a side note - I can speak 5 Languages and at one point I even learned American Sign Language as well which i don’t remember very well these days.\nSomething I believe so deeply is Free software, and Free Education for everyone which lead me conduct multiple meetups, workshop, events. What I learned from the community I gave back to the community.\nI advocate for open research. These days I am fueled by enthusiasm to do mathematics from completely different perspective.\n\n\nComputer Programming | Python | Java | C++ | PyTorch | GNU/Linux | Distributed Programming | Parallel Programming.\nDeep Learning | Machine Learning | Bayesian Active Learning | Metric Learning | Multi-Task Learning | Transfer Learning\nAI Specalities | NLP | NLG | NLU | Recommendation Systems | Computational Linguistics | Information Extraction\n\n\nMember in Free Software Movement Karnataka(FSMK) | Android Teacher for 7 days Bootcamp | Organized bootcamps in multiple colleges | Bringing Awarness and knowledge about the Open Eco-System to the society | Events and Conferences representing FSMK | 2015 - 2020\nPresident of College GNU/Linux group(Toggle) | Organizing Events and Workshops in college | 2018-2019\nEntirety.ai(Feb 2019 - Mar 2021) | Discuss on current trend and Hands on Experience in Deep Learning | Event Ambassador for Pie and AI Meetup Bangalore | Talks and Meetups in Bangalore | collaborating with other meetup group and companies like Nvidia and Amazon\nAdjunct faculty at Praxis Buisness School | Teaching Deep learning and Model Deployment to PG students | June 2019 - July 2021\nHaskell Study Group (2022) | Organized study group consisting of 50 members from world wide.| Learned Haskell and FP together\nPycon | Poster presentation | CFP Reviewer\n\n\n\n\n\n Back to top"
  }
]