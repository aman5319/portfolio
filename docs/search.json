[
  {
    "objectID": "job.html",
    "href": "job.html",
    "title": "Professional Experience",
    "section": "",
    "text": "This job pursuits\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MyBlog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJul 4, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\nMarkdown Demo\n\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2022\n\n\nNorah Smith\n\n\n\n\n\n\n  \n\n\n\n\nJupyter Demo\n\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2021\n\n\nNorah Smith\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html",
    "title": "Markdown Demo",
    "section": "",
    "text": "Let’s take an example of Binary classification where the task is to predict whether we have Dog or not. So in an Image if model predicts Dog that’s a positive class if it predicts no Dog that’s a negative class.\n\n\n\nA confusion matrix is a table that is used to describe the performance of a classification model.\n\nLet’s consider this example table where N denotes the total number of images our model will predict upon.\nN = 150 Total Number of Dog Images = 100 Total Number of No Dog Images = 50\nIn total 100 Dog Images 1. Model Predicted 60 Correct which is your True Positive. 2. Model Predicted 40 incorrect which is False Negative.(Since It has to predict Dog and It Predicted No Dog which is a False prediction to a Negative class.)\nSimilarly in 50 No Dog Images 1. Model Predicted 30 Correct which is True Negative. 2. Model Predicted 20 Incorrect which is False Positive. (Since It has to predict No Dog and It predicted Dog which is a False prediction to a Positive class.)\nTP-&gt;60 FN-&gt;40 TN-&gt;30 FP-&gt;20\n\n\n\nAccuracy is a basic metric which just tells models overall performance. How many predictions made by model is correct.\n\\[\nAccuracy = \\frac{True Positive + True Negative}{N}\n\\]\n\\[\nAccuracy = \\frac{60+30}{150} = 0.6 = 60\n\\]\nAccuracy gave us a idea about how 60 % prediction was correct. Accuracy can only be a good metric. if all the classes are balanced i.e No of positive sample is approximately equal to No of negative samples. Per class Accuracy can also be calculated to know for which classes model is behaving well.\n\n\n\nMisclassification Rate tells overall how poor model performance is. It just opposite of Accuracy.\n\\[\nmisclassification\\ rate = 1 - Accuracy\n\\]\n\\[\nmisclassifcation\\ rate = \\frac{False Postive + False Negative}{N}\n\\]\n\\[\nmisclassification\\ rate = \\frac{20+40}{150} = 0.4 = 40\\%\n\\]\n\n\n\nPrecision is another metric which tells while predicting how accurately can I predict positive classes .\n\\[\nPositive\\ class\\ prediction  = True\\ positive + False\\ Positive\n\\]\n\\[\nprecision = \\frac{True\\ Positive}{positive\\ class\\ prediction}\n\\]\n\\[\nprecision = \\frac {60}{60 + 20} = 0.75 = 75\\%\n\\]\nWe are only worried about the prediction of one class which is Dog and the positive prediction class will have (Positive class, Negative class) from Actuals. That’s why it is positive predictive value.\nLet’s consider two example where\n\nSpam detection -&gt; Classify whether an email is Spam or Not Spam.\n​ Here the goal is to accurately classify spam emails. It’s okay to classify a Spam mail as Not Spam mail as it will come in our inbox it does no harm to use. But if we classify as Not Spam Mail as Spam mail then there is a problem because we generally do not open our Spam Box.\n​ If you think about it the first case is False Negative and Second case is False Positive and we are okay with False Negative’s but we are not okay with False Positives and our goal is to reduce False Positive.\n​ So in Spam detection task precision is a good metric. since it is inversely proportionate to False Positives.\nCancer detection -&gt; Classify whether a person has a cancer or not.\n​ Here the goal is to accurately classify whether a person has a cancer or Not. It’s okay to classify a person Not having cancer as cancer. But it’s not okay to predict a person having cancer as Not cancer.\n​ If you think the first case is False Positive and Second case is False Negative and we are okay with False Positive but not okay with False Negative.\n​ Hence in this particular task Precision plays no role.\n\nHence to reduce False Positives Precision is used. Precision can easily be effected by class Imbalance.\n\n\n\nNegative Predictive Value is another metric which tells while predicting how accurately can I predict Negative classes .\n\\[\nNegative\\ class\\ prediction  = True\\ Negative + False\\ Negative\n\\]\n\\[\nNegative\\ Prediction\\ Value = \\frac{True\\ Negative}{Negative\\ class\\ prediction}\n\\]\n\\[\nnegative\\ prediction\\ value = \\frac {60}{60 + 20} = 0.75 = 75\\%\n\\]\nWe are only worried about the prediction of one class which is Dog and the positive prediction class will have (Positive class, Negative class) from Actuals. That’s why it is positive predictive value.\nLet Suppose we don’t want to have any additional process for screening patients checked as healthy (not cancer) then we may want to make sure that our negative predictive value is high.\n\n\n\nRecall is another metric which tells us while predicting how accurately can it predict positive classes given a set of Actual Positives. \\[\nActual\\ positive\\ class = True\\ Positive + False\\ Negative\n\\]\n\\[\nrecall = \\frac{True\\ Positive}{Actual\\ positive\\ class}\n\\]\n\\[\nrecall = \\frac{60}{100} = 0.6 = 60\\%\n\\]\nHere our concern is about Given a set of Positive Samples which is like giving all the Dog images and then making prediction on it. The prediction will have (positive class , Negative class ) from Actual positives. That’s why recall is also True Positive Rate .\nThe reason why it’s called recall is given all the positive sample knowledge how well the model can recall that knowledge to predicted accurately by decreasing the error rate for the Actual positive class.\nGoing back to the Cancer Example it is very clear know that for Cancer Detection we will use Recall as our metric. Recall is good metric to be used for class Imbalance problem.\n\n\n\nSimilar to True positive rate, True Negative rate tells us while predicting how accurately can it predict Negative classes given a set of Actual Negatives. \\[\nActual\\ negative\\ class = True\\ Negative + False\\ Positive\n\\]\n\\[\nTrue\\ Negative\\ Rate = \\frac{True\\ Negative}{Actual\\ negative\\ class}\n\\]\n\\[\nTrue\\ Negative\\ Rate = \\frac{60}{100} = 0.6 = 60\\%\n\\]\nHere our concern is about Given a set of Negative Samples which is like giving all the No Dog images and then making prediction on it. The prediction will have (positive class , Negative class ) from Actual Negatives.\nFor the same cancer example True Negative rate will show how many non cancer people are identified as not having cancer.\n\n\n\n\nn many tests, including diagnostic medical tests, sensitivity is the extent to which actual positives are not overlooked (so false negatives are few), and specificity is the extent to which actual negatives are classified as such (so false positives are few). Thus, a highly sensitive test rarely overlooks an actual positive (for example, showing “nothing bad” despite something bad existing); a highly specific test rarely registers a positive classification for anything that is not the target of testing (for example, finding one bacterial species and mistaking it for another closely related one that is the true target); and a test that is highly sensitive and highly specific does both, so it “rarely overlooks a thing that it is looking for” and it “rarely mistakes anything else for that thing.” Because most medical tests do not have sensitivity and specificity values above 99%, “rarely” does not equate to certainty. But for practical reasons, tests with sensitivity and specificity values above 90% have high credibility, albeit usually no certainty, in differential diagnosis.\nSensitivity, therefore, quantifies the avoidance of false negatives and specificity does the same for false positives.\n\n\n\nWhen the model predicts something Falsely to the positive class which then it is contributing to the False Positive rate. we can think of it as False alert. For example if in a production house based on certain machine parameters the model has to predict whether the situation insider the production house is dangerous or not and it has to raise alarm if its dangerous. Now if everything is fine and still the model predicts as dangerous situation then that’s a False alarm which you can say a False Positive Rate.\nFalse Positive Rate is just opposite of True Negative Rate \\[\nActual\\ negative\\ class = True\\ Negative + False\\ Positive\n\\]\n\\[\nFalse\\ positive\\ Rate = \\frac{False\\ Positive}{Actual\\ negative\\ class}\n\\]\n\\[\nFalse\\ Positive\\ Rate = 1 - True\\ Negative\\ Rate\n\\]\nThe lower the False Positive Rate the better the model.\n\n\n\nWhen the model doesn’t predict something which it should then it is contributing to the False Negative Rate. We can think it as Miss Rate. For example in Online fraud transaction if the model classifies a Fraud Transaction as a Non Fraud Transaction then the model basically missed to catch that Fraud transaction.\nFalse Negative Rate is just of True Positive Rate \\[\nActual\\ positive\\ class = True\\ Positive + False\\ Negative\n\\]\n\\[\nFalse\\ Negative\\ Rate = \\frac{False\\ Negative}{Actual\\ positive\\ class}\n\\]\n\\[\nFalse\\ Negative\\ Rate = 1 - True\\ Positive\\ Rate\n\\]\n\n\n\nFalse Discovery Rate is just opposite of Precision It measures how many predictions out of all positive predictions were incorrect. \\[\nPositive\\ class\\ prediction  = True\\ positive + False\\ Positive\n\\]\n\\[\nFalse\\ Discovery\\ Rate = \\frac{False\\ Positive}{positive\\ class\\ prediction}\n\\]\n\\[\nFalse\\ Discovery\\ Rate = 1 - Precision\n\\]\nWhen raising False alert is expensive it is worth looking every Positive prediction then we should optimize for precision.\n\n\n\nFalse Omission Rate is just opposite of Negative Predictive Value \\[\nFalse\\ Omission\\ Rate = 1 - Negative\\ Predictive\\ Value\n\\]\n\n\n\nNow that two important metric which is used often is precision and recall and rather then having too look two number F1 score combines precision and recall.\nThe score lies in the range [0,1] with 1 being ideal and 0 being the worst. The two ways to combine Precision and recall is \\[\nArithmetic\\ Mean\\\\\nF1\\ score = \\frac{precision + recall}{2}\n\\]\n\\[\nHarmonic\\ Mean\\\\\nF1\\ Score = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}}\n\\]\nThe reason to choose Harmonic mean over Arithmetic mean is precision and recall both have same numerator but different denominators so it makes no sense to average two different things as, fractions are more sensible to average by arithmetic mean when they have the same denominator. Rather we take reciprocal so that the average makes sense.\n\n\n\nIt’s a metric that combines precision and recall, putting 2x emphasis on recall. \\[\nF2\\ score = \\frac{1+2}{\\frac{2}{precision} + \\frac{1}{recall}}\n\\]\n\n\n\nF beta score is a general formula for F1 score and F2 score\nWhen choosing beta in your F-beta score the more you care about recall over precision the higher beta you should choose. For example, with F1 score we care equally about recall and precision with F2 score, recall is twice as important to us.\nWith 0&lt;beta&lt;1 we care more about precision \\[\nF beta \\ score = \\frac{1+\\beta}{\\frac{\\beta}{precision} + \\frac{1}{recall}}\n\\]\n\n\n\nmicro\nCalculate metrics globally by counting the total number of times each class was correctly predicted and incorrectly predicted. Micro Average captures class-imbalance and will bring down the precision\nFor example in Iris Dataset the model prediction result is given in the table\n\n\n\n\nTP\nFP\n\n\n\n\nSetosa\n45\n5\n\n\nVirgnica\n10\n60\n\n\nVersicolor\n40\n10\n\n\n\n\\[\nmicro\\ precision = \\frac{45 + 10 + 40}{45+ 10 + 40 + 5+60+10 }  = 0.55\n\\]\nmacro\nCalculate metrics for each “class” independently, and find their unweighted mean. This does not take label imbalance into account. In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class. \\[\nSetosa\\ precision = \\frac{45}{45+5} =0.9\\\\\nvirgnica\\ precision = \\frac{10}{10 + 60} =0.14\\\\\nversicolor\\ precision = \\frac{40}{40+10} = 0.8\\\\\n\\]\n\\[\nMacro\\ Precision = \\frac{0.9+0.14+0.8}{3} = 0.613\n\\]\nweighted accounts for class imbalance by computing the average of binary metrics in which each class’s score is weighted by its presence in the true data sample.\n\n\n\nPrecision-Recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.\nThe precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\nA system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly.\n\n\n\n\nA receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.\nThe top left corner of the plot is the “ideal” point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better.\nThe “steepness” of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate.\nIt can also be used for Mutli label classification problem.\n\n\n\n\nThe function cohen_kappa_score computes Cohen’s kappa statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.\nThe kappa score is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).\nKappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators.\nFor Kappa score formulae and calculation refer Cohen’s kappa\n\n\n\nThe Hamming loss is the fraction of labels that are incorrectly predicted.\nEvaluation metrics for multi-label classification performance are inherently different from those used in multi-class (or binary) classification, due to the inherent differences of the classification problem. If T denotes the true set of labels for a given sample, and P the predicted set of labels, then the following metrics can be defined on that sample:\nHamming loss: the fraction of the wrong labels to the total number of labels, i.e. \n\\[\nhamming\\ loss  = {\\frac {1}{|N|\\ . |L|}}\\sum_{i=1}^{|N|}\\sum_{j=1}^{|L|}xor (y_{i,j},z_{i,j})\n\\]\nwhere y_ij target and z_ij is the prediction. This is a loss function, so the optimal value is zero.\nHamming Loss computes Hamming distance and In information theory, the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other. In a more general context, the Hamming distance is one of several string metrics for measuring the edit distance between two sequences. It is named after the American mathematician Richard Hamming.\n\n\n\nTill Now For Binary Classification Problem we haven’t encountered any metric which incorporates all 4 parts of the confusion matrix and works good either we have balanced dataset or a Imbalanced one.\nMatthews Correlation Coefficient is the answer It is a more reliable statistical rate which produces high score only if the prediction obtained good results in all 4 parts of the confusion matrix.\nIt computes correlation coefficient between the true class and the predicted class the higher the correlation coefficient the better the model is at prediction.\nThe MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient.\n\\[\nMCC = \\frac{TP \\times TN - FP \\times FN }{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n\\]\nIf there is no False prediction, then the model has +1 as a correlation coefficient since (FP x FN = 0) vice-versa if (TP x TN = 0) then the model has -1 as a correlation coefficient.\nAdvantages of MCC over accuracy and F1 score\n\n\n\nAP summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight:\n\\[\nAP = \\sum_n{(R_n-R_{n-1}) P_n}\n\\]\nwhere Pn and Rn denotes the nth threshold.\nThis metric is also used in Object Detection.\n\nIntution Behind Average Precision\nWikipedia Average Precision\n\n\n\n\nBalanced Accuracy is metric used to deal with Imbalanced dataset. It is the average of Sensitivity and Specificity . In more generic term averaging recall of all classes.\nSensitivity covers the True Positive part and Specificity covers True Negative Part.\n\\[\nBalanced\\ Accuracy = \\frac{sensitivity + specificity}{2}\n\\]\n\n\n\nIn an ideal model, the probability scores of all true 1’s should be greater than the probability scores of ALL true 0’s. Such a model is said to be perfectly concordant and this phenomenon can be measured by Concordance and Discordance.\nSo how to calculate Concordance?\nLet’s consider the following 4 observation’s actual class and predicted probability scores.\n\n\n\nPatient No\nTrue Class\nProbability Score\n\n\n\n\nP1\n1\n0.9\n\n\nP2\n0\n0.42\n\n\nP3\n1\n0.30\n\n\nP4\n1\n0.80\n\n\n\nFrom the above 4 observations, there are 3 possible pairs of 1’s and 0’s. That is, P1-P2, P3-P2 and P4-P2.\nA pair is said to be concordant if the probability score of True 1 is greater than the probability score of True 0.\nP1-P2 =&gt; 0.9 &gt; 0.42 =&gt; Concordant! P3-P2 =&gt; 0.3 &lt; 0.42 =&gt; Discordant! P4-P2 =&gt; 0.8 &gt; 0.42 =&gt; Concordant!\nOut of the 3 pairs, only 2 are concordant. So, the concordance is 2/3 = 0.66 and discordance is 1 - 0.66 = 0.33.\nIn simpler words, we take all possible combinations of true events and non-events. Concordance is the percentage of pairs, where true event’s probability scores are greater than the scores of true non-events.\nFor a perfect model, this will be 100%. So, the higher the concordance, the better is the quality of the model."
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#example",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#example",
    "title": "Markdown Demo",
    "section": "",
    "text": "Let’s take an example of Binary classification where the task is to predict whether we have Dog or not. So in an Image if model predicts Dog that’s a positive class if it predicts no Dog that’s a negative class."
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#confusion-matrix",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#confusion-matrix",
    "title": "Markdown Demo",
    "section": "",
    "text": "A confusion matrix is a table that is used to describe the performance of a classification model.\n\nLet’s consider this example table where N denotes the total number of images our model will predict upon.\nN = 150 Total Number of Dog Images = 100 Total Number of No Dog Images = 50\nIn total 100 Dog Images 1. Model Predicted 60 Correct which is your True Positive. 2. Model Predicted 40 incorrect which is False Negative.(Since It has to predict Dog and It Predicted No Dog which is a False prediction to a Negative class.)\nSimilarly in 50 No Dog Images 1. Model Predicted 30 Correct which is True Negative. 2. Model Predicted 20 Incorrect which is False Positive. (Since It has to predict No Dog and It predicted Dog which is a False prediction to a Positive class.)\nTP-&gt;60 FN-&gt;40 TN-&gt;30 FP-&gt;20"
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#accuracy",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#accuracy",
    "title": "Markdown Demo",
    "section": "",
    "text": "Accuracy is a basic metric which just tells models overall performance. How many predictions made by model is correct.\n\\[\nAccuracy = \\frac{True Positive + True Negative}{N}\n\\]\n\\[\nAccuracy = \\frac{60+30}{150} = 0.6 = 60\n\\]\nAccuracy gave us a idea about how 60 % prediction was correct. Accuracy can only be a good metric. if all the classes are balanced i.e No of positive sample is approximately equal to No of negative samples. Per class Accuracy can also be calculated to know for which classes model is behaving well."
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#misclassification-rate",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#misclassification-rate",
    "title": "Markdown Demo",
    "section": "",
    "text": "Misclassification Rate tells overall how poor model performance is. It just opposite of Accuracy.\n\\[\nmisclassification\\ rate = 1 - Accuracy\n\\]\n\\[\nmisclassifcation\\ rate = \\frac{False Postive + False Negative}{N}\n\\]\n\\[\nmisclassification\\ rate = \\frac{20+40}{150} = 0.4 = 40\\%\n\\]"
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#precision-positive-predictive-value",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#precision-positive-predictive-value",
    "title": "Markdown Demo",
    "section": "",
    "text": "Precision is another metric which tells while predicting how accurately can I predict positive classes .\n\\[\nPositive\\ class\\ prediction  = True\\ positive + False\\ Positive\n\\]\n\\[\nprecision = \\frac{True\\ Positive}{positive\\ class\\ prediction}\n\\]\n\\[\nprecision = \\frac {60}{60 + 20} = 0.75 = 75\\%\n\\]\nWe are only worried about the prediction of one class which is Dog and the positive prediction class will have (Positive class, Negative class) from Actuals. That’s why it is positive predictive value.\nLet’s consider two example where\n\nSpam detection -&gt; Classify whether an email is Spam or Not Spam.\n​ Here the goal is to accurately classify spam emails. It’s okay to classify a Spam mail as Not Spam mail as it will come in our inbox it does no harm to use. But if we classify as Not Spam Mail as Spam mail then there is a problem because we generally do not open our Spam Box.\n​ If you think about it the first case is False Negative and Second case is False Positive and we are okay with False Negative’s but we are not okay with False Positives and our goal is to reduce False Positive.\n​ So in Spam detection task precision is a good metric. since it is inversely proportionate to False Positives.\nCancer detection -&gt; Classify whether a person has a cancer or not.\n​ Here the goal is to accurately classify whether a person has a cancer or Not. It’s okay to classify a person Not having cancer as cancer. But it’s not okay to predict a person having cancer as Not cancer.\n​ If you think the first case is False Positive and Second case is False Negative and we are okay with False Positive but not okay with False Negative.\n​ Hence in this particular task Precision plays no role.\n\nHence to reduce False Positives Precision is used. Precision can easily be effected by class Imbalance."
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#negative-predictive-value",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#negative-predictive-value",
    "title": "Markdown Demo",
    "section": "",
    "text": "Negative Predictive Value is another metric which tells while predicting how accurately can I predict Negative classes .\n\\[\nNegative\\ class\\ prediction  = True\\ Negative + False\\ Negative\n\\]\n\\[\nNegative\\ Prediction\\ Value = \\frac{True\\ Negative}{Negative\\ class\\ prediction}\n\\]\n\\[\nnegative\\ prediction\\ value = \\frac {60}{60 + 20} = 0.75 = 75\\%\n\\]\nWe are only worried about the prediction of one class which is Dog and the positive prediction class will have (Positive class, Negative class) from Actuals. That’s why it is positive predictive value.\nLet Suppose we don’t want to have any additional process for screening patients checked as healthy (not cancer) then we may want to make sure that our negative predictive value is high."
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#recall-true-positive-rate-sensitivity",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#recall-true-positive-rate-sensitivity",
    "title": "Markdown Demo",
    "section": "",
    "text": "Recall is another metric which tells us while predicting how accurately can it predict positive classes given a set of Actual Positives. \\[\nActual\\ positive\\ class = True\\ Positive + False\\ Negative\n\\]\n\\[\nrecall = \\frac{True\\ Positive}{Actual\\ positive\\ class}\n\\]\n\\[\nrecall = \\frac{60}{100} = 0.6 = 60\\%\n\\]\nHere our concern is about Given a set of Positive Samples which is like giving all the Dog images and then making prediction on it. The prediction will have (positive class , Negative class ) from Actual positives. That’s why recall is also True Positive Rate .\nThe reason why it’s called recall is given all the positive sample knowledge how well the model can recall that knowledge to predicted accurately by decreasing the error rate for the Actual positive class.\nGoing back to the Cancer Example it is very clear know that for Cancer Detection we will use Recall as our metric. Recall is good metric to be used for class Imbalance problem."
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#selectivity-true-negative-rate-specificity",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#selectivity-true-negative-rate-specificity",
    "title": "Markdown Demo",
    "section": "",
    "text": "Similar to True positive rate, True Negative rate tells us while predicting how accurately can it predict Negative classes given a set of Actual Negatives. \\[\nActual\\ negative\\ class = True\\ Negative + False\\ Positive\n\\]\n\\[\nTrue\\ Negative\\ Rate = \\frac{True\\ Negative}{Actual\\ negative\\ class}\n\\]\n\\[\nTrue\\ Negative\\ Rate = \\frac{60}{100} = 0.6 = 60\\%\n\\]\nHere our concern is about Given a set of Negative Samples which is like giving all the No Dog images and then making prediction on it. The prediction will have (positive class , Negative class ) from Actual Negatives.\nFor the same cancer example True Negative rate will show how many non cancer people are identified as not having cancer."
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#sensitivity-vs-specificity",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#sensitivity-vs-specificity",
    "title": "Markdown Demo",
    "section": "",
    "text": "n many tests, including diagnostic medical tests, sensitivity is the extent to which actual positives are not overlooked (so false negatives are few), and specificity is the extent to which actual negatives are classified as such (so false positives are few). Thus, a highly sensitive test rarely overlooks an actual positive (for example, showing “nothing bad” despite something bad existing); a highly specific test rarely registers a positive classification for anything that is not the target of testing (for example, finding one bacterial species and mistaking it for another closely related one that is the true target); and a test that is highly sensitive and highly specific does both, so it “rarely overlooks a thing that it is looking for” and it “rarely mistakes anything else for that thing.” Because most medical tests do not have sensitivity and specificity values above 99%, “rarely” does not equate to certainty. But for practical reasons, tests with sensitivity and specificity values above 90% have high credibility, albeit usually no certainty, in differential diagnosis.\nSensitivity, therefore, quantifies the avoidance of false negatives and specificity does the same for false positives."
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#false-positive-rate-type-i-error",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#false-positive-rate-type-i-error",
    "title": "Markdown Demo",
    "section": "",
    "text": "When the model predicts something Falsely to the positive class which then it is contributing to the False Positive rate. we can think of it as False alert. For example if in a production house based on certain machine parameters the model has to predict whether the situation insider the production house is dangerous or not and it has to raise alarm if its dangerous. Now if everything is fine and still the model predicts as dangerous situation then that’s a False alarm which you can say a False Positive Rate.\nFalse Positive Rate is just opposite of True Negative Rate \\[\nActual\\ negative\\ class = True\\ Negative + False\\ Positive\n\\]\n\\[\nFalse\\ positive\\ Rate = \\frac{False\\ Positive}{Actual\\ negative\\ class}\n\\]\n\\[\nFalse\\ Positive\\ Rate = 1 - True\\ Negative\\ Rate\n\\]\nThe lower the False Positive Rate the better the model."
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#false-negative-rate-type---ii-error",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#false-negative-rate-type---ii-error",
    "title": "Markdown Demo",
    "section": "",
    "text": "When the model doesn’t predict something which it should then it is contributing to the False Negative Rate. We can think it as Miss Rate. For example in Online fraud transaction if the model classifies a Fraud Transaction as a Non Fraud Transaction then the model basically missed to catch that Fraud transaction.\nFalse Negative Rate is just of True Positive Rate \\[\nActual\\ positive\\ class = True\\ Positive + False\\ Negative\n\\]\n\\[\nFalse\\ Negative\\ Rate = \\frac{False\\ Negative}{Actual\\ positive\\ class}\n\\]\n\\[\nFalse\\ Negative\\ Rate = 1 - True\\ Positive\\ Rate\n\\]"
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#false-discovery-rate",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#false-discovery-rate",
    "title": "Markdown Demo",
    "section": "",
    "text": "False Discovery Rate is just opposite of Precision It measures how many predictions out of all positive predictions were incorrect. \\[\nPositive\\ class\\ prediction  = True\\ positive + False\\ Positive\n\\]\n\\[\nFalse\\ Discovery\\ Rate = \\frac{False\\ Positive}{positive\\ class\\ prediction}\n\\]\n\\[\nFalse\\ Discovery\\ Rate = 1 - Precision\n\\]\nWhen raising False alert is expensive it is worth looking every Positive prediction then we should optimize for precision."
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#false-omission-rate",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#false-omission-rate",
    "title": "Markdown Demo",
    "section": "",
    "text": "False Omission Rate is just opposite of Negative Predictive Value \\[\nFalse\\ Omission\\ Rate = 1 - Negative\\ Predictive\\ Value\n\\]"
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#f-1-score-beta-1",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#f-1-score-beta-1",
    "title": "Markdown Demo",
    "section": "",
    "text": "Now that two important metric which is used often is precision and recall and rather then having too look two number F1 score combines precision and recall.\nThe score lies in the range [0,1] with 1 being ideal and 0 being the worst. The two ways to combine Precision and recall is \\[\nArithmetic\\ Mean\\\\\nF1\\ score = \\frac{precision + recall}{2}\n\\]\n\\[\nHarmonic\\ Mean\\\\\nF1\\ Score = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}}\n\\]\nThe reason to choose Harmonic mean over Arithmetic mean is precision and recall both have same numerator but different denominators so it makes no sense to average two different things as, fractions are more sensible to average by arithmetic mean when they have the same denominator. Rather we take reciprocal so that the average makes sense."
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#f-2-score-beta-2",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#f-2-score-beta-2",
    "title": "Markdown Demo",
    "section": "",
    "text": "It’s a metric that combines precision and recall, putting 2x emphasis on recall. \\[\nF2\\ score = \\frac{1+2}{\\frac{2}{precision} + \\frac{1}{recall}}\n\\]"
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#f-beta-score",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#f-beta-score",
    "title": "Markdown Demo",
    "section": "",
    "text": "F beta score is a general formula for F1 score and F2 score\nWhen choosing beta in your F-beta score the more you care about recall over precision the higher beta you should choose. For example, with F1 score we care equally about recall and precision with F2 score, recall is twice as important to us.\nWith 0&lt;beta&lt;1 we care more about precision \\[\nF beta \\ score = \\frac{1+\\beta}{\\frac{\\beta}{precision} + \\frac{1}{recall}}\n\\]"
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#averaging-parameter",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#averaging-parameter",
    "title": "Markdown Demo",
    "section": "",
    "text": "micro\nCalculate metrics globally by counting the total number of times each class was correctly predicted and incorrectly predicted. Micro Average captures class-imbalance and will bring down the precision\nFor example in Iris Dataset the model prediction result is given in the table\n\n\n\n\nTP\nFP\n\n\n\n\nSetosa\n45\n5\n\n\nVirgnica\n10\n60\n\n\nVersicolor\n40\n10\n\n\n\n\\[\nmicro\\ precision = \\frac{45 + 10 + 40}{45+ 10 + 40 + 5+60+10 }  = 0.55\n\\]\nmacro\nCalculate metrics for each “class” independently, and find their unweighted mean. This does not take label imbalance into account. In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class. \\[\nSetosa\\ precision = \\frac{45}{45+5} =0.9\\\\\nvirgnica\\ precision = \\frac{10}{10 + 60} =0.14\\\\\nversicolor\\ precision = \\frac{40}{40+10} = 0.8\\\\\n\\]\n\\[\nMacro\\ Precision = \\frac{0.9+0.14+0.8}{3} = 0.613\n\\]\nweighted accounts for class imbalance by computing the average of binary metrics in which each class’s score is weighted by its presence in the true data sample."
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#precision-recall-curve",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#precision-recall-curve",
    "title": "Markdown Demo",
    "section": "",
    "text": "Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.\nThe precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\nA system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly."
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#roc-auc-curve",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#roc-auc-curve",
    "title": "Markdown Demo",
    "section": "",
    "text": "A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.\nThe top left corner of the plot is the “ideal” point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better.\nThe “steepness” of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate.\nIt can also be used for Mutli label classification problem."
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#cohens-kappa",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#cohens-kappa",
    "title": "Markdown Demo",
    "section": "",
    "text": "The function cohen_kappa_score computes Cohen’s kappa statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.\nThe kappa score is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).\nKappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators.\nFor Kappa score formulae and calculation refer Cohen’s kappa"
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#hamming-loss",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#hamming-loss",
    "title": "Markdown Demo",
    "section": "",
    "text": "The Hamming loss is the fraction of labels that are incorrectly predicted.\nEvaluation metrics for multi-label classification performance are inherently different from those used in multi-class (or binary) classification, due to the inherent differences of the classification problem. If T denotes the true set of labels for a given sample, and P the predicted set of labels, then the following metrics can be defined on that sample:\nHamming loss: the fraction of the wrong labels to the total number of labels, i.e. \n\\[\nhamming\\ loss  = {\\frac {1}{|N|\\ . |L|}}\\sum_{i=1}^{|N|}\\sum_{j=1}^{|L|}xor (y_{i,j},z_{i,j})\n\\]\nwhere y_ij target and z_ij is the prediction. This is a loss function, so the optimal value is zero.\nHamming Loss computes Hamming distance and In information theory, the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other. In a more general context, the Hamming distance is one of several string metrics for measuring the edit distance between two sequences. It is named after the American mathematician Richard Hamming."
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#matthews-correlation-coefficient",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#matthews-correlation-coefficient",
    "title": "Markdown Demo",
    "section": "",
    "text": "Till Now For Binary Classification Problem we haven’t encountered any metric which incorporates all 4 parts of the confusion matrix and works good either we have balanced dataset or a Imbalanced one.\nMatthews Correlation Coefficient is the answer It is a more reliable statistical rate which produces high score only if the prediction obtained good results in all 4 parts of the confusion matrix.\nIt computes correlation coefficient between the true class and the predicted class the higher the correlation coefficient the better the model is at prediction.\nThe MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient.\n\\[\nMCC = \\frac{TP \\times TN - FP \\times FN }{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n\\]\nIf there is no False prediction, then the model has +1 as a correlation coefficient since (FP x FN = 0) vice-versa if (TP x TN = 0) then the model has -1 as a correlation coefficient.\nAdvantages of MCC over accuracy and F1 score"
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#average-precision-score",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#average-precision-score",
    "title": "Markdown Demo",
    "section": "",
    "text": "AP summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight:\n\\[\nAP = \\sum_n{(R_n-R_{n-1}) P_n}\n\\]\nwhere Pn and Rn denotes the nth threshold.\nThis metric is also used in Object Detection.\n\nIntution Behind Average Precision\nWikipedia Average Precision"
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#balanced-accuracy",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#balanced-accuracy",
    "title": "Markdown Demo",
    "section": "",
    "text": "Balanced Accuracy is metric used to deal with Imbalanced dataset. It is the average of Sensitivity and Specificity . In more generic term averaging recall of all classes.\nSensitivity covers the True Positive part and Specificity covers True Negative Part.\n\\[\nBalanced\\ Accuracy = \\frac{sensitivity + specificity}{2}\n\\]"
  },
  {
    "objectID": "posts/markdown-example/2020-05-06-metrics-blog.html#concordance-and-discordance",
    "href": "posts/markdown-example/2020-05-06-metrics-blog.html#concordance-and-discordance",
    "title": "Markdown Demo",
    "section": "",
    "text": "In an ideal model, the probability scores of all true 1’s should be greater than the probability scores of ALL true 0’s. Such a model is said to be perfectly concordant and this phenomenon can be measured by Concordance and Discordance.\nSo how to calculate Concordance?\nLet’s consider the following 4 observation’s actual class and predicted probability scores.\n\n\n\nPatient No\nTrue Class\nProbability Score\n\n\n\n\nP1\n1\n0.9\n\n\nP2\n0\n0.42\n\n\nP3\n1\n0.30\n\n\nP4\n1\n0.80\n\n\n\nFrom the above 4 observations, there are 3 possible pairs of 1’s and 0’s. That is, P1-P2, P3-P2 and P4-P2.\nA pair is said to be concordant if the probability score of True 1 is greater than the probability score of True 0.\nP1-P2 =&gt; 0.9 &gt; 0.42 =&gt; Concordant! P3-P2 =&gt; 0.3 &lt; 0.42 =&gt; Discordant! P4-P2 =&gt; 0.8 &gt; 0.42 =&gt; Concordant!\nOut of the 3 pairs, only 2 are concordant. So, the concordance is 2/3 = 0.66 and discordance is 1 - 0.66 = 0.33.\nIn simpler words, we take all possible combinations of true events and non-events. Concordance is the percentage of pairs, where true event’s probability scores are greater than the scores of true non-events.\nFor a perfect model, this will be 100%. So, the higher the concordance, the better is the quality of the model."
  },
  {
    "objectID": "non_job.html",
    "href": "non_job.html",
    "title": "Non Job Pursuits",
    "section": "",
    "text": "This non-job pursuits\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/jupyter-example/2020-06-01-profiling.html",
    "href": "posts/jupyter-example/2020-06-01-profiling.html",
    "title": "Jupyter Demo",
    "section": "",
    "text": "Introduction\nEvery Developer desires to make their code optimzied and efficient. A dream where the developers want that their code to execute faster, with no memory leakage on the production system.\nLet’s make this dream true…\nCreating Data-processing pipeline, writing new algorithms, Deploying Machine Learning models to server million users, Scientific calculation in astropyhsics, these are few areas where when we write code we want to profile every single line of code for two things 1. The amount of time it is taking to execute where our goal is to reduce the time taken a.k.a Time Complexity. 2. The memory consumption for execution of that code where our goal is to reduce the memory usage a.k.a Space complexity.\nThere always a trade-off between both of them some time we are fine with memory consumption but not with the time it takes and vice-versa based on the needs we check for the trade-off, but the best system is where we can reduce both space and time complexity.\n\n\nPremature Optimization is evil.\nEarly in developing Algorithms we should think less about these things because it can be counter-productive which can lead to premature optimization and its the root cause of all evil.\nSo first make it work then optimize it.\n\n\nMagic functions and tools\nWhile most of the data science experiments starts in Ipython Notebook. The Ipython enviroment gives us some magic functions which can be utilized to profile our code.\n\n%%timeit: Measuring time taken for the codeblock to run\n%lprun: Run code with the line-by-line profiler\n%mprun: Run code with the line-by-line memory profiler\n\nFor Tracing Memory Leakage we can use Pympler.\n\n\nCode\nimport numpy as np\n\n\n\n\nTimeit\nThe usage of timeit is very simple just put the magic method on the top of the cell and it will calculate the time taken to execute the cell.\nLet’s compare vectorized vs non-vectorized version of numpy code.\n\n\nCode\nnumber = np.random.randint(0,100,10000)\n\n\n\n\nCode\n%%timeit\ntotal = 0\nfor i in number:\n    total+=i\n\n\n3.11 ms ± 86.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\nCode\n%%timeit\nnumber.sum()\n\n\n14.9 µs ± 74.3 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n\n\nThe difference in the execution time is evident one.\nNon-vectorized code in Milliseconds, 10-3. Vectorized code in Microseconds, 10-6.\nVectorized code is the winner here.\n\n\nTiming Profiling with Lprun\nline_profiler is a package for doing line-by-line timing profiling of functions.\nInstall using\npip install line_profiler\nPython provides a builtin profiler, but we will be using Line profiler for reasons stated below.\nThe current profiling tools supported in Python 2.7 and later only time function calls. This is a good first step for locating hotspots in one’s program and is frequently all one needs to do to optimize the program. However, sometimes the cause of the hotspot is actually a single line in the function, and that line may not be obvious from just reading the source code. These cases are particularly frequent in scientific computing. Functions tend to be larger (sometimes because of legitimate algorithmic complexity, sometimes because the programmer is still trying to write FORTRAN code), and a single statement without function calls can trigger lots of computation when using libraries like numpy. cProfile only times explicit function calls, not special methods called because of syntax. Consequently, a relatively slow numpy operation on large arrays like this,\na[large_index_array] = some_other_large_array\nis a hotspot that never gets broken out by cProfile because there is no explicit function call in that statement.\nLineProfiler can be given functions to profile, and it will time the execution of each individual line inside those functions. In a typical workflow, one only cares about line timings of a few functions because wading through the results of timing every single line of code would be overwhelming. However, LineProfiler does need to be explicitly told what functions to profile.\n\n\nCode\n# once installed we have load the extension\n%load_ext line_profiler\n\n\n\n\nCode\ndef some_operation(x):\n    x = x **2\n    x = x +2\n    x = np.concatenate([x,x,x],axis=0)\n    return x\n\n\nNow the %lprun command will do a line-by-line profiling of any function–in this case, we need to tell it explicitly which functions we’re interested in profiling:\n\n\nCode\n%lprun -f some_operation some_operation(np.random.randn(100))\n\n\nTimer unit: 1e-06 s\n\nTotal time: 7.7e-05 s\nFile: &lt;ipython-input-30-80aca4fcfa96&gt;\nFunction: some_operation at line 1\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     1                                           def some_operation(x):\n     2         1         24.0     24.0     31.2      x = x **2\n     3         1         22.0     22.0     28.6      x = x +2\n     4         1         30.0     30.0     39.0      x = np.concatenate([x,x,x],axis=0)\n     5         1          1.0      1.0      1.3      return x\n\nThe source code of the function is printed with the timing information for each line. There are six columns of information.\n\nLine : The line number in the file.\nHits: The number of times that line was executed.\nTime: The total amount of time spent executing the line in the timer’s units. In the header information before the tables, you will see a line “Timer unit:” giving the conversion factor to seconds. It may be different on different systems.\nPer Hit: The average amount of time spent executing the line once in the timer’s units.\n% Time: The percentage of time spent on that line relative to the total amount of recorded time spent in the function.\nLine Contents: The actual source code. Note that this is always read from disk when the formatted results are viewed, not when the code was executed. If you have edited the file in the meantime, the lines will not match up, and the formatter may not even be able to locate the function for display.\n\n\n\nMemory Profiling with mprun\nThis is a python module for monitoring memory consumption of a process as well as line-by-line analysis of memory consumption for python programs.\nInstall\npip install -U memory_profiler\nThe only issue mprun doesn’t work on notebook rather on a python file so we will write the code in notebook %%file magic function we will write that into a file and execute mprun on it\n\n\nCode\n%load_ext memory_profiler\n\n\n\n\nCode\n%%file mprun.py\nimport numpy as np\ndef some_operation(x):\n    y = x **2\n    z = y +2\n    result = np.concatenate([x,y,z],axis=0)\n    return result\n\n\nOverwriting mprun.py\n\n\n\n\nCode\nfrom mprun import some_operation\n%mprun -f some_operation some_operation(np.random.randn(100000))\n\n\n\n\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     2     62.5 MiB     62.5 MiB   def some_operation(x):\n     3     63.3 MiB      0.8 MiB       y = x **2\n     4     64.0 MiB      0.8 MiB       z = y +2\n     5     66.3 MiB      2.3 MiB       result = np.concatenate([x,y,z],axis=0)\n     6     66.3 MiB      0.0 MiB       return result\n\nThe first column represents the line number of the code that has been profiled.\nThe second column (Mem usage) the memory usage of the Python interpreter after that line has been executed.\nThe third column (Increment) represents the difference in memory of the current line with respect to the last one.\nThe last column (Line Contents) prints the code that has been profiled.\n\n\n\nMemory Leakage using pympler\nPympler is a development tool to measure, monitor and analyze the memory behavior of Python objects in a running Python application.\nBy pympling a Python application, detailed insight in the size and the lifetime of Python objects can be obtained. Undesirable or unexpected runtime behavior like memory bloat and other “pymples” can easily be identified.\nPympler integrates three previously separate modules into a single, comprehensive profiling tool. The asizeof module provides basic size information for one or several Python objects, module muppy is used for on-line monitoring of a Python application and module Class Tracker provides off-line analysis of the lifetime of selected Python objects.\nA web profiling frontend exposes process statistics, garbage visualisation and class tracker statistics.\nHit table of content for tutorial\n\n\nRead More\nUnderstanding Python Memory Managment\nPython Garbage Collector\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog I am a aman.\n\n\n Back to top"
  }
]