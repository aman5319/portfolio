[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Aman Pandey",
    "section": "",
    "text": "Data Scientist with expertise in Natural Language Processing and Recommendation Engines.\nEnthusiastic about Math, Programming and Free Software. \n\nBioSkillsNon Job Pursuits\n\n\nI have worked majorly in Natual Language Processing, and Applied AI research across different industry domains with a global perspective. I’ve collaborated with international teams under cross-functional environments both in industry and academia.\nSomething I believe so deeply is Free software, and Free Education for everyone which lead me conduct multiple meetups, workshop, events. What I learned from the community I give back to the community.\nI advocate for open research. These days I am fueled by enthusiasm to do mathematics from completely different perspective.\n\n\nComputer Programming | Python | Java | C++ | PyTorch | GNU/Linux | Distributed and Parallel Programming | Large Scale Data Processing | AWS | Model Monitoring\nDeep Learning | Machine Learning | Bayesian Active Learning | Metric Learning | Multi-Task Learning | Transfer Learning\nAI Specalities | NLP | NLG | NLU | Recommendation Systems | Explainable AI\n\n\nMember in Free Software Movement Karnataka(FSMK 2015 - 2020) | Android Teacher for 7 days Bootcamp | Organized bootcamps in multiple colleges | Bringing Awarness and knowledge about the Open Eco-System to the society | Events and Conferences representing FSMK |\nPresident of College GNU/Linux group(Toggle 2018-2019) | Organizing Events and Workshops in college |\nEntirety.ai(Feb 2019 - Mar 2021) | Discuss on current trend and Hands on Experience in Deep Learning | Event Ambassador for Pie and AI Meetup Bangalore | Talks and Meetups in Bangalore | collaborating with other meetup group and companies like Nvidia and Amazon\nAdjunct faculty at Praxis Buisness School (July 2019 - July 2020) | Teaching Deep learning and Model Deployment to PG students |\nHaskell Study Group (2022) | Organized study group consisting of 50 members from world wide.| Learned Haskell and FP together\nPycon | Poster presentation(2019) | CFP Reviewer (2023) | CFP Mentorship (2023)\n100 Hour Certification Course in Astronomy and Astrophysics (2023 - Present) | M.P Birla Institute of Fundamental Research"
  },
  {
    "objectID": "posts/NLP/rnn_lstm_gru.html",
    "href": "posts/NLP/rnn_lstm_gru.html",
    "title": "RNN Models",
    "section": "",
    "text": "Code\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\nWe don’t use Normal MLP to handle textual data because they"
  },
  {
    "objectID": "posts/NLP/rnn_lstm_gru.html#backprop-for-w_y",
    "href": "posts/NLP/rnn_lstm_gru.html#backprop-for-w_y",
    "title": "RNN Models",
    "section": "BackProp For \\(W_y\\)",
    "text": "BackProp For \\(W_y\\)\n\\[\n\\frac{\\partial E_3}{\\partial W_y} = \\frac{\\partial E_3}{\\partial \\bar y_3}. \\frac{\\partial \\bar y_3}{\\partial W_y} \\\\\\frac{\\partial E_2}{\\partial W_y} = \\frac{\\partial E_2}{\\partial \\bar y_2}. \\frac{\\partial \\bar y_2}{\\partial W_y} \\\\\\frac{\\partial E_1}{\\partial W_y} = \\frac{\\partial E_1}{\\partial \\bar y_1}. \\frac{\\partial \\bar y_1}{\\partial W_y}\\\\\n\\]\nGeneral Equation \\[\n\\frac{\\partial E_N}{\\partial W_y} = \\frac{\\partial E_N}{\\partial \\bar y_N}. \\frac{\\partial \\bar y_N}{\\partial W_y}\n\\]"
  },
  {
    "objectID": "posts/NLP/rnn_lstm_gru.html#backprop-for-w_s",
    "href": "posts/NLP/rnn_lstm_gru.html#backprop-for-w_s",
    "title": "RNN Models",
    "section": "Backprop for \\(W_s\\)",
    "text": "Backprop for \\(W_s\\)\nAt time step t=3 the gradient contribution for \\[ {\\bar s_3}\\] is\n\\[\n\\frac{\\partial E_3}{\\partial W_x} = \\frac{\\partial E_3}{\\partial \\bar y_3}. \\frac{\\partial \\bar y_3}{\\partial \\bar s_3} . \\frac{\\partial \\bar s_3}{\\partial \\bar W_s}\n\\] At time step t=2 the gradient contribution for \\[ \\bar s_2 \\] is \\[\n\\frac{\\partial E_3}{\\partial W_s} = \\frac{\\partial E_3}{\\partial \\bar y_3}. \\frac{\\partial \\bar y_3}{\\partial \\bar s_3} . \\frac{\\partial \\bar s_3}{\\partial \\bar s_2} .\\frac{\\partial \\bar s_2}{\\partial \\bar W_s}\n\\] At time step t=1 the gradient contribution for \\[ \\bar s_1\\] is \\[\n\\frac{\\partial E_3}{\\partial W_s} = \\frac{\\partial E_3}{\\partial \\bar y_3}. \\frac{\\partial \\bar y_3}{\\partial \\bar s_3} . \\frac{\\partial \\bar s_3}{\\partial \\bar s_2} . \\frac{\\partial \\bar s_2}{\\partial \\bar s_1} . \\frac{\\partial \\bar s_1}{\\partial \\bar W_s}\n\\] After considering the contributions from all three states: \\[\\bar{s_3}\\] ,\\[\\bar{s_2}\\] and \\[\\bar{s_1}\\], we will accumulate them to find the final gradient calculation.\nThe following equation is the gradient contributing to the adjustment of \\[W_s\\] using Backpropagation Through Time: \\[\n\\frac{\\partial E_3}{\\partial W_s} = \\frac{\\partial E_3}{\\partial \\bar y_3}. \\frac{\\partial \\bar y_3}{\\partial \\bar s_3} . \\frac{\\partial \\bar s_3}{\\partial \\bar W_s} +\n\\\\\n\\frac{\\partial E_3}{\\partial \\bar y_3}. \\frac{\\partial \\bar y_3}{\\partial \\bar s_3} . \\frac{\\partial \\bar s_3}{\\partial \\bar s_2} .\\frac{\\partial \\bar s_2}{\\partial \\bar W_s}+\n\\\\\n\\frac{\\partial E_3}{\\partial \\bar y_3}. \\frac{\\partial \\bar y_3}{\\partial \\bar s_3} . \\frac{\\partial \\bar s_3}{\\partial \\bar s_2} . \\frac{\\partial \\bar s_2}{\\partial \\bar s_1} . \\frac{\\partial \\bar s_1}{\\partial \\bar W_s}\n\\] The General Equation is \\[\n\\frac{\\partial E_N}{\\partial W_s} = \\sum_{i=1}^N\\frac{\\partial E_N}{\\partial \\bar y_N}. \\frac{\\partial \\bar y_N}{\\partial \\bar s_i} . \\frac{\\partial \\bar s_i}{\\partial \\bar W_s}\n\\]"
  },
  {
    "objectID": "posts/NLP/rnn_lstm_gru.html#backprop-for-w_x",
    "href": "posts/NLP/rnn_lstm_gru.html#backprop-for-w_x",
    "title": "RNN Models",
    "section": "Backprop for \\(W_x\\)",
    "text": "Backprop for \\(W_x\\)\nAt time step t=3 the gradient contribution for \\[ \\bar s_3\\] is \\[\n\\frac{\\partial E_3}{\\partial W_x} = \\frac{\\partial E_3}{\\partial \\bar y_3}. \\frac{\\partial \\bar y_3}{\\partial \\bar s_3} . \\frac{\\partial \\bar s_3}{\\partial  W_x}\n\\] At time step t=2 the gradient contribution for \\[ \\bar s_2\\] is \\[\n\\frac{\\partial E_3}{\\partial W_x} = \\frac{\\partial E_3}{\\partial \\bar y_3}. \\frac{\\partial \\bar y_3}{\\partial \\bar s_3} . \\frac{\\partial \\bar s_3}{\\partial \\bar s_2} .\\frac{\\partial \\bar s_2}{\\partial  W_x}\n\\] At time step t=1 the gradient contribution for \\[ \\bar s_1\\] is \\[\n\\frac{\\partial E_3}{\\partial W_x} = \\frac{\\partial E_3}{\\partial \\bar y_3}. \\frac{\\partial \\bar y_3}{\\partial \\bar s_3} . \\frac{\\partial \\bar s_3}{\\partial \\bar s_2} . \\frac{\\partial \\bar s_2}{\\partial \\bar s_1} . \\frac{\\partial \\bar s_1}{\\partial W_x}\n\\] After considering the contributions from all three states: \\[\\bar{s_3}\\] ,\\[\\bar{s_2}\\] and \\[\\bar{s_1}\\], we will accumulate them to find the final gradient calculation.\nThe following equation is the gradient contributing to the adjustment of \\[W_s\\] using Backpropagation Through Time: \\[\n\\frac{\\partial E_3}{\\partial W_x} = \\frac{\\partial E_3}{\\partial \\bar y_3}. \\frac{\\partial \\bar y_3}{\\partial \\bar s_3} . \\frac{\\partial \\bar s_3}{\\partial  W_x}+\n\\\\\n\\frac{\\partial E_3}{\\partial \\bar y_3}. \\frac{\\partial \\bar y_3}{\\partial \\bar s_3} . \\frac{\\partial \\bar s_3}{\\partial \\bar s_2} .\\frac{\\partial \\bar s_2}{\\partial  W_x}\n+\n\\\\\n\\frac{\\partial E_3}{\\partial \\bar y_3}. \\frac{\\partial \\bar y_3}{\\partial \\bar s_3} . \\frac{\\partial \\bar s_3}{\\partial \\bar s_2} . \\frac{\\partial \\bar s_2}{\\partial \\bar s_1} . \\frac{\\partial \\bar s_1}{\\partial W_x}\n\\] The General Equation is \\[\n\\frac{\\partial E_N}{\\partial W_x} = \\sum_{i=1}^N\\frac{\\partial E_N}{\\partial \\bar y_N}. \\frac{\\partial \\bar y_N}{\\partial \\bar s_i} . \\frac{\\partial \\bar s_i}{\\partial \\bar W_x}\n\\] RNNs have a short memory and suffers from vanishing gradient problem."
  },
  {
    "objectID": "posts/NLP/rnn_lstm_gru.html#forget-gate",
    "href": "posts/NLP/rnn_lstm_gru.html#forget-gate",
    "title": "RNN Models",
    "section": "- Forget gate",
    "text": "- Forget gate\nFor the sake of this illustration, lets assume we are reading words in a piece of text, and want use an LSTM to keep track of grammatical structures, such as whether the subject is singular or plural. If the subject changes from a singular word to a plural word, we need to find a way to get rid of our previously stored memory value of the singular/plural state. In an LSTM, the forget gate lets us do this:\n\\[\\Gamma_f^{\\langle t \\rangle} = \\sigma(W_f[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_f)\\tag{1} \\]\nHere, \\(W_f\\) are weights that govern the forget gate’s behavior. We concatenate \\([a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}]\\) and multiply by \\(W_f\\). The equation above results in a vector \\(\\Gamma_f^{\\langle t \\rangle}\\) with values between 0 and 1. This forget gate vector will be multiplied element-wise by the previous cell state \\(c^{\\langle t-1 \\rangle}\\). So if one of the values of \\(\\Gamma_f^{\\langle t \\rangle}\\) is 0 (or close to 0) then it means that the LSTM should remove that piece of information (e.g. the singular subject) in the corresponding component of \\(c^{\\langle t-1 \\rangle}\\). If one of the values is 1, then it will keep the information."
  },
  {
    "objectID": "posts/NLP/rnn_lstm_gru.html#update-gate",
    "href": "posts/NLP/rnn_lstm_gru.html#update-gate",
    "title": "RNN Models",
    "section": "- Update gate",
    "text": "- Update gate\nOnce we forget that the subject being discussed is singular, we need to find a way to update it to reflect that the new subject is now plural. Here is the formulat for the update gate:\n\\[\\Gamma_u^{\\langle t \\rangle} = \\sigma(W_u[a^{\\langle t-1 \\rangle}, x^{\\{t\\}}] + b_u)\\tag{2} \\]\nSimilar to the forget gate, here \\(\\Gamma_u^{\\langle t \\rangle}\\) is again a vector of values between 0 and 1. This will be multiplied element-wise with \\(\\tilde{c}^{\\langle t \\rangle}\\), in order to compute \\(c^{\\langle t \\rangle}\\)."
  },
  {
    "objectID": "posts/NLP/rnn_lstm_gru.html#updating-the-cell",
    "href": "posts/NLP/rnn_lstm_gru.html#updating-the-cell",
    "title": "RNN Models",
    "section": "- Updating the cell",
    "text": "- Updating the cell\nTo update the new subject we need to create a new vector of numbers that we can add to our previous cell state. The equation we use is:\n\\[ \\tilde{c}^{\\langle t \\rangle} = \\tanh(W_c[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_c)\\tag{3} \\]\nFinally, the new cell state is:\n\\[ c^{\\langle t \\rangle} = \\Gamma_f^{\\langle t \\rangle}* c^{\\langle t-1 \\rangle} + \\Gamma_u^{\\langle t \\rangle} *\\tilde{c}^{\\langle t \\rangle} \\tag{4} \\]"
  },
  {
    "objectID": "posts/NLP/rnn_lstm_gru.html#output-gate",
    "href": "posts/NLP/rnn_lstm_gru.html#output-gate",
    "title": "RNN Models",
    "section": "- Output gate",
    "text": "- Output gate\nTo decide which outputs we will use, we will use the following two formulas:\n\\[ \\Gamma_o^{\\langle t \\rangle}=  \\sigma(W_o[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_o)\\tag{5}\\] \\[ a^{\\langle t \\rangle} = \\Gamma_o^{\\langle t \\rangle}* \\tanh(c^{\\langle t \\rangle})\\tag{6} \\]\nWhere in equation 5 you decide what to output using a sigmoid function and in equation 6 you multiply that by the \\(\\tanh\\) of the previous state.\nTo summarize : 1. Forget gate : Need to forget irrelevant parts of the previous state 2. Update Gate: Process current input and previous state to update cell state(c(t-1) to c(t)) 3. Updating the cell: Store relevant new information to cell state 4. Output gate : To output certain parts of cell state"
  },
  {
    "objectID": "posts/NLP/rnn_lstm_gru.html#thank-you",
    "href": "posts/NLP/rnn_lstm_gru.html#thank-you",
    "title": "RNN Models",
    "section": "Thank you",
    "text": "Thank you"
  },
  {
    "objectID": "posts/NLP/embeddings.html",
    "href": "posts/NLP/embeddings.html",
    "title": "Word Embeddings",
    "section": "",
    "text": "When you’re dealing with words in text, you end up with tens of thousands of word classes to analyze; one for each word in a vocabulary. Trying to one-hot encode these words is massively inefficient because most values in a one-hot vector will be set to zero. So, the matrix multiplication that happens in between a one-hot input vector and a first, hidden layer will result in mostly zero-valued hidden outputs.\nTo solve this problem and greatly increase the efficiency of our networks, we use what are called embeddings. Embeddings are just a fully connected layer like you’ve seen before. We call this layer the embedding layer and the weights are embedding weights. We skip the multiplication into the embedding layer by instead directly grabbing the hidden layer values from the weight matrix. We can do this because the multiplication of a one-hot encoded vector with a matrix returns the row of the matrix corresponding the index of the “on” input unit.\nBOW and TF-IDF are spare representation of Tokens. In contrast Embedding refer to dense vector representations of Tokens in a continuous vector space. These embeddings are used to represent words or other linguistic units in a way that captures semantic relationships and contextual information.\nEmbeddings are a fundamental component of many NLP applications, enabling models to understand and work with textual data in a way that captures semantic information and relationships between words. They have revolutionized the field of NLP and have significantly improved the performance of various NLP tasks.\n1. Word2Vec\n2. Glove\n3. FastText        \n4. ELMO\n\nInstead of doing the matrix multiplication, we use the weight matrix as a lookup table. We encode the words as integers, for example “heart” is encoded as 958, “mind” as 18094. Then to get hidden layer values for “heart”, you just take the 958th row of the embedding matrix. This process is called an embedding lookup and the number of hidden units is the embedding dimension.\nThere is nothing magical going on here. The embedding lookup table is just a weight matrix. The embedding layer is just a hidden layer. The lookup is just a shortcut for the matrix multiplication. The lookup table is trained just like any weight matrix.\nEmbeddings aren’t only used for words of course. You can use them for any model where you have a massive number of classes. A particular type of model called Word2Vec uses the embedding layer to find vector representations of words that contain semantic meaning.\n\n\nWord2Vec Efficient Estimation of Word Representations in Vector Space\nWord2Vec is one of the most popular technique to learn word embeddings using shallow neural network.\nWe’re going to train a simple neural network with a single hidden layer to perform a certain task, but then we’re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer–we’ll see that these weights are actually the “word vectors” that we’re trying to learn.\nDifferent model architectures that can be used with Word2Vec\n\nCBOW Neural Network Model\nSkipgram Neural Network Model\n\n\nDifferent ways to train Word2vec Model: 1. Heirarchial Softmax 2. Negative Sampling\n\n\n\neg: The quick brown fox jumps over the lazy dog \nWe’re going to train the neural network to do the following. Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.\nnearby - there is actually a “window size” parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total).\nWe’ll train the neural network to do this by feeding it word pairs found in our training documents. The network is going to learn the statistics from the number of times each pairing shows up. So, for example, the network is probably going to get many more training samples of (“Soviet”, “Union”) than it is of (“Soviet”, “Sasquatch”). When the training is finished, if you give it the word “Soviet” as input, then it will output a much higher probability for “Union” or “Russia” than it will for “Sasquatch”.\n\n\nSkipgram architecture consists of: 1. Embedding layer / Hidden Layer\nAn Embedding layer takes in a number of inputs, importantly: * num_embeddings – the size of the dictionary of embeddings, or how many rows you’ll want in the embedding weight matrix * embedding_dim – the size of each embedding vector; the embedding dimension.(300 features is what Google used in their published model trained on the Google news dataset (you can download it from here)\n\nSoftmax Output Layer The output layer will have output neuron (one per word in our vocabulary) will produce an output between 0 and 1, and the sum of all these output values will add up to 1.\n\n\nWorking 1. The input words are passed in as batches of input word tokens. 2. This will go into a hidden layer of linear units (our embedding layer). 3. Then, finally into a softmax output layer. We’ll use the softmax layer to make a prediction about the context words by sampling, as usual.\n\n\n\n\nWord2Vec network is a huge network in terms of parameters. Say we had word vectors with 300 components, and a vocabulary of 10,000 words. Recall that the neural network had two weight matrices–a hidden layer and output layer. Both of these layers would have a weight matrix with 300 x 10,000 = 3 million weights each!\nHence to reduce compute burden of the training process,the research paper proposed following two innovations: 1. Subsampling frequent words to decrease the number of training examples. 2. Modifying the optimization objective with a technique they called Negative Sampling,instead of normal cross entropy which causes each training sample to update only a small percentage of the model’s weights which makes training the network very inefficient.\n\n\nWords that show up often such as “the”, “of”, and “for” don’t provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. This process is called subsampling by Mikolov. For each word \\(w_i\\) in the training set, we’ll discard it with probability given by\n\\[ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} \\]\nwhere \\(t\\) is a threshold parameter and \\(f(w_i)\\) is the frequency of word \\(w_i\\) in the total dataset.\n\n\n\nAs discussed above,the the skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our millions or billions of training samples!\nNegative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them.\nWith negative sampling, we are instead going to randomly select just a small number of “negative” words (let’s say 5) to update the weights for. (In this context, a “negative” word is one for which we want the network to output a 0 for). We will also still update the weights for our “positive” word (which is the word “quick” in our current example).\nThe paper says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets.\n\n\n\nThe “negative samples” (that is, the 5 output words that we’ll train to output 0) are selected using a “unigram distribution”, where more frequent words are more likely to be selected as negative samples.\nModification in Cost Function\nWe change the loss function to only care about correct example and a small amount of wrong examples. \nFirst part of the Loss function:  we take the log-sigmoid of the inner product of the output word vector and the input word vector.\nSecond part of the Loss function:\nlet’s first look at\n\\[\\large \\sum_i^N \\mathbb{E}_{w_i \\sim P_n(w)}\\]\nThis means we’re going to take a sum over words \\(w_i\\) drawn from a noise distribution \\(w_i \\sim P_n(w)\\). The noise distribution is basically our vocabulary of words that aren’t in the context of our input word. In effect, we can randomly sample words from our vocabulary to get these words. \\(P_n(w)\\) is an arbitrary probability distribution though, which means we get to decide how to weight the words that we’re sampling. This could be a uniform distribution, where we sample all words with equal probability. Or it could be according to the frequency that each word shows up in our text corpus, the unigram distribution \\(U(w)\\). The authors found the best distribution to be \\(U(w)^{3/4}\\), empirically. The power makes less frequent words be sampled more often\nFinally, in\n\\[\\large \\log{\\sigma\\left(-u_{w_i}\\hspace{0.001em}^\\top v_{w_I}\\right)},\\]\nwe take the log-sigmoid of the negated inner product of a noise vector with the input vector.\n\nTo give you an intuition for what we’re doing here, remember that the sigmoid function returns a probability between 0 and 1. The first term in the loss pushes the probability that our network will predict the correct word \\(w_O\\) towards 1. In the second term, since we are negating the sigmoid input, we’re pushing the probabilities of the noise words towards 0.\n\n\n\n\n\nGLoVe\nMotivation\nThe Word2Vec -context window-based methods suffer from the disadvantage of not learning from the global corpus statistics. As a result, repetition and large-scale patterns may not be learned as well with these models.\nThis method combines elements from the two main word embedding models which existed when GloVe was proposed: 1. Count based approach - Global matrix factorization 2. Direct Prediction - Local context window methods\n\n\n\nIt is the process of using matrix factorization methods from linear algebra to perform rank reduction on a large term-frequency matrix. These matrices can be represented as : 1. Term-Document Frequency - rows are words and the columns are documents (or sometimes paragraphs) 2. Term-Term Frequencies - words on both axes and measure co-occurrence\nLatent semantic analysis (LSA) It is a technique in natural language processing for analyzing relationships between a set of documents and the terms they contain by applying Global matrix factorization to term-document frequency matrices. In LSA the high-dimensional matrix is reduced via singular value decomposition (SVD).\n\n\n\n\n\nSkip-gram model By passing a window over the corpus line-by-line and learning to predict either the surroundings of a given word\n\nContinuous Bag of Words Model (CBOW) Predict a word given its surroundings. Note the bag-of-words problem is often shortened to “CBOW”.\n\nSkip-gram: works well with small amount of the training data, represents well even rare words or phrases. CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words.\n\n\n\n\nGloVe technique improves on these previous methods by making changes in the following:\n\nCo-occurance Probabilities Instead of learning the raw co-occurrence probabilities, it may make more sense to learn ratios of these co-occurrence probabilities, which seem to better discriminate subtleties in term-term relevance.\n\nTo illustrate this, we borrow an example from their paper: suppose we wish to study the relationship between two words, i = ice and j = steam. We’ll do this by examining the co-occurrence probabilities of these words with various “probe” words.\nCo-occurrence probability of an arbitrary word i with an arbitrary word j to be the probability that word j appears in the context of word i.  X_i is defined as the number of times any word appears in the context of word i, so it’s defined as the sum over all words k of the number of times word k occurs in the context of word i.\nLet us take few probe words and see how does the ratio appears: 1. If we choose a probe word k = solid which is closely related to i = ice but not to j = steam, we expect the ratio P_{ik}/P_{jk} of co-occurrence probabilities to be large 2. If we choose a probe word k = gas we would expect the same ratio to be small, since steam is more closely related to gas than ice is. 3. If we choose a probe word k = water , which are closely related to both ice and steam, but not more to one than the other ,we expect our ratio to be close to 1 since there shouldn’t be any bias to one of ice or steam 4. If we choose a probe word k = fashion ,which are not closely related to either of the words in question, we expect our ratio to be close to 1 since there shouldn’t be any bias to one of ice or steam\n\nNoting that the ratio Pik /Pjk depends on three words i, j, and k, the most general model takes the form, \nIn this equation, the right-hand side is extracted from the corpus, and F may depend on some as-of-yet unspecified parameters. The number of possibilities for F is vast, but by enforcing a few desiderata we can select a unique choice. We have two word vectors which we’d like to discriminate between, and a context word vector which is used to this effect.So to encode information about the ratios between two words, the authors suggest using vector differences as inputs to our function Vector Difference Model \nSo now,vector difference of the two words i and j we’re comparing as an input instead of both of these words individually, since our output is a ratio between their co-occurrence probabilities with the context word.\nNow we have two arguments, the context word vector, and the vector difference of the two words we’re comparing. Since the authors wish to take scalar values to scalar values (note the ratio of probabilities is a scalar), the dot product of these two arguments is taken\n\nNext, note that for word-word co-occurrence matrices, the distinction between a word and a context word is arbitrary and that we are free to exchange the two roles.Our final model should be invariant under this relabeling, but above equation is not.\nAfter applying Homomorphism condition: \nwe arrive at the equation as in the paper \n\n\n\n\nThere is no concept of word analogies with algorithms like word2vec and Glove.They just suddenly emerge out of the model and training process.\n\n\n\n\nContinuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a dis-tinct vector to each word. This is a limitation,especially for languages with large vocabularies and many rare words.\nConsidering Unigram word model they are higher chances of getting rare words during testing because we can’t take infinite size corpous which includes all the words.\nHere each word is represented as a bag of character n-grams. A vector representation of each character n-gram is summed to get word vector.\nBy representing each word as a bag of character n-grams a SkipGram model with Negative sampling is trained.\nBy using a distinct vector representation for each word, the skipgram model ignores the internal structure of words.\nHence Fasttext comes with the approach of incorporating sub-word level information.\nSo breaking a token -&gt; “characteristics” into 3 token -&gt; char_, acter_, istics_\nand to make it even better they incorporate n-grams with the actual tokens in text. This adds more feature regarding the local context of how the words are used together.\nMore specifically they have used bi-gram.\nFor example:-\ntext = I love my country\n\nbi-gram feature = \"I\" , \"love\" , \"my\", \"country\" , \"I love\" , \"love my\" , \"my country\"\n \nHyperparameter choice for generating Fasttext embeddings\n\nGenerating fasttext embedding will take more time compared to word2vec model.\nAs the corpus size grows, the memory requirement grows too - the number of ngrams that get hashed into the same ngram bucket would grow. So the choice of hyperparameter controlling the total hash buckets including the n-gram min and max size have a bearing.\n\nThis N-gram feature was used to generate embeddings which then fasttext used to perform text classification which gave them phenomenal results.\n\n\n\nAll the input tokens for a document are passed to an Embedding layer after which they are averaged out to generate an embedding for the sentence. The Sentence embeddings is passed to a Fully Connected Softmax layer for classification.\nWhen the number of classes is large, computing the linear classifier is computationally expensive. Moreprecisely, the computational complexity is O(kh) where k is the number of classes and h the dimension of the text representation. In order to improve the running time, Hierarchical softmax is used droping the the computational complexity O(hlog2(k)).\nNormal Softmax \nHierarchical Softmax \nIn Hierarchical softmax the task is to form tree.\nGood Tree \nBad Tree \nHierarchical softmax is not an approximate optimization algorithm. It accelerates the optimization by adding human orchestrations which could be highly biased.\n\n\n\n\nThe point where word2vec, Glove and fasttext failed is, The meaning, semantics of a words changes with respect to the words it is surrounded by in a sentence and these models wheren’t able to represent that.\nFor example:- ” 1. Apple is in profit. 2. Apple is tasty.\nApple in first sentence refers to the company whereas apple in second sentence refers to fruit.\nIn word2vec model this one word with two meaning will be represented by same vector.\nTo solve this issue ELMo uses stacked bi-LSTM to generate embeddings for each words which is context dependent.\nso In the above example Apple in first and second case will have two different vector representation.\n  ELMo is a deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). These word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. They can be easily added to existing models and significantly improve the state of the art across a broad range of challenging NLP problems, including question answering, textual entailment and sentiment analysis.\n       \n    ELMo representations are:\n\n    * Contextual: The representation for each word depends on the entire context in which it is used.\n    * Deep: The word representations combine all layers of a deep pre-trained neural network.\n    * Direction: From both left-to-right and right-to-left directions.\nSince ELMo embeddings are trained in a unsupervised way. These embeddings can be used for downstream task in supervised way.\n\n\n\nCharacter-aware language models work at a character level. They model words as sequences of characters and use character-level neural networks, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs), to generate word embeddings directly from characters."
  },
  {
    "objectID": "posts/NLP/embeddings.html#word2vec",
    "href": "posts/NLP/embeddings.html#word2vec",
    "title": "Word Embeddings",
    "section": "",
    "text": "Word2Vec Efficient Estimation of Word Representations in Vector Space\nWord2Vec is one of the most popular technique to learn word embeddings using shallow neural network.\nWe’re going to train a simple neural network with a single hidden layer to perform a certain task, but then we’re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer–we’ll see that these weights are actually the “word vectors” that we’re trying to learn.\nDifferent model architectures that can be used with Word2Vec\n\nCBOW Neural Network Model\nSkipgram Neural Network Model\n\n\nDifferent ways to train Word2vec Model: 1. Heirarchial Softmax 2. Negative Sampling\n\n\n\neg: The quick brown fox jumps over the lazy dog \nWe’re going to train the neural network to do the following. Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.\nnearby - there is actually a “window size” parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total).\nWe’ll train the neural network to do this by feeding it word pairs found in our training documents. The network is going to learn the statistics from the number of times each pairing shows up. So, for example, the network is probably going to get many more training samples of (“Soviet”, “Union”) than it is of (“Soviet”, “Sasquatch”). When the training is finished, if you give it the word “Soviet” as input, then it will output a much higher probability for “Union” or “Russia” than it will for “Sasquatch”.\n\n\nSkipgram architecture consists of: 1. Embedding layer / Hidden Layer\nAn Embedding layer takes in a number of inputs, importantly: * num_embeddings – the size of the dictionary of embeddings, or how many rows you’ll want in the embedding weight matrix * embedding_dim – the size of each embedding vector; the embedding dimension.(300 features is what Google used in their published model trained on the Google news dataset (you can download it from here)\n\nSoftmax Output Layer The output layer will have output neuron (one per word in our vocabulary) will produce an output between 0 and 1, and the sum of all these output values will add up to 1.\n\n\nWorking 1. The input words are passed in as batches of input word tokens. 2. This will go into a hidden layer of linear units (our embedding layer). 3. Then, finally into a softmax output layer. We’ll use the softmax layer to make a prediction about the context words by sampling, as usual.\n\n\n\n\nWord2Vec network is a huge network in terms of parameters. Say we had word vectors with 300 components, and a vocabulary of 10,000 words. Recall that the neural network had two weight matrices–a hidden layer and output layer. Both of these layers would have a weight matrix with 300 x 10,000 = 3 million weights each!\nHence to reduce compute burden of the training process,the research paper proposed following two innovations: 1. Subsampling frequent words to decrease the number of training examples. 2. Modifying the optimization objective with a technique they called Negative Sampling,instead of normal cross entropy which causes each training sample to update only a small percentage of the model’s weights which makes training the network very inefficient.\n\n\nWords that show up often such as “the”, “of”, and “for” don’t provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. This process is called subsampling by Mikolov. For each word \\(w_i\\) in the training set, we’ll discard it with probability given by\n\\[ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} \\]\nwhere \\(t\\) is a threshold parameter and \\(f(w_i)\\) is the frequency of word \\(w_i\\) in the total dataset.\n\n\n\nAs discussed above,the the skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our millions or billions of training samples!\nNegative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them.\nWith negative sampling, we are instead going to randomly select just a small number of “negative” words (let’s say 5) to update the weights for. (In this context, a “negative” word is one for which we want the network to output a 0 for). We will also still update the weights for our “positive” word (which is the word “quick” in our current example).\nThe paper says that selecting 5-20 words works well for smaller datasets, and you can get away with only 2-5 words for large datasets.\n\n\n\nThe “negative samples” (that is, the 5 output words that we’ll train to output 0) are selected using a “unigram distribution”, where more frequent words are more likely to be selected as negative samples.\nModification in Cost Function\nWe change the loss function to only care about correct example and a small amount of wrong examples. \nFirst part of the Loss function:  we take the log-sigmoid of the inner product of the output word vector and the input word vector.\nSecond part of the Loss function:\nlet’s first look at\n\\[\\large \\sum_i^N \\mathbb{E}_{w_i \\sim P_n(w)}\\]\nThis means we’re going to take a sum over words \\(w_i\\) drawn from a noise distribution \\(w_i \\sim P_n(w)\\). The noise distribution is basically our vocabulary of words that aren’t in the context of our input word. In effect, we can randomly sample words from our vocabulary to get these words. \\(P_n(w)\\) is an arbitrary probability distribution though, which means we get to decide how to weight the words that we’re sampling. This could be a uniform distribution, where we sample all words with equal probability. Or it could be according to the frequency that each word shows up in our text corpus, the unigram distribution \\(U(w)\\). The authors found the best distribution to be \\(U(w)^{3/4}\\), empirically. The power makes less frequent words be sampled more often\nFinally, in\n\\[\\large \\log{\\sigma\\left(-u_{w_i}\\hspace{0.001em}^\\top v_{w_I}\\right)},\\]\nwe take the log-sigmoid of the negated inner product of a noise vector with the input vector.\n\nTo give you an intuition for what we’re doing here, remember that the sigmoid function returns a probability between 0 and 1. The first term in the loss pushes the probability that our network will predict the correct word \\(w_O\\) towards 1. In the second term, since we are negating the sigmoid input, we’re pushing the probabilities of the noise words towards 0."
  },
  {
    "objectID": "posts/NLP/embeddings.html#global-vector-for-word-representationglove",
    "href": "posts/NLP/embeddings.html#global-vector-for-word-representationglove",
    "title": "Word Embeddings",
    "section": "",
    "text": "GLoVe\nMotivation\nThe Word2Vec -context window-based methods suffer from the disadvantage of not learning from the global corpus statistics. As a result, repetition and large-scale patterns may not be learned as well with these models.\nThis method combines elements from the two main word embedding models which existed when GloVe was proposed: 1. Count based approach - Global matrix factorization 2. Direct Prediction - Local context window methods\n\n\n\nIt is the process of using matrix factorization methods from linear algebra to perform rank reduction on a large term-frequency matrix. These matrices can be represented as : 1. Term-Document Frequency - rows are words and the columns are documents (or sometimes paragraphs) 2. Term-Term Frequencies - words on both axes and measure co-occurrence\nLatent semantic analysis (LSA) It is a technique in natural language processing for analyzing relationships between a set of documents and the terms they contain by applying Global matrix factorization to term-document frequency matrices. In LSA the high-dimensional matrix is reduced via singular value decomposition (SVD).\n\n\n\n\n\nSkip-gram model By passing a window over the corpus line-by-line and learning to predict either the surroundings of a given word\n\nContinuous Bag of Words Model (CBOW) Predict a word given its surroundings. Note the bag-of-words problem is often shortened to “CBOW”.\n\nSkip-gram: works well with small amount of the training data, represents well even rare words or phrases. CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words.\n\n\n\n\nGloVe technique improves on these previous methods by making changes in the following:\n\nCo-occurance Probabilities Instead of learning the raw co-occurrence probabilities, it may make more sense to learn ratios of these co-occurrence probabilities, which seem to better discriminate subtleties in term-term relevance.\n\nTo illustrate this, we borrow an example from their paper: suppose we wish to study the relationship between two words, i = ice and j = steam. We’ll do this by examining the co-occurrence probabilities of these words with various “probe” words.\nCo-occurrence probability of an arbitrary word i with an arbitrary word j to be the probability that word j appears in the context of word i.  X_i is defined as the number of times any word appears in the context of word i, so it’s defined as the sum over all words k of the number of times word k occurs in the context of word i.\nLet us take few probe words and see how does the ratio appears: 1. If we choose a probe word k = solid which is closely related to i = ice but not to j = steam, we expect the ratio P_{ik}/P_{jk} of co-occurrence probabilities to be large 2. If we choose a probe word k = gas we would expect the same ratio to be small, since steam is more closely related to gas than ice is. 3. If we choose a probe word k = water , which are closely related to both ice and steam, but not more to one than the other ,we expect our ratio to be close to 1 since there shouldn’t be any bias to one of ice or steam 4. If we choose a probe word k = fashion ,which are not closely related to either of the words in question, we expect our ratio to be close to 1 since there shouldn’t be any bias to one of ice or steam\n\nNoting that the ratio Pik /Pjk depends on three words i, j, and k, the most general model takes the form, \nIn this equation, the right-hand side is extracted from the corpus, and F may depend on some as-of-yet unspecified parameters. The number of possibilities for F is vast, but by enforcing a few desiderata we can select a unique choice. We have two word vectors which we’d like to discriminate between, and a context word vector which is used to this effect.So to encode information about the ratios between two words, the authors suggest using vector differences as inputs to our function Vector Difference Model \nSo now,vector difference of the two words i and j we’re comparing as an input instead of both of these words individually, since our output is a ratio between their co-occurrence probabilities with the context word.\nNow we have two arguments, the context word vector, and the vector difference of the two words we’re comparing. Since the authors wish to take scalar values to scalar values (note the ratio of probabilities is a scalar), the dot product of these two arguments is taken\n\nNext, note that for word-word co-occurrence matrices, the distinction between a word and a context word is arbitrary and that we are free to exchange the two roles.Our final model should be invariant under this relabeling, but above equation is not.\nAfter applying Homomorphism condition: \nwe arrive at the equation as in the paper"
  },
  {
    "objectID": "posts/NLP/embeddings.html#word-analogies",
    "href": "posts/NLP/embeddings.html#word-analogies",
    "title": "Word Embeddings",
    "section": "",
    "text": "There is no concept of word analogies with algorithms like word2vec and Glove.They just suddenly emerge out of the model and training process."
  },
  {
    "objectID": "posts/NLP/embeddings.html#enriching-word-vectors-with-subword-information",
    "href": "posts/NLP/embeddings.html#enriching-word-vectors-with-subword-information",
    "title": "Word Embeddings",
    "section": "",
    "text": "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a dis-tinct vector to each word. This is a limitation,especially for languages with large vocabularies and many rare words.\nConsidering Unigram word model they are higher chances of getting rare words during testing because we can’t take infinite size corpous which includes all the words.\nHere each word is represented as a bag of character n-grams. A vector representation of each character n-gram is summed to get word vector.\nBy representing each word as a bag of character n-grams a SkipGram model with Negative sampling is trained.\nBy using a distinct vector representation for each word, the skipgram model ignores the internal structure of words.\nHence Fasttext comes with the approach of incorporating sub-word level information.\nSo breaking a token -&gt; “characteristics” into 3 token -&gt; char_, acter_, istics_\nand to make it even better they incorporate n-grams with the actual tokens in text. This adds more feature regarding the local context of how the words are used together.\nMore specifically they have used bi-gram.\nFor example:-\ntext = I love my country\n\nbi-gram feature = \"I\" , \"love\" , \"my\", \"country\" , \"I love\" , \"love my\" , \"my country\"\n \nHyperparameter choice for generating Fasttext embeddings\n\nGenerating fasttext embedding will take more time compared to word2vec model.\nAs the corpus size grows, the memory requirement grows too - the number of ngrams that get hashed into the same ngram bucket would grow. So the choice of hyperparameter controlling the total hash buckets including the n-gram min and max size have a bearing.\n\nThis N-gram feature was used to generate embeddings which then fasttext used to perform text classification which gave them phenomenal results.\n\n\n\nAll the input tokens for a document are passed to an Embedding layer after which they are averaged out to generate an embedding for the sentence. The Sentence embeddings is passed to a Fully Connected Softmax layer for classification.\nWhen the number of classes is large, computing the linear classifier is computationally expensive. Moreprecisely, the computational complexity is O(kh) where k is the number of classes and h the dimension of the text representation. In order to improve the running time, Hierarchical softmax is used droping the the computational complexity O(hlog2(k)).\nNormal Softmax \nHierarchical Softmax \nIn Hierarchical softmax the task is to form tree.\nGood Tree \nBad Tree \nHierarchical softmax is not an approximate optimization algorithm. It accelerates the optimization by adding human orchestrations which could be highly biased."
  },
  {
    "objectID": "posts/NLP/embeddings.html#elmo-embeddings-from-language-model",
    "href": "posts/NLP/embeddings.html#elmo-embeddings-from-language-model",
    "title": "Word Embeddings",
    "section": "",
    "text": "The point where word2vec, Glove and fasttext failed is, The meaning, semantics of a words changes with respect to the words it is surrounded by in a sentence and these models wheren’t able to represent that.\nFor example:- ” 1. Apple is in profit. 2. Apple is tasty.\nApple in first sentence refers to the company whereas apple in second sentence refers to fruit.\nIn word2vec model this one word with two meaning will be represented by same vector.\nTo solve this issue ELMo uses stacked bi-LSTM to generate embeddings for each words which is context dependent.\nso In the above example Apple in first and second case will have two different vector representation.\n  ELMo is a deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). These word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. They can be easily added to existing models and significantly improve the state of the art across a broad range of challenging NLP problems, including question answering, textual entailment and sentiment analysis.\n       \n    ELMo representations are:\n\n    * Contextual: The representation for each word depends on the entire context in which it is used.\n    * Deep: The word representations combine all layers of a deep pre-trained neural network.\n    * Direction: From both left-to-right and right-to-left directions.\nSince ELMo embeddings are trained in a unsupervised way. These embeddings can be used for downstream task in supervised way."
  },
  {
    "objectID": "posts/NLP/embeddings.html#character-aware-language-model",
    "href": "posts/NLP/embeddings.html#character-aware-language-model",
    "title": "Word Embeddings",
    "section": "",
    "text": "Character-aware language models work at a character level. They model words as sequences of characters and use character-level neural networks, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs), to generate word embeddings directly from characters."
  },
  {
    "objectID": "posts/Python/big_data_processing.html",
    "href": "posts/Python/big_data_processing.html",
    "title": "Big Data Processing",
    "section": "",
    "text": "My day-to-day work revolves around understanding the data, analysis, visualisation, and training models.\nMost of the time, I am dealing with huge datasets. One of the characteristics of any data is\nThe frequency can range from a couple of seconds to days to weeks, etc.\nAs a data scientist, I never have to be bothered about how exactly the data getting generated on the client side is to be stored, updated, and processed.\nIt comes to me ready for my models to train.\nIn this blog, we will talk about some data engineering stuff."
  },
  {
    "objectID": "posts/Python/big_data_processing.html#basic-terminologies",
    "href": "posts/Python/big_data_processing.html#basic-terminologies",
    "title": "Big Data Processing",
    "section": "Basic Terminologies",
    "text": "Basic Terminologies\nBefore we get into the core part, we will discuss some important terminologies. I will use AWS services to give my examples.\n\nData Lake -&gt; Any place where you can dump any type of data, i.e., video, text, files, images, or binary blobs, is a data lake. In the AWS context, S3 (simple storage service) is the tool for the purpose.\nData Warehouse -&gt; The data that is stored in the data lake has to be processed for some purpose, such as training models, reporting, or analysis; it could be anything. Processing the unstructured data from the data lake and storing it in a structured way at a particular place—that place is a data warehouse.\n\nNow keen observers can say, Can’t we store the process data in a data lake instead of a data warehouse? In that case, you are correct; we can store it.\nFor example, take a bunch of user queries stored in S3, process them, create a single CSV file, and again store it in S3.\nAlso, sometimes we want to store that processed data in a database, which is fine.\n\nBatch Processing -&gt; When data processing happens in batches or at scheduled intervals, for example, if we take all the user queries for a particular day and on night if we process all of them that is called batch processing.\n\n    2. Stream Processing -&gt; Processing of data in real-time the moment it is generated. For example, the moment a user submits a query we do some processing, that is called stream processing.\n\nEvent -&gt; When a user clicks a button, writes a query, or scrolls, everything is an event, and it depends completely on the business on a fundamental level what they want to call an event."
  },
  {
    "objectID": "posts/Python/big_data_processing.html#data-streaming-ingestion-and-processing.",
    "href": "posts/Python/big_data_processing.html#data-streaming-ingestion-and-processing.",
    "title": "Big Data Processing",
    "section": "Data Streaming, Ingestion and Processing.",
    "text": "Data Streaming, Ingestion and Processing.\nWhen a event happens how to process that event for example - when on instagram we do share, like and comment, couple of things need to be done, first the UI has to be updated, and the data has to stored in some type of database, then may be some real time metric has to be shown to the user if they have a business account.\nso when a event happens there can be n things which needs to be updated on the same time.\nHere comes Apache Kafka to Rescue\nApache Kafka is an open-source distributed event streaming platform. i.e in simple terms when millions of users are sending different types of queries, that has to be handled by different components and update different service, all of this can be done by Apache Kafka. It helps integrate data from a variety of sources and sinks\nKafka follows a publish-subscribe model, where data producers (publishers) send messages to Kafka topics, and data consumers (subscribers) can subscribe to those topics to receive and process the messages.\n\nTo draw an analogy we can take example of google map as we enter source and drop location map shows us the way but we have to go by ourself by driving or walking.\nSo, Kafka is often used in conjunction with stream processing frameworks like Apache Flink, Apache Beam, Kafka Streams etc to perform real-time data processing and store the processed data for future use.\nNow either we deploy apache kafka and manage by our own self our use AWS Kinesis. They both serve the same purpose.\nApache Flink - It is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.\n\nFor example -\n\nTake example of any Algo Trading bot for every news and fluctuation that happens inside the market the data has to be quickly processed and understood, flink would be a suitable candidate.\nCalculate metrics like click-through rates (CTR) for products.\nDetect patterns, such as identifying when users abandon their shopping carts.\nPerform sessionization to group user interactions into sessions for personalized recommendations.\nDetect anomalies or fraud in real-time, such as unusual purchase patterns.\n\nTo draw a similarity Kinesis Data Streams does the exact same thing.\nNote:- Neither Flink nor Kafka provide its own data-storage system, but provides data-source and sink connectors to systems.\nIf the data needs to stored some where directly may be Data Lake, Database, files. We can use Kafka Connect which connects Kafka with external data sources and sinks. It helps to ingest data.\n\nTo draw a similarity Amazon Kinesis Firehose provides the same functionality.\nAmazon Kinesis Firehose is a fully managed AWS service that simplifies the process of ingesting, transforming, and delivering real-time streaming data to other AWS services, such as Amazon S3, Amazon Redshift, or Amazon Elastic-search. Kinesis Firehose is primarily used for data ingestion and data loading into AWS services. It’s particularly useful for situations where you want to move data from a streaming source to AWS data storage or analytics services without complex processing.\nDebezium - It is an open-source platform for change data capture (CDC) that captures and streams database changes in real-time. It allows you to monitor and react to changes in your databases as they occur, making it a valuable tool for building real-time data pipelines, event-driven micro-services, and other applications that require access to fresh, up-to-date data.\nJust to sum it up we have seen how to stream events using kafka store it using kafka connect, some stream processing using Flink and react to certain changes in the database using debezium.\nWhen it comes selection of the Database we have plethora of option with pros and cons of each one."
  },
  {
    "objectID": "posts/Python/big_data_processing.html#data-storage-and-querying",
    "href": "posts/Python/big_data_processing.html#data-storage-and-querying",
    "title": "Big Data Processing",
    "section": "Data Storage and Querying",
    "text": "Data Storage and Querying\nStoring the processed data in Data Lake vs. Data Warehouse, often depends on factors like data volume, query performance requirements, and cost considerations.\nNow let suppose their is an example scenario where some data from data lake is processed\n\nEither we can store the processed data back to data lake.\nStore the processed data onto some Database.\n\nFor case 1 - we can have a big CSV or parquet file that has all the processed data, what are the use cases of the data\n\nUse directly in model training, analysis.\nOr there is need to query that particular data which is stored in the data lake may be run a SQL query to fetch a subset of the data in that case use Presto a distributed SQL query engine.\n\n\nPresto acts a single query engine for data engineers who struggle with managing multiple query languages and interfaces to siloed databases and storage, Presto is the fast and reliable engine that provides one simple ANSI SQL interface for all your data analytics and your open lakehouse.\non AWS- Athena gives the same functionality, actually Athena is built on top of presto.\nExample - let suppose we store some processed data into GB scale into S3 now what we can do is use Athena to query that data using SQL syntax.\nFor case 2- It is simple that we directly dump the data on to a Database which is designed for business use case some one is looking for. Amazon RedShift is a good option. Also it is a good for PB scale.\nMany times we just need data integration service where we just want to move data from different sources to one single source for different application. For example we want to perform some ETL on data stored in 5 different DBs to one single source, in that situation AWS provide Glue\n\nWe now know how to store the processed data and query it."
  },
  {
    "objectID": "posts/Python/big_data_processing.html#batch-processing",
    "href": "posts/Python/big_data_processing.html#batch-processing",
    "title": "Big Data Processing",
    "section": "Batch Processing",
    "text": "Batch Processing\nWe looked that we can use Flink to do stream processing now when it comes to batch processing of data we can use all time fav tool Apache Spark.\nBased on the data size you might need processing power from 1 node to multiples clusters.\nIn case we have a single computer with lot of compute power we can do parallel processing by our selves, without requiring any special library but when it comes to distributed processing using a matured library is always recommended.\nMost of the distributed system works in a Master-Slave configuration where there is one master node and multiple slave nodes. the master schedules and monitors the job and slave nodes actually perform the job.\nSince the work is distributed across cluster we need a cluster manager also, Apache Mesos or Kubernetes is used to manage the allocation of resources (CPU and memory) for Spark applications.\n\nOn AWS we can run spark jobs on Amazon EMR(Elastic Map Reduce)"
  },
  {
    "objectID": "posts/Python/big_data_processing.html#data-visualization",
    "href": "posts/Python/big_data_processing.html#data-visualization",
    "title": "Big Data Processing",
    "section": "Data visualization",
    "text": "Data visualization\nThe data is processed and stored in some place. There is need to visualize but before visualizing, we need to query the data. Querying depends on multiple points\n\nhow quickly we need the data.\nhow many people are querying it at a given moment.\nwhat is the size of the database we are querying.\n\nWe Take a step back on to data processing system, when a database is designed either it is designed for heavy read operation OLAP or Write operation OLTP\nOLTP (Online Transaction Processing):\n\nOLTP databases are designed for transactional processing and day-to-day operations.\nThey handle tasks like order processing, inventory management, and customer record updates.\nOLTP systems are optimized for write-heavy operations and maintaining data integrity.\nData consistency is critical in OLTP databases, and transactions are ACID-compliant (Atomicity, Consistency, Isolation, Durability).\nOLTP queries are relatively simple and involve tasks like INSERT, UPDATE, DELETE, and SELECT of individual records. Response times are typically low to ensure efficient transaction processing.\nSo banks, atms, ecommerce websites are using OLTP for many write operations.\n\nOLAP (Online Analytical Processing): 1. OLAP databases are designed for complex queries and reporting. 2. They are used for business intelligence, data analysis, and decision support systems. 3. OLAP systems are optimized for read-heavy operations and are well-suited for data analysis tasks. 4. The data is denormalized, meaning redundant data is stored to optimize query performance.\nNow that we know about data processing databases Let’s take two OLAP scenario- 1. Every morning some manager at linkedin wants to tracks some numbers may be Active users, interaction etc. 2. Uber Eats wants to show real time analytics to the restaurant owners for taking quick decisions.\nThe difference in both the use case is given a query how much time it has before it returns a result.\nIn case 1 - query take 5hrs, 6hrs hell even 10hrs we don’t care because it runs once everyday, here we can use any query engine or database we have Athena, Redshift, Postgres, MongoDb it doesn’t matter.\nIn case 2 - query has to execute within milliseconds. Here something different is needed we can’t rely on let say redshift to quickly query PB scale data and give result in milliseconds.\nLet me introduce Apache Pinot - Realtime distributed OLAP datastore, designed to answer OLAP queries with low latency.\nApache Pinot allows RealTime user facing analytics.\n\nHow do they do it? To make any read operation fast we have to index the data, if we slice and dice the data to maintain 100’s of indexes across different dimension we can make the read of different queries faster.\nOnce the query is executed use any data visualization tool to visualize the data we have many options Apache SuperSet, Amazon QuickSight, Tableau."
  },
  {
    "objectID": "posts/Python/big_data_processing.html#job-scheduling",
    "href": "posts/Python/big_data_processing.html#job-scheduling",
    "title": "Big Data Processing",
    "section": "Job Scheduling",
    "text": "Job Scheduling\nHow do we schedule our ETL(Extract Transform Load) jobs, we saw how spark can do batch processing, superset can do visualization for us. Last piece of the puzzle is how do we schedule the job based on conditions, triggers and at a particular time.\nApache Airflow is a full blown platform for orchestrating, scheduling, and monitoring complex workflows of data pipelines, ETL (Extract, Transform, Load) processes, and other data-related tasks. It provides a framework for defining, scheduling, and executing workflows as a directed acyclic graph (DAG), making it a powerful tool for automating and managing data workflows.\nIt also comes with parallel and distributed execution engine, error handling capabilities, monitoring and logging.\nApache Airflow can be said as Cron on steroids."
  },
  {
    "objectID": "posts/Python/big_data_processing.html#backup-and-archival-of-data",
    "href": "posts/Python/big_data_processing.html#backup-and-archival-of-data",
    "title": "Big Data Processing",
    "section": "Backup and Archival of Data",
    "text": "Backup and Archival of Data\nBackup and Archival are also important parts of Data Management\nAWS Backup is a centralized, fully managed backup service that streamlines the process of protecting your data across various AWS resources, including EC2 instances, RDS databases, EBS volumes, and more.\nAWS S3 Glacier complements AWS Backup by offering a cost-effective solution for long-term data archiving. Here are the key features of AWS S3 Glacier:\nLow-Cost Storage: S3 Glacier offers significantly lower storage costs compared to standard AWS S3 storage classes. It’s ideal for archiving data that is rarely accessed but needs to be retained for compliance or historical purposes.\nTiered Storage Options: Glacier provides different storage tiers to meet specific retrieval time requirements. You can choose between three retrieval options: Expedited, Standard, and Bulk, depending on the urgency of accessing archived data.\nData Lifecycle Policies: Automate data lifecycle management with policies that transition data from frequently accessed S3 storage classes to Glacier after a certain period. This helps optimize storage costs without manual intervention.\nVaults and Archives: S3 Glacier organizes data into “vaults,” and each vault can contain multiple “archives.” Archives are individual objects or files that can be stored, retrieved, and managed as needed.\nThis below picture can give insights on to how different tools interact with each other."
  },
  {
    "objectID": "posts/Python/big_data_processing.html#thank-you.",
    "href": "posts/Python/big_data_processing.html#thank-you.",
    "title": "Big Data Processing",
    "section": "Thank you.",
    "text": "Thank you."
  },
  {
    "objectID": "posts/Python/profiling.html",
    "href": "posts/Python/profiling.html",
    "title": "Memory and Time Profiling",
    "section": "",
    "text": "Introduction\nEvery Developer desires to make their code optimzied and efficient. A dream where the developers want that their code to execute faster, with no memory leakage on the production system.\nLet’s make this dream true…\nCreating Data-processing pipeline, writing new algorithms, Deploying Machine Learning models to server million users, Scientific calculation in astropyhsics, these are few areas where when we write code we want to profile every single line of code for two things 1. The amount of time it is taking to execute where our goal is to reduce the time taken a.k.a Time Complexity. 2. The memory consumption for execution of that code where our goal is to reduce the memory usage a.k.a Space complexity.\nThere always a trade-off between both of them some time we are fine with memory consumption but not with the time it takes and vice-versa based on the needs we check for the trade-off, but the best system is where we can reduce both space and time complexity.\n\n\nPremature Optimization is evil.\nEarly in developing Algorithms we should think less about these things because it can be counter-productive which can lead to premature optimization and its the root cause of all evil.\nSo first make it work then optimize it.\n\n\nMagic functions and tools\nWhile most of the data science experiments starts in Ipython Notebook. The Ipython enviroment gives us some magic functions which can be utilized to profile our code.\n\n%%timeit: Measuring time taken for the codeblock to run\n%lprun: Run code with the line-by-line profiler\n%mprun: Run code with the line-by-line memory profiler\n\nFor Tracing Memory Leakage we can use Pympler.\n\n\nCode\nimport numpy as np\n\n\n\n\nTimeit\nThe usage of timeit is very simple just put the magic method on the top of the cell and it will calculate the time taken to execute the cell.\nLet’s compare vectorized vs non-vectorized version of numpy code.\n\n\nCode\nnumber = np.random.randint(0,100,10000)\n\n\n\n\nCode\n%%timeit\ntotal = 0\nfor i in number:\n    total+=i\n\n\n3.11 ms ± 86.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\nCode\n%%timeit\nnumber.sum()\n\n\n14.9 µs ± 74.3 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n\n\nThe difference in the execution time is evident one.\nNon-vectorized code in Milliseconds, 10-3. Vectorized code in Microseconds, 10-6.\nVectorized code is the winner here.\n\n\nTiming Profiling with Lprun\nline_profiler is a package for doing line-by-line timing profiling of functions.\nInstall using\npip install line_profiler\nPython provides a builtin profiler, but we will be using Line profiler for reasons stated below.\nThe current profiling tools supported in Python 2.7 and later only time function calls. This is a good first step for locating hotspots in one’s program and is frequently all one needs to do to optimize the program. However, sometimes the cause of the hotspot is actually a single line in the function, and that line may not be obvious from just reading the source code. These cases are particularly frequent in scientific computing. Functions tend to be larger (sometimes because of legitimate algorithmic complexity, sometimes because the programmer is still trying to write FORTRAN code), and a single statement without function calls can trigger lots of computation when using libraries like numpy. cProfile only times explicit function calls, not special methods called because of syntax. Consequently, a relatively slow numpy operation on large arrays like this,\na[large_index_array] = some_other_large_array\nis a hotspot that never gets broken out by cProfile because there is no explicit function call in that statement.\nLineProfiler can be given functions to profile, and it will time the execution of each individual line inside those functions. In a typical workflow, one only cares about line timings of a few functions because wading through the results of timing every single line of code would be overwhelming. However, LineProfiler does need to be explicitly told what functions to profile.\n\n\nCode\n# once installed we have load the extension\n%load_ext line_profiler\n\n\n\n\nCode\ndef some_operation(x):\n    x = x **2\n    x = x +2\n    x = np.concatenate([x,x,x],axis=0)\n    return x\n\n\nNow the %lprun command will do a line-by-line profiling of any function–in this case, we need to tell it explicitly which functions we’re interested in profiling:\n\n\nCode\n%lprun -f some_operation some_operation(np.random.randn(100))\n\n\nTimer unit: 1e-06 s\n\nTotal time: 7.7e-05 s\nFile: &lt;ipython-input-30-80aca4fcfa96&gt;\nFunction: some_operation at line 1\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     1                                           def some_operation(x):\n     2         1         24.0     24.0     31.2      x = x **2\n     3         1         22.0     22.0     28.6      x = x +2\n     4         1         30.0     30.0     39.0      x = np.concatenate([x,x,x],axis=0)\n     5         1          1.0      1.0      1.3      return x\n\nThe source code of the function is printed with the timing information for each line. There are six columns of information.\n\nLine : The line number in the file.\nHits: The number of times that line was executed.\nTime: The total amount of time spent executing the line in the timer’s units. In the header information before the tables, you will see a line “Timer unit:” giving the conversion factor to seconds. It may be different on different systems.\nPer Hit: The average amount of time spent executing the line once in the timer’s units.\n% Time: The percentage of time spent on that line relative to the total amount of recorded time spent in the function.\nLine Contents: The actual source code. Note that this is always read from disk when the formatted results are viewed, not when the code was executed. If you have edited the file in the meantime, the lines will not match up, and the formatter may not even be able to locate the function for display.\n\n\n\nMemory Profiling with mprun\nThis is a python module for monitoring memory consumption of a process as well as line-by-line analysis of memory consumption for python programs.\nInstall\npip install -U memory_profiler\nThe only issue mprun doesn’t work on notebook rather on a python file so we will write the code in notebook %%file magic function we will write that into a file and execute mprun on it\n\n\nCode\n%load_ext memory_profiler\n\n\n\n\nCode\n%%file mprun.py\nimport numpy as np\ndef some_operation(x):\n    y = x **2\n    z = y +2\n    result = np.concatenate([x,y,z],axis=0)\n    return result\n\n\nOverwriting mprun.py\n\n\n\n\nCode\nfrom mprun import some_operation\n%mprun -f some_operation some_operation(np.random.randn(100000))\n\n\n\n\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     2     62.5 MiB     62.5 MiB   def some_operation(x):\n     3     63.3 MiB      0.8 MiB       y = x **2\n     4     64.0 MiB      0.8 MiB       z = y +2\n     5     66.3 MiB      2.3 MiB       result = np.concatenate([x,y,z],axis=0)\n     6     66.3 MiB      0.0 MiB       return result\n\nThe first column represents the line number of the code that has been profiled.\nThe second column (Mem usage) the memory usage of the Python interpreter after that line has been executed.\nThe third column (Increment) represents the difference in memory of the current line with respect to the last one.\nThe last column (Line Contents) prints the code that has been profiled.\n\n\n\nMemory Leakage using pympler\nPympler is a development tool to measure, monitor and analyze the memory behavior of Python objects in a running Python application.\nBy pympling a Python application, detailed insight in the size and the lifetime of Python objects can be obtained. Undesirable or unexpected runtime behavior like memory bloat and other “pymples” can easily be identified.\nPympler integrates three previously separate modules into a single, comprehensive profiling tool. The asizeof module provides basic size information for one or several Python objects, module muppy is used for on-line monitoring of a Python application and module Class Tracker provides off-line analysis of the lifetime of selected Python objects.\nA web profiling frontend exposes process statistics, garbage visualisation and class tracker statistics.\nHit table of content for tutorial\n\n\nRead More\nUnderstanding Python Memory Managment\nPython Garbage Collector\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Computer Vision/convolution.html",
    "href": "posts/Computer Vision/convolution.html",
    "title": "Convolution and Common architectures",
    "section": "",
    "text": "1. Regular Neural Nets don’t scale well to full images\nIn MNIST dataset,images are only of size 28x28x1 (28 wide, 28 high, 1 color channels), so a single fully-connected neuron in a first hidden layer of a regular Neural Network would have 28x28x1 = 786 weights. This amount still seems manageable,\nBut what if we move to larger images.\nFor example, an image of more respectable size, e.g. 200x200x3, would lead to neurons that have 200x200x3 = 120,000 weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting.\n2.Parameter Sharing  A feature detector that is useful in one part of the image is probably useful in another part of the image.Thus CNN are good in capturing translation invariance.\nSparsity of connections In each layer,each output value depends only on a small number of inputs.This makes CNN networks easy to train on smaller training datasets and is less prone to overfitting.\n2.3D volumes of neurons. Convolutional Neural Networks take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular, unlike a regular Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth."
  },
  {
    "objectID": "posts/Computer Vision/convolution.html#inception-v1",
    "href": "posts/Computer Vision/convolution.html#inception-v1",
    "title": "Convolution and Common architectures",
    "section": "Inception V1",
    "text": "Inception V1\nInception v1\n Problems this network tried to solve: 1. What is the right kernel size for convolution  A larger kernel is preferred for information that is distributed more globally, and a smaller kernel is preferred for information that is distributed more locally.  Ans- Filters with multiple sizes.The network essentially would get a bit “wider” rather than “deeper”   3. How to stack convolution which can be less computationally expensive  Stacking them naively computationally expensive.  Ans-Limit the number of input channels by adding an extra 1x1 convolution before the 3x3 and 5x5 convolutions   2. How to avoid overfitting in a very deep network  Very deep networks are prone to overfitting. It also hard to pass gradient updates through the entire network.  Ans-Introduce two auxiliary classifiers (The purple boxes in the image). They essentially applied softmax to the outputs of two of the inception modules, and computed an auxiliary loss over the same labels. The total loss function is a weighted sum of the auxiliary loss and the real loss.\nThe total loss used by the inception net during training.  total_loss = real_loss + 0.3 * aux_loss_1 + 0.3 * aux_loss_2  \n\nPoints to note\n\nUsed 9 Inception modules in the whole architecture, with over 100 layers in total! Now that is deep…\nNo use of fully connected layers! They use an average pool instead, to go from a 7x7x1024 volume to a 1x1x1024 volume. This saves a huge number of parameters.\nUses 12x fewer parameters than AlexNet.\nTrained on “a few high-end GPUs within a week”.\nIt achieved a top-5 error rate of 6.67%"
  },
  {
    "objectID": "posts/Computer Vision/convolution.html#inception-v2",
    "href": "posts/Computer Vision/convolution.html#inception-v2",
    "title": "Convolution and Common architectures",
    "section": "Inception V2",
    "text": "Inception V2\nRethinking the Inception Architecture for Computer Vision\nUpgrades were targeted towards: 1. Reducing representational bottleneck by replacing 5x5 convolution to two 3x3 convolution operations which further improves computational speed  The intuition was that, neural networks perform better when convolutions didn’t alter the dimensions of the input drastically. Reducing the dimensions too much may cause loss of information, known as a “representational bottleneck”   2. Using smart factorization method where they factorize convolutions of filter size nxn to a combination of 1xn and nx1 convolutions.  For example, a 3x3 convolution is equivalent to first performing a 1x3 convolution, and then performing a 3x1 convolution on its output. They found this method to be 33% more cheaper than the single 3x3 convolution."
  },
  {
    "objectID": "posts/Computer Vision/convolution.html#spatial-seperable-convolution",
    "href": "posts/Computer Vision/convolution.html#spatial-seperable-convolution",
    "title": "Convolution and Common architectures",
    "section": "Spatial Seperable Convolution",
    "text": "Spatial Seperable Convolution\n\nDivides a kernel into two, smaller kernels\n\nInstead of doing one convolution with 9 multiplications(parameters), we do two convolutions with 3 multiplications(parameters) each (6 in total) to achieve the same effect\n\nWith less multiplications, computational complexity goes down, and the network is able to run faster.\nThis was used in an architecture called Effnet showing promising results.\nThe main issue with the spatial separable convolution is that not all kernels can be “separated” into two, smaller kernels. This becomes particularly bothersome during training, since of all the possible kernels the network could have adopted, it can only end up using one of the tiny portion that can be separated into two smaller kernels."
  },
  {
    "objectID": "posts/Computer Vision/convolution.html#depthwise-convolution",
    "href": "posts/Computer Vision/convolution.html#depthwise-convolution",
    "title": "Convolution and Common architectures",
    "section": "Depthwise Convolution",
    "text": "Depthwise Convolution\n\nSay we need to increase the number of channels from 16 to 32 using 3x3 kernel. \nNormal Convolution  Total No of Parameters = 3 x 3 x 16 x 32 = 4608\n\nDepthwise Convolution\n\nDepthWise Convolution = 16 x [3 x 3 x 1]\nPointWise Convolution = 32 x [1 x 1 x 16]\n\nTotal Number of Parameters = 656\nMobile net uses depthwise seperable convolution to reduce the number of parameters"
  },
  {
    "objectID": "posts/Machine Learning/Regularization.html",
    "href": "posts/Machine Learning/Regularization.html",
    "title": "Improve Model’s Prediction power using Regularization",
    "section": "",
    "text": "A central problem in machine learning is how to make an algorithm that will perform well not just on the training data, but also on new inputs. The great OverFitting Problem.\nMany strategies used in machine learning are explicitly designed to reduce the test error, possibly at the expense of increased training error. These strategies are known collectively as regularization.\nOne of the easiet to say but harder to do things is to increase the amount of training data.\nRegularization increases training error but reduces generalization error hence more no of epochs are needed to get the desired result. Regularization helps to reduce overfitting of the model.\nThere are many regularization techniques used some but extra term in objective function and some but extra constraint on the model.\n\nL1/L2 regularizers\nDropOut\nLabel Smoothing\nData Augmentation\nEarly Stopping\nWeight Clipping and Gradient Clipping\nPruning\nNormalization\n\n\n\nL1 and L2 regularizers are some time known as weight decay.\nL1 Regularization works by adding an l1 norm to the cost function.\n\\[\nL1\\ Norm :\n||X||_1 = \\sum_i |x_i|\n\\] L2 Regularization works by adding an l2 norm to the cost function.\n\\[\nL2\\ Norm :\n||X||_2 = \\sqrt {\\sum_i |x_i^2|}\n\\]\nThe idea behind l1 and l2 norm is smaller weight generalizes the model better so both of these norm perform some kind of weight decay.\n\n\n\\[\n    C = any\\ loss\\ function  + \\frac{\\lambda}{2n}\\sum w^2\n\\]\nHere λ is a regularization parameter and n is the size of training data w is the weight.we are adding a sum of squares of all weights to the cost function which is scaled by λ/2n where λ &gt; 0.\nThe intitution behind the l2 reguarization is to make it so the network prefers to learn small weights. Large weights will only be allowed if they considerably improve the first part of the cost function.\nPut another way, regularization can be viewed as a way of compromising between finding small weights and minimizing the original cost function.\nThe relative importance of the two elements of the compromise depends on the value of λ: when λ is small we prefer to minimize the original cost function, but when λ is large we prefer small weights.\nUpdating weight formulae while backprop \\[\nw = w - {lr} \\frac{\\partial C}{\\partial w} - \\frac {{lr} \\lambda} {n} w\n\\]\n\\[\nw = \\left( 1 - \\frac{{lr}\\lambda } {n} \\right) w - {lr} \\frac{\\partial C}{\\partial w}\n\\]\nHere \\[\n\\left( 1 - \\frac{{lr} \\lambda } {n} \\right)\n\\] is the rescaling factor for weights or the weight decay factor.For very small λ value it is allowing big weights and if λ value is big it is penealizing the weights.\nWhy is this going on? Heuristically, if the cost function is unregularized, then the length of the weight vector is likely to grow, all other things being equal. Over time this can lead to the weight vector being very large indeed. This can cause the weight vector to get stuck pointing in more or less the same direction, since changes due to gradient descent only make tiny changes to the direction, when the length is long. I believe this phenomenon is making it hard for our learning algorithm to properly explore the weight space, and consequently harder to find good minima of the cost function.\n\n\n\n\\[\nC = any\\ loss\\ function  + \\frac{\\lambda}{n}\\sum_w |w|\n\\]\nL1 regularization is similar to l2 just the norm formulae changes from sum of squares to absolute value.\nUpdating weight formulae while backprop \\[\nw = w - {lr} \\frac{\\partial C}{\\partial w} - \\frac {{lr} \\lambda} {n} sign(w)\n\\]\nsign(w) is just the sign of the weight vector +1 for positive weights and -1 for negative weights\n\n\nIn both expressions the effect of regularization is to shrink the weights. This accords with our intuition that both kinds of regularization penalize large weights. But the way the weights shrink is different.\nIn L1 regularization, the weights shrink by a constant amount toward 0.\nIn L2 regularization, the weights shrink by an amount which is proportional to w. And so when a particular weight has a large magnitude, |w|, L1 regularization shrinks the weight much less than L2 regularization does.\nBy contrast, when |w| is small, L1 regularization shrinks the weight much more than L2 regularization. The net result is that L1 regularization tends to concentrate the weight of the network in a relatively small number of high-importance connections, while the other weights are driven toward zero.\nHence L1 regularization makes the Network Spare.\n\n\n\n\n\nDropout is another regularization techniques which is very simple to understand.\n\nSo it takes a probability p and based on the value of p it randomly disables that percentage of neuron.\nFor example if the dropout value is 0.3 on a layer. It will disable 30% neuron in the layer i.e zero the value of those neuron.\nWhile training with every batch a different set on neurons are disabled which is completely random.\nSo why does dropout increases the robustness of the model? Heuristically, when we dropout different sets of neurons, it’s rather like we’re training different neural networks. And so the dropout procedure is like averaging the effects of a very large number of different networks. The different networks will overfit in different ways, and so, hopefully, the net effect of dropout will be to reduce overfitting.\nFor example In cnn if the model is trained on dogs vs cats example and few particular neurons having higher weight, everytime the model witnesses the whiskers in the image it activates those neurons and we get cat. But what if those whiskers are no there then model fails significantly. so dropout forces the model to learn more attributes of the training data while training.\nwhen p = 0.5\nBy repeating dropout over and over, our network will learn a set of weights and biases. Of course, those weights and biases will have been learnt under conditions in which half the hidden neurons were dropped out. When we actually run the full network that means that twice as many hidden neurons will be active. To compensate for that, we halve the weights outgoing from the hidden neurons.\nif we Pytorch implementation of Dropout their , the outputs are scaled by a factor of \\[ 1/(1-p)​\\] during training. This means that during evaluation the module simply computes an identity function.\nThere is also DropConnect which is on similar lines as Dropout\n\n\n\n\n\n\n\n\n\n\n\nWhen we apply the cross-entropy loss to a classification task, we’re expecting true labels to have 1, while the others 0. In other words, we have no doubts that the true labels are true, and the others are not. Is that always true? Maybe not. Many manual annotations are the results of multiple participants. They might have different criteria. They might make some mistakes. They are human, after all. As a result, the ground truth labels we have had perfect beliefs on are possible wrong.\nThe impact of this on model is First, it may result in over-fitting: if the model learns to assign full probability to the ground truth label for each training example, it is not guaranteed to generalize. Second, it encourages the differences between the largest logit and all others to become large, and this, combined with the bounded gradient reduces the ability of the model to adapt. Intuitively, this happens because the model becomes too confident about its predictions.\nOne possibile solution to this is to relax our confidence on the labels. For instance, we can slighly lower the loss target values from 1 to, say, 0.9. And naturally we increase the target value of 0 for the others slightly as such. This idea is called label smoothing.\nCheck the result of imagenet model after applying on imagenet\nPytorch Supports it in Cross Entropy Loss"
  },
  {
    "objectID": "posts/Machine Learning/Regularization.html#l1l2-regularizers",
    "href": "posts/Machine Learning/Regularization.html#l1l2-regularizers",
    "title": "Improve Model’s Prediction power using Regularization",
    "section": "",
    "text": "L1 and L2 regularizers are some time known as weight decay.\nL1 Regularization works by adding an l1 norm to the cost function.\n\\[\nL1\\ Norm :\n||X||_1 = \\sum_i |x_i|\n\\] L2 Regularization works by adding an l2 norm to the cost function.\n\\[\nL2\\ Norm :\n||X||_2 = \\sqrt {\\sum_i |x_i^2|}\n\\]\nThe idea behind l1 and l2 norm is smaller weight generalizes the model better so both of these norm perform some kind of weight decay.\n\n\n\\[\n    C = any\\ loss\\ function  + \\frac{\\lambda}{2n}\\sum w^2\n\\]\nHere λ is a regularization parameter and n is the size of training data w is the weight.we are adding a sum of squares of all weights to the cost function which is scaled by λ/2n where λ &gt; 0.\nThe intitution behind the l2 reguarization is to make it so the network prefers to learn small weights. Large weights will only be allowed if they considerably improve the first part of the cost function.\nPut another way, regularization can be viewed as a way of compromising between finding small weights and minimizing the original cost function.\nThe relative importance of the two elements of the compromise depends on the value of λ: when λ is small we prefer to minimize the original cost function, but when λ is large we prefer small weights.\nUpdating weight formulae while backprop \\[\nw = w - {lr} \\frac{\\partial C}{\\partial w} - \\frac {{lr} \\lambda} {n} w\n\\]\n\\[\nw = \\left( 1 - \\frac{{lr}\\lambda } {n} \\right) w - {lr} \\frac{\\partial C}{\\partial w}\n\\]\nHere \\[\n\\left( 1 - \\frac{{lr} \\lambda } {n} \\right)\n\\] is the rescaling factor for weights or the weight decay factor.For very small λ value it is allowing big weights and if λ value is big it is penealizing the weights.\nWhy is this going on? Heuristically, if the cost function is unregularized, then the length of the weight vector is likely to grow, all other things being equal. Over time this can lead to the weight vector being very large indeed. This can cause the weight vector to get stuck pointing in more or less the same direction, since changes due to gradient descent only make tiny changes to the direction, when the length is long. I believe this phenomenon is making it hard for our learning algorithm to properly explore the weight space, and consequently harder to find good minima of the cost function.\n\n\n\n\\[\nC = any\\ loss\\ function  + \\frac{\\lambda}{n}\\sum_w |w|\n\\]\nL1 regularization is similar to l2 just the norm formulae changes from sum of squares to absolute value.\nUpdating weight formulae while backprop \\[\nw = w - {lr} \\frac{\\partial C}{\\partial w} - \\frac {{lr} \\lambda} {n} sign(w)\n\\]\nsign(w) is just the sign of the weight vector +1 for positive weights and -1 for negative weights\n\n\nIn both expressions the effect of regularization is to shrink the weights. This accords with our intuition that both kinds of regularization penalize large weights. But the way the weights shrink is different.\nIn L1 regularization, the weights shrink by a constant amount toward 0.\nIn L2 regularization, the weights shrink by an amount which is proportional to w. And so when a particular weight has a large magnitude, |w|, L1 regularization shrinks the weight much less than L2 regularization does.\nBy contrast, when |w| is small, L1 regularization shrinks the weight much more than L2 regularization. The net result is that L1 regularization tends to concentrate the weight of the network in a relatively small number of high-importance connections, while the other weights are driven toward zero.\nHence L1 regularization makes the Network Spare."
  },
  {
    "objectID": "posts/Machine Learning/Regularization.html#dropout",
    "href": "posts/Machine Learning/Regularization.html#dropout",
    "title": "Improve Model’s Prediction power using Regularization",
    "section": "",
    "text": "Dropout is another regularization techniques which is very simple to understand.\n\nSo it takes a probability p and based on the value of p it randomly disables that percentage of neuron.\nFor example if the dropout value is 0.3 on a layer. It will disable 30% neuron in the layer i.e zero the value of those neuron.\nWhile training with every batch a different set on neurons are disabled which is completely random.\nSo why does dropout increases the robustness of the model? Heuristically, when we dropout different sets of neurons, it’s rather like we’re training different neural networks. And so the dropout procedure is like averaging the effects of a very large number of different networks. The different networks will overfit in different ways, and so, hopefully, the net effect of dropout will be to reduce overfitting.\nFor example In cnn if the model is trained on dogs vs cats example and few particular neurons having higher weight, everytime the model witnesses the whiskers in the image it activates those neurons and we get cat. But what if those whiskers are no there then model fails significantly. so dropout forces the model to learn more attributes of the training data while training.\nwhen p = 0.5\nBy repeating dropout over and over, our network will learn a set of weights and biases. Of course, those weights and biases will have been learnt under conditions in which half the hidden neurons were dropped out. When we actually run the full network that means that twice as many hidden neurons will be active. To compensate for that, we halve the weights outgoing from the hidden neurons.\nif we Pytorch implementation of Dropout their , the outputs are scaled by a factor of \\[ 1/(1-p)​\\] during training. This means that during evaluation the module simply computes an identity function.\nThere is also DropConnect which is on similar lines as Dropout"
  },
  {
    "objectID": "posts/Machine Learning/Regularization.html#label-smoothing",
    "href": "posts/Machine Learning/Regularization.html#label-smoothing",
    "title": "Improve Model’s Prediction power using Regularization",
    "section": "",
    "text": "When we apply the cross-entropy loss to a classification task, we’re expecting true labels to have 1, while the others 0. In other words, we have no doubts that the true labels are true, and the others are not. Is that always true? Maybe not. Many manual annotations are the results of multiple participants. They might have different criteria. They might make some mistakes. They are human, after all. As a result, the ground truth labels we have had perfect beliefs on are possible wrong.\nThe impact of this on model is First, it may result in over-fitting: if the model learns to assign full probability to the ground truth label for each training example, it is not guaranteed to generalize. Second, it encourages the differences between the largest logit and all others to become large, and this, combined with the bounded gradient reduces the ability of the model to adapt. Intuitively, this happens because the model becomes too confident about its predictions.\nOne possibile solution to this is to relax our confidence on the labels. For instance, we can slighly lower the loss target values from 1 to, say, 0.9. And naturally we increase the target value of 0 for the others slightly as such. This idea is called label smoothing.\nCheck the result of imagenet model after applying on imagenet\nPytorch Supports it in Cross Entropy Loss"
  },
  {
    "objectID": "posts/Machine Learning/Regularization.html#early-stopping",
    "href": "posts/Machine Learning/Regularization.html#early-stopping",
    "title": "Improve Model’s Prediction power using Regularization",
    "section": "Early Stopping",
    "text": "Early Stopping\nMonitor the model’s performance on a validation set during training and stop when the performance starts degrading. This prevents the model from overfitting the training data.\nHere you can look at the code to implement the same link"
  },
  {
    "objectID": "posts/Machine Learning/Regularization.html#weight-clipping-and-gradient-clipping",
    "href": "posts/Machine Learning/Regularization.html#weight-clipping-and-gradient-clipping",
    "title": "Improve Model’s Prediction power using Regularization",
    "section": "Weight Clipping and Gradient Clipping",
    "text": "Weight Clipping and Gradient Clipping\n\nLimit the magnitude of weights in the network to prevent them from becoming too large. This can be achieved using techniques like weight clipping.\nLimit the gradients during training to prevent exploding gradients. This is especially useful in recurrent neural networks (RNNs).\n\nopt = optim.SGD(model.parameters(), lr=0.1)\nfor i in range(1000):\n    out = model(inputs)\n    loss = loss_fn(out, labels)\n    print(i, loss.item())\n    opt.zero_grad()\n    loss.backward()\n    # standard way of clipping the gradient\n    torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n    \n    # another way of doing it\n    # https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html#clip_grad_value_\n    for param in model.parameters():\n        param.grad.clamp_(-1, 1)  # weight clipping in range of -1 to 1\n\n    opt.step()\n    with torch.no_grad():\n        for param in model.parameters():\n            param.clamp_(-1, 1)  # weight clipping in range of -1 to 1"
  },
  {
    "objectID": "posts/Machine Learning/Regularization.html#pruning",
    "href": "posts/Machine Learning/Regularization.html#pruning",
    "title": "Improve Model’s Prediction power using Regularization",
    "section": "Pruning",
    "text": "Pruning\n Pruning is a technique that removes weights or biases (parameters) from a neural network model. Now there are many ways of doing it based on different criteria and what the need is overall if done properly , makes the model training/inference fast, better generalization, resource friendly.\nFollow few tutorials\nTutorial\nDoc"
  },
  {
    "objectID": "posts/Machine Learning/Regularization.html#thank-you.",
    "href": "posts/Machine Learning/Regularization.html#thank-you.",
    "title": "Improve Model’s Prediction power using Regularization",
    "section": "Thank you.",
    "text": "Thank you."
  },
  {
    "objectID": "posts/Machine Learning/Attention.html",
    "href": "posts/Machine Learning/Attention.html",
    "title": "Different types of Attentions",
    "section": "",
    "text": "A way to selectively focus on important subset of input from the bigger set.\n\n\n\n\\[\n(context\\ Vector)\\ c_t = \\sum_{i=1}^{j} \\alpha_{t,i}* h_i ;\\ where\\ \\alpha\\ is\\ attention\\ vector\\ and\\ h_i\\ is\\ RNN\\ output\\ at\\ i\\ time\\ step\n\\]\n\\[\n\\alpha_{t,i} = align(y_t,x_i)\\ ;\\ how\\ well\\ two\\ words\\ y_t\\ and\\ x_i\\ are\\ aligned\n\\]\n\\[\nE_t = v_t^T * tanh(W_a*S_{t-1} + U_a * h_i)\\ ;\\      W_a, U_a, v_t^T are\\ all\\ weight\\ parameters\n\\]\n\\[\n\\alpha_{t,i} = softmax(E_t)\\ ;\\ Location\\ Based\n\\]\n\nInitially we pass the entire sequence and generate two things the combined Hidden weights called output which in this case concat(h1 ,h2, h3, h4) let refer this as encoder_output and the decoder hidden state which z .\nWe generate a the attention based context vector as\n1. Add encoder_output, decoder_hidden_state\n2. Apply tanh and then multiply transpose of vt\n3. Apply mask to the Et vector if you don't want to put attention on padding\n4. Apply softmax to Et which gives the attention vector\n5. Multiply attention vector with encoder_output to get context vector\n\nIn decoder step\n\nConcat the input embedding for decoder sequence with context vector and pass as the input to Decoder RNN\nConcat the embedding , Decoder RNN output and Context Vector and pass it to linear layer to get the output.\nAt each time the context vector is updated using above steps with new hidden state for next time step\n\nimport torch.nn as nn\nimport torch\n\nclass BahdanauAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.w = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.u = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.v = nn.Linear(hidden_dim, 1)\n\n    def forward(self, decoder_hidden_state, encoder_output):\n        # decoder_hidden_state, encoder_output -&gt; (batch_size, seq_len, hidden_dim)\n\n        energy = self.v(torch.tanh(self.u(decoder_hidden_state) + self.v(encoder_output))).squeeze(-1)\n        #energy = (batch_size, seq_len)\n  \n        attn = nn.functional.softmax(energy, dim=-1).unsqueeze(1)\n        # attn = (batch_size, 1, seq_len)\n        context = torch.bmm(attn, encoder_output)\n        # context (batch_size, 1,hidden_dim)\n        return context, attn\n \n\n\n\n\\[\nscore(h_t , h_s^-)\\  ;\\ where\\ h_t\\ is\\ currect\\ decoder\\ hidden\\ state\\ and\\ h_s^-\\ is\\ encoder\\ output\n\\]\nIn loung paper there are basically 3 types of scoring function\n\\[\nscore(h_t , h_s^-) = h_t^T h_s^-\\ ; dot\\ product\n\\]\nThe Intuition behind this is dot product is a cosine similarity measure.\nIn this scoring function we are making an assumption that both the hidden states share the same embedding space so this will work for text summarization but on machine translation it will fail so we add weight matrix Wa\n\nAfter calculating scoring function which are then given as input to the softmax function to generate a attention vector.\nand then calculate the context vector using the same formulae\n\\[\n(context\\ Vector)\\ c_t = \\sum_{i=1}^{j} \\alpha_{t,i}* h_i\n\\]\nThe after the context vector is generated in the decoder step\n1. we concat the the context vector with current decoder hidden state which is then passed to linear layer with tanh acctivation to generate the output.\n\\[\nh_t^- = tanh(W_c[c_t;h_t])\n\\]\n1. The next step whatever output we get is input for decoder RNN for next time step with new hidden state, calucate again context vector and repeat the step.\nThe difference between the bahdanau paper is the simplicity in the steps\n\\[\nfrom\\ h_t -&gt; a_t -&gt; c_t-&gt;h_t^- \\\\\nbut\\ in\\ bahdanau\\ paper\\\\\nfrom\\ h_{t-1} -&gt; a_t -&gt; c_t-&gt;h_t\n\\]\nNote in the bahdanau paper\n\\[\nh_{t-1}\n\\]\nwhich means before feeding to decoder RNN calculate context vector but in loung paper\n\\[\nh_t\n\\]\nmeans attention vector is compute on current decoder RNN outputted hidden state.\nLoung Attention is all about matrix Multiplication so it is fast to implement but Additive Attention works well with larger sequence when compared to loung attention and also a little computation expensive.\nclass LoungAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.wa = nn.Linear(hidden_dim,hidden_dim,bias=False)\n\n    def forward(self, decoder_hidden_state, encoder_output) :\n        # decoder_hidden_state, encoder_output -&gt; (batch_size, seq_len, hidden_dim)\n\n        batch_size,input_size, hidden_dim,  = encoder_output.size()\n        score = torch.bmm(decoder_hidden_state, self.wa(encoder_output).transpose(1,2))\n        # score -&gt;  (batch_size, seq_len, seq_len)\n   \n        attn = nn.functional.softmax(score.view(-1, input_size), dim=1).view(batch_size, -1, input_size)\n        # score -&gt;  (batch_size, seq_len, seq_len)\n        context = torch.bmm(attn, encoder_output)\n        # context -&gt; (batch_size, seq_len, hidden_dim)\n        return context, attn\nNow Both the Attentions are Global attention because both uses a global constant encoder output to make decision and only the changing part is decoder hidden state. The global attention has a drawback that it has to attend to all words on the source side for each tar-get word, which is expensive and can potentially render it impractical to translate longer sequences,e.g., paragraphs or documents. To address this deficiency, A local attentional mechanism that chooses to focus only on a small subset of the source positions per target word Self Attention.\n\nThe “soft” vs “hard” attention is another way to categorize how attention is defined. The original idea was proposed in the show, attend and tell paper. Based on whether the attention has access to the entire image or only a patch:\n\nSoft\nAttention: the alignment weights are learned and placed “softly” over all patches in the source image; essentially the same idea as in\n\nPro: the model is smooth and differentiable.\nCon: expensive when the source input is large.\n\nHard\nAttention: only selects one patch of the image to attend to at a time.\n\nPro: less calculation at the inference time.\nCon: the model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train.\n\n\n\n\n\n\n\n\nSay the following sentence is an input sentence we want to translate:\n”The animal didn't cross the street because it was too tired”\nWhat does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.\nWhen the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.\nAs the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\nSelf-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.\n\nAs we are encoding the word “it” in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on “The Animal”, and baked a part of its representation into the encoding of “it”.\nWithin self attention there are three Matrices Query Matrix (Q), Key Matrix (K), Value Matrix(V) .\nLet’s say we have source sentence src = India is a great country and after tokenizng this sentence and passing it down by an embedding layer we have a matrix of\n\\[\n[5,256]\n\\]\nwhere 256 is the hidden dimension.\n\\[\nsrc=embedding(src)\n\\]\nNow with self attention we have a special property where the shape of\n\\[\nQ,K,V = [256,256]\\ all\\ of\\ them\\ have\\ same\\ shape\n\\]\nThe formulae for self attention is.\n\n\\[\nAttention(Q,K,V) = softmax(\\frac {Q.K^T} {\\sqrt{d_{model}}})V\n\\]\nLet’s see the first step where Q and K are multiplied we know when two vectors are multiplied there similarity is governed by cos function where\n\\[\n\\theta = 0\n\\]\nmeans two vectors are similar and\n\\[\n\\theta = -1\n\\]\nmeans two vectors are opposite \nSo when we are multiplying two matrices we found out where we want to put out attention. (Note because of linear layer we will putting attentions at different vectors.) By putting Softmax we are just making sure similar vectors have a high probability and dissimilar vectors are have a minuscule value.\nNow when we again multiply with V we get a matrix were the vectors were we want to put attention has a high value compared to the vector where we don’t want to put attention.\nThe\n\\[\n\\sqrt{d_{model}}\n\\]\nis just a scaling factor used in Multi Head Attention. where\n\\[\nd_{model} =\\frac {hid\\_dim}{n\\_heads}\n\\]\n\n\n\n\nTo Boost the self Attention Mechanism Multi Head Attention was introduced where we have multiple heads. We have a source sentence src and if n_heads=8 then 8 separate query, key, value Matrices are present.\nwe calculate self Attention on all 8 heads and then we concat it and pass it to a linear Layer.\nThe Reason for Multi Head Attention is\n\nIt expands the model’s ability to focus on different positions.\nIt gives the attention layer multiple “representation subspaces”.\n\n\nPytorch Multi Head Attention\n \n\n\n\nFlash attention is Fast and Memory-Efficient Exact Attention with IO-Awareness\nNote - Read This Before moving forward - Making Deep Learning Go Brrrr From First Principles\nTransformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length.\nTo see the problem\nQuery -&gt; Batch_size, seq_len,hidden_dim\nKey -&gt; Batch_size, seq_len, hidden_dim\nUsually the batch_size and hidden_dim are parameters with small numbers.\nWhen we Batch matrix Multiplication the result is Batch_size, seq_len, seq_len.\nThe bigger the seq_len the bigger the bigger the memory complexity.\nVarious past methods tried to reduce the FLOP but their implementation was not that successful here in flash attention the intention is reduce the memory access(IO).\nThe excerpt from the paper which states things very clearly.\n ” We propose FlashAttention, a new attention algorithm that computes exact attention with far fewer memory accesses.\nOur main goal is to avoid reading and writing the attention matrix to and from HBM.\nThis requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large intermediate attention matrix for the backward pass.\nWe apply two well-established techniques to address these challenges. (i) We restructure the attention computation to split the input into blocks and make several passes over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We store the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the backward pass, which is faster than the standard approach of reading the intermediate attention matrix from HBM.\nNow the problem would be clear that most of the time is spent in Loading the data from HBM and Saving the data to HBM. Calculation of softmax over a large input. ”\n\nIf we try to understand from this picture itself without looking the algo it says,\n\nHBM is storage is big. SRAM is smaller, hence the entire Q or K or V can’t be loaded fully.\nBreak it into smaller chunks of respectable size which can fit on SRAM. here d is no of heads and N is seq_len\nThere is an outer loop which loads K,V in chunks to SRAM.\n\nThere is an inner loop which loads Q in chunks to SRAM.\nThe entire self-attention computation with softmax happens.\nThe output block is written to HBM.\n\n\nThe IO complexity of flash attention -\n\n\nThey save softmax statistics over blocks which over many iteration approximates the correct softmax values which in their algo they call it tilling.\nDuring backward pass softmax inputs and outputs are needed they recompute those inputs and outputs from the final output and softmax normalization statistics from blocks of Q K V in SRAM. This can be seen as a form of selective gradient checkpointing.\nLet suppose there is bigger transformer architecture which has got lot of activations in forward pass we compute these activations and save each of the intermediary results. During backward pass when we access those intermediary results their is huge memory footprint to load these value from HBM to SRAM, to in gradient checkpointing rather then storing all of the intermediary, we keep only a few and rest we compute on fly, because compute is faster, much faster then loading the data too and fro from HBM."
  },
  {
    "objectID": "posts/Machine Learning/Attention.html#bahdanau-additive-attention",
    "href": "posts/Machine Learning/Attention.html#bahdanau-additive-attention",
    "title": "Different types of Attentions",
    "section": "",
    "text": "\\[\n(context\\ Vector)\\ c_t = \\sum_{i=1}^{j} \\alpha_{t,i}* h_i ;\\ where\\ \\alpha\\ is\\ attention\\ vector\\ and\\ h_i\\ is\\ RNN\\ output\\ at\\ i\\ time\\ step\n\\]\n\\[\n\\alpha_{t,i} = align(y_t,x_i)\\ ;\\ how\\ well\\ two\\ words\\ y_t\\ and\\ x_i\\ are\\ aligned\n\\]\n\\[\nE_t = v_t^T * tanh(W_a*S_{t-1} + U_a * h_i)\\ ;\\      W_a, U_a, v_t^T are\\ all\\ weight\\ parameters\n\\]\n\\[\n\\alpha_{t,i} = softmax(E_t)\\ ;\\ Location\\ Based\n\\]\n\nInitially we pass the entire sequence and generate two things the combined Hidden weights called output which in this case concat(h1 ,h2, h3, h4) let refer this as encoder_output and the decoder hidden state which z .\nWe generate a the attention based context vector as\n1. Add encoder_output, decoder_hidden_state\n2. Apply tanh and then multiply transpose of vt\n3. Apply mask to the Et vector if you don't want to put attention on padding\n4. Apply softmax to Et which gives the attention vector\n5. Multiply attention vector with encoder_output to get context vector\n\nIn decoder step\n\nConcat the input embedding for decoder sequence with context vector and pass as the input to Decoder RNN\nConcat the embedding , Decoder RNN output and Context Vector and pass it to linear layer to get the output.\nAt each time the context vector is updated using above steps with new hidden state for next time step\n\nimport torch.nn as nn\nimport torch\n\nclass BahdanauAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.w = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.u = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.v = nn.Linear(hidden_dim, 1)\n\n    def forward(self, decoder_hidden_state, encoder_output):\n        # decoder_hidden_state, encoder_output -&gt; (batch_size, seq_len, hidden_dim)\n\n        energy = self.v(torch.tanh(self.u(decoder_hidden_state) + self.v(encoder_output))).squeeze(-1)\n        #energy = (batch_size, seq_len)\n  \n        attn = nn.functional.softmax(energy, dim=-1).unsqueeze(1)\n        # attn = (batch_size, 1, seq_len)\n        context = torch.bmm(attn, encoder_output)\n        # context (batch_size, 1,hidden_dim)\n        return context, attn"
  },
  {
    "objectID": "posts/Machine Learning/Attention.html#loung-multiplicative-attention",
    "href": "posts/Machine Learning/Attention.html#loung-multiplicative-attention",
    "title": "Different types of Attentions",
    "section": "",
    "text": "\\[\nscore(h_t , h_s^-)\\  ;\\ where\\ h_t\\ is\\ currect\\ decoder\\ hidden\\ state\\ and\\ h_s^-\\ is\\ encoder\\ output\n\\]\nIn loung paper there are basically 3 types of scoring function\n\\[\nscore(h_t , h_s^-) = h_t^T h_s^-\\ ; dot\\ product\n\\]\nThe Intuition behind this is dot product is a cosine similarity measure.\nIn this scoring function we are making an assumption that both the hidden states share the same embedding space so this will work for text summarization but on machine translation it will fail so we add weight matrix Wa\n\nAfter calculating scoring function which are then given as input to the softmax function to generate a attention vector.\nand then calculate the context vector using the same formulae\n\\[\n(context\\ Vector)\\ c_t = \\sum_{i=1}^{j} \\alpha_{t,i}* h_i\n\\]\nThe after the context vector is generated in the decoder step\n1. we concat the the context vector with current decoder hidden state which is then passed to linear layer with tanh acctivation to generate the output.\n\\[\nh_t^- = tanh(W_c[c_t;h_t])\n\\]\n1. The next step whatever output we get is input for decoder RNN for next time step with new hidden state, calucate again context vector and repeat the step.\nThe difference between the bahdanau paper is the simplicity in the steps\n\\[\nfrom\\ h_t -&gt; a_t -&gt; c_t-&gt;h_t^- \\\\\nbut\\ in\\ bahdanau\\ paper\\\\\nfrom\\ h_{t-1} -&gt; a_t -&gt; c_t-&gt;h_t\n\\]\nNote in the bahdanau paper\n\\[\nh_{t-1}\n\\]\nwhich means before feeding to decoder RNN calculate context vector but in loung paper\n\\[\nh_t\n\\]\nmeans attention vector is compute on current decoder RNN outputted hidden state.\nLoung Attention is all about matrix Multiplication so it is fast to implement but Additive Attention works well with larger sequence when compared to loung attention and also a little computation expensive.\nclass LoungAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.wa = nn.Linear(hidden_dim,hidden_dim,bias=False)\n\n    def forward(self, decoder_hidden_state, encoder_output) :\n        # decoder_hidden_state, encoder_output -&gt; (batch_size, seq_len, hidden_dim)\n\n        batch_size,input_size, hidden_dim,  = encoder_output.size()\n        score = torch.bmm(decoder_hidden_state, self.wa(encoder_output).transpose(1,2))\n        # score -&gt;  (batch_size, seq_len, seq_len)\n   \n        attn = nn.functional.softmax(score.view(-1, input_size), dim=1).view(batch_size, -1, input_size)\n        # score -&gt;  (batch_size, seq_len, seq_len)\n        context = torch.bmm(attn, encoder_output)\n        # context -&gt; (batch_size, seq_len, hidden_dim)\n        return context, attn\nNow Both the Attentions are Global attention because both uses a global constant encoder output to make decision and only the changing part is decoder hidden state. The global attention has a drawback that it has to attend to all words on the source side for each tar-get word, which is expensive and can potentially render it impractical to translate longer sequences,e.g., paragraphs or documents. To address this deficiency, A local attentional mechanism that chooses to focus only on a small subset of the source positions per target word Self Attention.\n\nThe “soft” vs “hard” attention is another way to categorize how attention is defined. The original idea was proposed in the show, attend and tell paper. Based on whether the attention has access to the entire image or only a patch:\n\nSoft\nAttention: the alignment weights are learned and placed “softly” over all patches in the source image; essentially the same idea as in\n\nPro: the model is smooth and differentiable.\nCon: expensive when the source input is large.\n\nHard\nAttention: only selects one patch of the image to attend to at a time.\n\nPro: less calculation at the inference time.\nCon: the model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train."
  },
  {
    "objectID": "posts/Machine Learning/Attention.html#self-attention",
    "href": "posts/Machine Learning/Attention.html#self-attention",
    "title": "Different types of Attentions",
    "section": "",
    "text": "Say the following sentence is an input sentence we want to translate:\n”The animal didn't cross the street because it was too tired”\nWhat does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.\nWhen the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.\nAs the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\nSelf-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.\n\nAs we are encoding the word “it” in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on “The Animal”, and baked a part of its representation into the encoding of “it”.\nWithin self attention there are three Matrices Query Matrix (Q), Key Matrix (K), Value Matrix(V) .\nLet’s say we have source sentence src = India is a great country and after tokenizng this sentence and passing it down by an embedding layer we have a matrix of\n\\[\n[5,256]\n\\]\nwhere 256 is the hidden dimension.\n\\[\nsrc=embedding(src)\n\\]\nNow with self attention we have a special property where the shape of\n\\[\nQ,K,V = [256,256]\\ all\\ of\\ them\\ have\\ same\\ shape\n\\]\nThe formulae for self attention is.\n\n\\[\nAttention(Q,K,V) = softmax(\\frac {Q.K^T} {\\sqrt{d_{model}}})V\n\\]\nLet’s see the first step where Q and K are multiplied we know when two vectors are multiplied there similarity is governed by cos function where\n\\[\n\\theta = 0\n\\]\nmeans two vectors are similar and\n\\[\n\\theta = -1\n\\]\nmeans two vectors are opposite \nSo when we are multiplying two matrices we found out where we want to put out attention. (Note because of linear layer we will putting attentions at different vectors.) By putting Softmax we are just making sure similar vectors have a high probability and dissimilar vectors are have a minuscule value.\nNow when we again multiply with V we get a matrix were the vectors were we want to put attention has a high value compared to the vector where we don’t want to put attention.\nThe\n\\[\n\\sqrt{d_{model}}\n\\]\nis just a scaling factor used in Multi Head Attention. where\n\\[\nd_{model} =\\frac {hid\\_dim}{n\\_heads}\n\\]"
  },
  {
    "objectID": "posts/Machine Learning/Attention.html#multi-head-attention",
    "href": "posts/Machine Learning/Attention.html#multi-head-attention",
    "title": "Different types of Attentions",
    "section": "",
    "text": "To Boost the self Attention Mechanism Multi Head Attention was introduced where we have multiple heads. We have a source sentence src and if n_heads=8 then 8 separate query, key, value Matrices are present.\nwe calculate self Attention on all 8 heads and then we concat it and pass it to a linear Layer.\nThe Reason for Multi Head Attention is\n\nIt expands the model’s ability to focus on different positions.\nIt gives the attention layer multiple “representation subspaces”.\n\n\nPytorch Multi Head Attention"
  },
  {
    "objectID": "posts/Machine Learning/Attention.html#flash-attention",
    "href": "posts/Machine Learning/Attention.html#flash-attention",
    "title": "Different types of Attentions",
    "section": "",
    "text": "Flash attention is Fast and Memory-Efficient Exact Attention with IO-Awareness\nNote - Read This Before moving forward - Making Deep Learning Go Brrrr From First Principles\nTransformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length.\nTo see the problem\nQuery -&gt; Batch_size, seq_len,hidden_dim\nKey -&gt; Batch_size, seq_len, hidden_dim\nUsually the batch_size and hidden_dim are parameters with small numbers.\nWhen we Batch matrix Multiplication the result is Batch_size, seq_len, seq_len.\nThe bigger the seq_len the bigger the bigger the memory complexity.\nVarious past methods tried to reduce the FLOP but their implementation was not that successful here in flash attention the intention is reduce the memory access(IO).\nThe excerpt from the paper which states things very clearly.\n ” We propose FlashAttention, a new attention algorithm that computes exact attention with far fewer memory accesses.\nOur main goal is to avoid reading and writing the attention matrix to and from HBM.\nThis requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large intermediate attention matrix for the backward pass.\nWe apply two well-established techniques to address these challenges. (i) We restructure the attention computation to split the input into blocks and make several passes over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We store the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the backward pass, which is faster than the standard approach of reading the intermediate attention matrix from HBM.\nNow the problem would be clear that most of the time is spent in Loading the data from HBM and Saving the data to HBM. Calculation of softmax over a large input. ”\n\nIf we try to understand from this picture itself without looking the algo it says,\n\nHBM is storage is big. SRAM is smaller, hence the entire Q or K or V can’t be loaded fully.\nBreak it into smaller chunks of respectable size which can fit on SRAM. here d is no of heads and N is seq_len\nThere is an outer loop which loads K,V in chunks to SRAM.\n\nThere is an inner loop which loads Q in chunks to SRAM.\nThe entire self-attention computation with softmax happens.\nThe output block is written to HBM.\n\n\nThe IO complexity of flash attention -\n\n\nThey save softmax statistics over blocks which over many iteration approximates the correct softmax values which in their algo they call it tilling.\nDuring backward pass softmax inputs and outputs are needed they recompute those inputs and outputs from the final output and softmax normalization statistics from blocks of Q K V in SRAM. This can be seen as a form of selective gradient checkpointing.\nLet suppose there is bigger transformer architecture which has got lot of activations in forward pass we compute these activations and save each of the intermediary results. During backward pass when we access those intermediary results their is huge memory footprint to load these value from HBM to SRAM, to in gradient checkpointing rather then storing all of the intermediary, we keep only a few and rest we compute on fly, because compute is faster, much faster then loading the data too and fro from HBM."
  },
  {
    "objectID": "posts/Machine Learning/Weight_Initializer.html",
    "href": "posts/Machine Learning/Weight_Initializer.html",
    "title": "Weight Intialization",
    "section": "",
    "text": "Weight and bias intialization is one of the important factor responsible for today’s state of the art algorithm.\nWeights intialization is done in random fashion but that randomness is to be tuned in various ways to get optimum result.\n\nPitfall: all zero initialization. Lets start with what we should not do. Note that we do not know what the final value of every weight should be in the trained network, but with proper data normalization it is reasonable to assume that approximately half of the weights will be positive and half of them will be negative. A reasonable-sounding idea then might be to set all the initial weights to zero, which we expect to be the “best guess” in expectation. This turns out to be a mistake, because if every neuron in the network computes the same output, then they will also all compute the same gradients during backpropagation and undergo the exact same parameter updates. In other words, there is no source of asymmetry between neurons if their weights are initialized to be the same.\n\n\nSmall random numbers. Therefore, we still want the weights to be very close to zero, but as we have argued above, not identically zero. As a solution, it is common to initialize the weights of the neurons to small numbers and refer to doing so as symmetry breaking. The idea is that the neurons are all random and unique in the beginning, so they will compute distinct updates and integrate themselves as diverse parts of the full network. The implementation for one weight matrix might look like W = 0.01* np.random.randn(D,H), where randn samples from a zero mean, unit standard deviation gaussian. With this formulation, every neuron’s weight vector is initialized as a random vector sampled from a multi-dimensional gaussian, so the neurons point in random direction in the input space. It is also possible to use small numbers drawn from a uniform distribution, but this seems to have relatively little impact on the final performance in practice.\n\n\nWarning: It’s not necessarily the case that smaller numbers will work strictly better. For example, a Neural Network layer that has very small weights will during backpropagation compute very small gradients on its data (since this gradient is proportional to the value of the weights). This could greatly diminish the “gradient signal” flowing backward through a network, and could become a concern for deep networks.\n\n\nBradley (2009) found that back-propagated gradients were smaller as one moves from the output layer towards the input layer, just after initialization. He studied networks with linear activation at each layer, finding that the variance of the back-propagated gradients decreases as we go back- wards in the network. We will also start by studying the linear regime.\n\n\nCalibrating the variances with 1/sqrt(n) Xavier/Glorot Initialization. One problem with the above suggestion is that the distribution of the outputs from a randomly initialized neuron has a variance that grows with the number of inputs. It turns out that we can normalize the variance of each neuron’s output to 1 by scaling its weight vector by the square root of its fan-in (i.e. its number of inputs). That is, the recommended heuristic is to initialize each neuron’s weight vector as: w = np.random.randn(n) / sqrt(n), where n is the number of its inputs. This ensures that all neurons in the network initially have approximately the same output distribution and empirically improves the rate of convergence.\n\n\nThe sketch of the derivation is as follows: Consider the inner product (s = _i^n w_i x_i) between the weights (w) and input (x), which gives the raw activation of a neuron before the non-linearity. We can examine the variance of (s):\n\n\\[\\begin{align}\n\\text{Var}(s) &= \\text{Var}(\\sum_i^n w_ix_i) \\\\\\\\\n&= \\sum_i^n \\text{Var}(w_ix_i) \\\\\\\\\n&= \\sum_i^n [E(w_i)]^2\\text{Var}(x_i) + E[(x_i)]^2\\text{Var}(w_i) + \\text{Var}(x_i)\\text{Var}(w_i) \\\\\\\\\n&= \\sum_i^n \\text{Var}(x_i)\\text{Var}(w_i) \\\\\\\\\n&= \\left( n \\text{Var}(w) \\right) \\text{Var}(x)\n\\end{align}\\]\n\nwhere in the first 2 steps we have used properties of variance. In third step we assumed zero mean inputs and weights, so (E[x_i] = E[w_i] = 0). Note that this is not generally the case: For example ReLU units will have a positive mean. In the last step we assumed that all (w_i, x_i) are identically distributed. From this derivation we can see that if we want (s) to have the same variance as all of its inputs (x), then during initialization we should make sure that the variance of every weight (w) is (1/n). And since ((aX) = a^2(X)) for a random variable (X) and a scalar (a), this implies that we should draw from unit gaussian and then scale it by (a = ), to make its variance (1/n). This gives the initialization w = np.random.randn(n) / sqrt(nin+nout).\n\n\nSparse initialization. Another way to address the uncalibrated variances problem is to set all weight matrices to zero, but to break symmetry every neuron is randomly connected (with weights sampled from a small gaussian as above) to a fixed number of neurons below it. A typical number of neurons to connect to may be as small as 10.\n\n\nInitializing the biases. It is possible and common to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights. For ReLU non-linearities, some people like to use small constant value such as 0.01 for all biases because this ensures that all ReLU units fire in the beginning and therefore obtain and propagate some gradient. However, it is not clear if this provides a consistent improvement (in fact some results seem to indicate that this performs worse) and it is more common to simply use 0 bias initialization.\n\n\nIn practice, the current recommendation is to use ReLU units and use the w = np.random.randn(n) * sqrt(2.0/n), as discussed in He et al..\n\nWhy is initialization essential to deep networks? It turns out that if you do it wrong, it can lead to exploding or vanishing weights and gradients.\n\nVanishing Gradient In deep nueral network if sigmoid neuron is used there will be a problem of vanishing gradient because the value of sigmoid is between 0 and 1 and derivative of sigmoid is \\[sigmoid(x)*(1-sigmoid(x))\\] and when gradient flows backward in the network it is multiplied by subsequent wights which decreases the gradient value so the gradient to the beginning layers is very close to zero that means there is no learning. Using ReLU solves this problem.\n\nExploding Gradient:- Vanishing gradient is not much of a problem but in the contrary Exploding gradient is something on which we can ponder. This problem occours when weight are intialized with huge number like 10 or 100 and input to the network is not normalized. In this scenario either of these three can happen.\n1. The model is unable to get traction on your training data (e.g. poor loss).\n2. The model is unstable, resulting in large changes in loss from update to update.\n3. The model loss goes to NaN during training.\nTo Solve this problem the solution given are: 1. Redesign the network In deep neural networks, exploding gradients may be addressed by redesigning the network to have fewer layers. There may also be some benefit in using a smaller batch size while training the network.\n\nWeight Regularizers Another approach, if exploding gradients are still occurring, is to check the size of network weights and apply a penalty to the networks loss function for large weight values.\n\nThis is called weight regularization and often an L1 (absolute weights) or an L2 (squared weights) penalty can be used.\n\nGradient Clipping Clip the gradient before updating the weights. Either the gradient can be clipped by using l2 norm or the easiest way to clip is by values. we define a threshold value with range [-limit ,limit] if the grad value exceeds the limit we replace it with the limit value\nPytorch Supports various popular intialization type which can seen here -&gt; https://pytorch.org/docs/stable/nn.init.html\nGradient and Weights can be tracked using TensorBoard distribution Tracking.\nRandom sampling using different distributions -&gt; https://pytorch.org/docs/stable/torch.html#in-place-random-sampling\n\n# Random sampling Techniques\n    torch.Tensor.bernoulli_() - in-place version of torch.bernoulli()\n\n    torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution\n\n    torch.Tensor.exponential_() - numbers drawn from the exponential distribution\n\n    torch.Tensor.geometric_() - elements drawn from the geometric distribution\n\n    torch.Tensor.log_normal_() - samples from the log-normal distribution\n\n    torch.Tensor.normal_() - in-place version of torch.normal()\n\n    torch.Tensor.random_() - numbers sampled from the discrete uniform distribution\n\n    torch.Tensor.uniform_() - numbers sampled from the continuous uniform distribution\n# one example \ntorch.randn(32,32).uniform_()\n\n\nCode\nfrom torch import nn\n\n# a simple network\nmodel=nn.Sequential(nn.Linear(2, 5),\n                         nn.ReLU(),)\nnext(model.parameters())\n\n\nParameter containing:\ntensor([[-0.4624,  0.1237],\n        [ 0.3361,  0.3213],\n        [ 0.0601, -0.5251],\n        [ 0.0932, -0.6454],\n        [-0.1530,  0.3163]], requires_grad=True)\n\n\n\n\nCode\n\n# initialization function, first checks the module type,\n# then applies the desired changes to the weights\ndef init_normal(m):\n    if type(m) == nn.Linear:\n        nn.init.uniform_(m.weight)\n    elif isinstance(m , nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight)\n\n# use the modules apply function to recursively apply the initialization to every submodule.\nmodel.apply(init_normal)\nnext(model.parameters())\n\n\nParameter containing:\ntensor([[0.9828, 0.0986],\n        [0.8059, 0.4303],\n        [0.2424, 0.9537],\n        [0.5189, 0.4018],\n        [0.1175, 0.8017]], requires_grad=True)\n\n\nWeight initialization happens once, when a model is created and before it trains. Having good initial weights can place the neural network close to the optimalsolution. This allows the neural network to come to the best solution quicker.\n\n\n\nTo see how different weights perform, we’ll test on the same dataset and neural network. That way, we know that any changes in model behavior are due to the weights and not any changing data or model structure. &gt; We’ll instantiate at least two of the same models, with different initial weights and see how the training loss decreases over time, such as in the example below.\n\nSometimes the differences in training loss, over time, will be large and other times, certain weights offer only small improvements.\n\n\nWe’ll train an MLP to classify images from the Fashion-MNIST database to demonstrate the effect of different initial weights. As a reminder, the FashionMNIST dataset contains images of clothing types; classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']. The images are normalized so that their pixel values are in a range [0.0 - 1.0). Run the cell below to download and load the dataset.\n\n\n\n\n\nCode\nimport torch\nimport numpy as np\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n# number of subprocesses to use for data loading\nnum_workers = 0\n# how many samples per batch to load\nbatch_size = 100\n# percentage of training set to use as validation\nvalid_size = 0.2\n\n# convert data to torch.FloatTensor\ntransform = transforms.ToTensor()\n\n# choose the training and test datasets\ntrain_data = datasets.FashionMNIST(root='data', train=True,\n                                   download=True, transform=transform)\ntest_data = datasets.FashionMNIST(root='data', train=False,\n                                  download=True, transform=transform)\n\n# obtain training indices that will be used for validation\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n# prepare data loaders (combine dataset and sampler)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n    sampler=train_sampler, num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n    sampler=valid_sampler, num_workers=num_workers)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n    num_workers=num_workers)\n\n# specify the image classes\nclasses = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n%matplotlib inline\n    \n# obtain one batch of training images\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nimages = images.numpy()\n\n# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(25, 4))\nfor idx in np.arange(20):\n    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n    ax.set_title(classes[labels[idx]])\n\n\n\n\n\n\n\n\n\nWe’ve defined the MLP that we’ll use for classifying the dataset.\n\n\n\n\nA 3 layer MLP with hidden dimensions of 256 and 128.\nThis MLP accepts a flattened image (784-value long vector) as input and produces 10 class scores as output. We’ll test the effect of different initial weights on this 3 layer neural network with ReLU activations and an Adam optimizer.\n\nThe lessons you learn apply to other neural networks, including different activations and optimizers.\n\n\n\n\nLet’s start looking at some initial weights. ### All Zeros or Ones If you follow the principle of Occam’s razor, you might think setting all the weights to 0 or 1 would be the best solution. This is not the case.\nWith every weight the same, all the neurons at each layer are producing the same output. This makes it hard to decide which weights to adjust.\nLet’s compare the loss with all ones and all zero weights by defining two models with those constant weights.\nBelow, we are using PyTorch’s nn.init to initialize each Linear layer with a constant weight. The init library provides a number of weight initialization functions that give you the ability to initialize the weights of each layer according to layer type.\nIn the case below, we look at every layer/module in our model. If it is a Linear layer (as all three layers are for this MLP), then we initialize those layer weights to be a constant_weight with bias=0 using the following code: &gt;if isinstance(m, nn.Linear):     nn.init.constant_(m.weight, constant_weight)     nn.init.constant_(m.bias, 0)\nThe constant_weight is a value that you can pass in when you instantiate the model.\n\n\nCode\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# define the NN architecture\nclass Net(nn.Module):\n    def __init__(self, hidden_1=256, hidden_2=128, constant_weight=None):\n        super(Net, self).__init__()\n        # linear layer (784 -&gt; hidden_1)\n        self.fc1 = nn.Linear(28 * 28, hidden_1)\n        # linear layer (hidden_1 -&gt; hidden_2)\n        self.fc2 = nn.Linear(hidden_1, hidden_2)\n        # linear layer (hidden_2 -&gt; 10)\n        self.fc3 = nn.Linear(hidden_2, 10)\n        # dropout layer (p=0.2)\n        self.dropout = nn.Dropout(0.2)\n        \n        # initialize the weights to a specified, constant value\n        if(constant_weight is not None):\n            for m in self.modules():\n                if isinstance(m, nn.Linear):\n                    nn.init.constant_(m.weight, constant_weight)\n                    nn.init.constant_(m.bias, 0)\n    \n            \n    def forward(self, x):\n        # flatten image input\n        x = x.view(-1, 28 * 28)\n        # add hidden layer, with relu activation function\n        x = F.relu(self.fc1(x))\n        # add dropout layer\n        x = self.dropout(x)\n        # add hidden layer, with relu activation function\n        x = F.relu(self.fc2(x))\n        # add dropout layer\n        x = self.dropout(x)\n        # add output layer\n        x = self.fc3(x)\n        return x\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim\n\n\ndef _get_loss_acc(model, train_loader, valid_loader):\n    \"\"\"\n    Get losses and validation accuracy of example neural network\n    \"\"\"\n    n_epochs = 2\n    learning_rate = 0.001\n    \n    # Training loss\n    criterion = nn.CrossEntropyLoss()\n\n    # Optimizer\n    optimizer = optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n\n    # Measurements used for graphing loss\n    loss_batch = []\n\n    for epoch in range(1, n_epochs+1):\n        # initialize var to monitor training loss\n        train_loss = 0.0\n        ###################\n        # train the model #\n        ###################\n        for data, target in train_loader:\n            # clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data)\n            # calculate the batch loss\n            loss = criterion(output, target)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            # record average batch loss \n            loss_batch.append(loss.item())\n             \n    # after training for 2 epochs, check validation accuracy \n    correct = 0\n    total = 0\n    for data, target in valid_loader:\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # get the predicted class from the maximum class score\n        _, predicted = torch.max(output.data, 1)\n        # count up total number of correct labels\n        # for which the predicted and true labels are equal\n\n\n\n\nBelow, we are using compare_init_weights to compare the training and validation loss for the two models we defined above, model_0 and model_1. This function takes in a list of models (each with different initial weights), the name of the plot to produce, and the training and validation dataset loaders. For each given model, it will plot the training loss for the first 100 batches and print out the validation accuracy after 2 training epochs. Note: if you’ve used a small batch_size, you may want to increase the number of epochs here to better compare how models behave after seeing a few hundred images.\nWe plot the loss over the first 100 batches to better judge which model weights performed better at the start of training.\nRun the cell below to see the difference between weights of all zeros against all ones.\n\n\nCode\n# initialize two NN's with 0 and 1 constant weights\nmodel_0 = Net(constant_weight=0)\nmodel_1 = Net(constant_weight=1)\n\n\n\n\nCode\n# put them in list form to compare\nmodel_list = [(model_0, 'All Zeros'),\n              (model_1, 'All Ones')]\n\n\n# plot the loss over the first 100 batches\ncompare_init_weights(model_list, \n                             'All Zeros vs All Ones', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n    9.675% -- All Zeros\n    9.675% -- All Ones\nTraining Loss\n    2.305  -- All Zeros\n  478.745  -- All Ones\n\n\nAs you can see the accuracy is close to guessing for both zeros and ones, around 10%.\nThe neural network is having a hard time determining which weights need to be changed, since the neurons have the same output for each layer. To avoid neurons with the same output, let’s use unique weights. We can also randomly select these weights to avoid being stuck in a local minimum for each run.\nA good solution for getting these random weights is to sample from a uniform distribution.\n\n\n\nA uniform distribution has the equal probability of picking any number from a set of numbers. We’ll be picking from a continuous distribution, so the chance of picking the same number is low. We’ll use NumPy’s np.random.uniform function to pick random numbers from a uniform distribution.\n\n\nOutputs random values from a uniform distribution.\n\n\nThe generated values follow a uniform distribution in the range [low, high). The lower bound minval is included in the range, while the upper bound maxval is excluded.\n\n\n\nlow: The lower bound on the range of random values to generate. Defaults to 0.\nhigh: The upper bound on the range of random values to generate. Defaults to 1.\nsize: An int or tuple of ints that specify the shape of the output array.\n\n\nWe can visualize the uniform distribution by using a histogram. Let’s map the values from np.random_uniform(-3, 3, [1000]) to a histogram using the hist_dist function. This will be 1000 random float values from -3 to 3, excluding the value 3.\n\n\nCode\nhist_dist('Random Uniform (low=-3, high=3)', np.random.uniform(-3, 3, [1000]))\n\n\n\n\n\nThe histogram used 500 buckets for the 1000 values. Since the chance for any single bucket is the same, there should be around 2 values for each bucket. That’s exactly what we see with the histogram. Some buckets have more and some have less, but they trend around 2.\nNow that you understand the uniform function, let’s use PyTorch’s nn.init to apply it to a model’s initial weights.\n\n\n\nLet’s see how well the neural network trains using a uniform weight initialization, where low=0.0 and high=1.0. Below, I’ll show you another way (besides in the Net class code) to initialize the weights of a network. To define weights outside of the model definition, you can: &gt;1. Define a function that assigns weights by the type of network layer, then 2. Apply those weights to an initialized model using model.apply(fn), which applies a function to each model layer.\nThis time, we’ll use weight.data.uniform_ to initialize the weights of our model, directly.\n\n\nCode\n# takes in a module and applies the specified weight initialization\ndef weights_init_uniform(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # apply a uniform distribution to the weights and a bias=0\n        m.weight.data.uniform_(0.0, 1.0)\n        m.bias.data.fill_(0)\n\n\n\n\nCode\n# create a new model with these weights\nmodel_uniform = Net()\nmodel_uniform.apply(weights_init_uniform)\n\n\nNet(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\n  (dropout): Dropout(p=0.2)\n)\n\n\n\n\nCode\n# evaluate behavior \ncompare_init_weights([(model_uniform, 'Uniform Weights')], \n                             'Uniform Baseline', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n   33.933% -- Uniform Weights\nTraining Loss\n    4.697  -- Uniform Weights\n\n\nThe loss graph is showing the neural network is learning, which it didn’t with all zeros or all ones. We’re headed in the right direction!\n\n\n\n\nThe general rule for setting the weights in a neural network is to set them to be close to zero without being too small. &gt;Good practice is to start your weights in the range of \\([-y, y]\\) where \\(y=1/\\sqrt{n}\\)\n(\\(n\\) is the number of inputs to a given neuron).\nLet’s see if this holds true; let’s create a baseline to compare with and center our uniform range over zero by shifting it over by 0.5. This will give us the range [-0.5, 0.5).\n\n\nCode\n# takes in a module and applies the specified weight initialization\ndef weights_init_uniform_center(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # apply a centered, uniform distribution to the weights\n        m.weight.data.uniform_(-0.5, 0.5)\n        m.bias.data.fill_(0)\n\n# create a new model with these weights\nmodel_centered = Net()\nmodel_centered.apply(weights_init_uniform_center)\n\n\nNet(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\n  (dropout): Dropout(p=0.2)\n)\n\n\nThen let’s create a distribution and model that uses the general rule for weight initialization; using the range \\([-y, y]\\), where \\(y=1/\\sqrt{n}\\) .\nAnd finally, we’ll compare the two models.\n\n\nCode\n# takes in a module and applies the specified weight initialization\ndef weights_init_uniform_rule(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # get the number of the inputs\n        n = m.in_features\n        y = 1.0/np.sqrt(n)\n        m.weight.data.uniform_(-y, y)\n        m.bias.data.fill_(0)\n\n# create a new model with these weights\nmodel_rule = Net()\nmodel_rule.apply(weights_init_uniform_rule)\n\n\nNet(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\n  (dropout): Dropout(p=0.2)\n)\n\n\n\n\nCode\n# compare these two models\nmodel_list = [(model_centered, 'Centered Weights [-0.5, 0.5)'), \n              (model_rule, 'General Rule [-y, y)')]\n\n# evaluate behavior \ncompare_init_weights(model_list, \n                             '[-0.5, 0.5) vs [-y, y)', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n   74.875% -- Centered Weights [-0.5, 0.5)\n   84.833% -- General Rule [-y, y)\nTraining Loss\n    0.789  -- Centered Weights [-0.5, 0.5)\n    0.487  -- General Rule [-y, y)\n\n\nThis behavior is really promising! Not only is the loss decreasing, but it seems to do so very quickly for our uniform weights that follow the general rule; after only two epochs we get a fairly high validation accuracy and this should give you some intuition for why starting out with the right initial weights can really help your training process!\n\nSince the uniform distribution has the same chance to pick any value in a range, what if we used a distribution that had a higher chance of picking numbers closer to 0? Let’s look at the normal distribution.\n\n\nUnlike the uniform distribution, the normal distribution has a higher likelihood of picking number close to it’s mean. To visualize it, let’s plot values from NumPy’s np.random.normal function to a histogram.\n\nnp.random.normal(loc=0.0, scale=1.0, size=None)\n\n\nOutputs random values from a normal distribution.\n\n\n\nloc: The mean of the normal distribution.\nscale: The standard deviation of the normal distribution.\nshape: The shape of the output array.\n\n\n\n\nCode\nhist_dist('Random Normal (mean=0.0, stddev=1.0)', np.random.normal(size=[1000]))\n\n\n\n\n\nLet’s compare the normal distribution against the previous, rule-based, uniform distribution.\nBelow, we define a normal distribution that has a mean of 0 and a standard deviation of \\(y=1/\\sqrt{n}\\).\n\n\nCode\n# takes in a module and applies the specified weight initialization\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # get the number of the inputs\n        n = m.in_features\n        y = (1.0/np.sqrt(n))\n        m.weight.data.normal_(0, y)\n        m.bias.data.fill_(0)\n\n\n\n\nCode\n# create a new model with the rule-based, uniform weights\nmodel_uniform_rule = Net()\nmodel_uniform_rule.apply(weights_init_uniform_rule)\n\n# create a new model with the rule-based, NORMAL weights\nmodel_normal_rule = Net()\nmodel_normal_rule.apply(weights_init_normal)\n\n\nNet(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\n  (dropout): Dropout(p=0.2)\n)\n\n\n\n\nCode\n# compare the two models\nmodel_list = [(model_uniform_rule, 'Uniform Rule [-y, y)'), \n              (model_normal_rule, 'Normal Distribution')]\n\n# evaluate behavior \ncompare_init_weights(model_list, \n                             'Uniform vs Normal', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n   85.442% -- Uniform Rule [-y, y)\n   85.233% -- Normal Distribution\nTraining Loss\n    0.318  -- Uniform Rule [-y, y)\n    0.333  -- Normal Distribution\n\n\nThe normal distribution gives us pretty similar behavior compared to the uniform distribution, in this case. This is likely because our network is so small; a larger neural network will pick more weight values from each of these distributions, magnifying the effect of both initialization styles. In general, a normal distribution will result in better performance for a model.\n\n\n\n\nLet’s quickly take a look at what happens without any explicit weight initialization.\n\n\nCode\nmodel_no_initialization = Net()\n\n\n\n\nCode\nmodel_list = [(model_no_initialization, 'No Weights')]\n\n# evaluate behavior \ncompare_init_weights(model_list, \n                             'No Weight Initialization', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n   84.458% -- No Weights\nTraining Loss\n    0.530  -- No Weights\n\n\n\n\n\nSomething really interesting is happening here. You may notice that the red line “no weights” looks a lot like our uniformly initialized weights. It turns out that PyTorch has default weight initialization behavior for every kind of layer. You can see that linear layers are initialized with a uniform distribution (uniform weights and biases) in the module source code.\nHowever, you can also see that the weights taken from a normal distribution are comparable, perhaps even a little better! So, it may still be useful, especially if you are trying to train the best models, to initialize the weights of a model according to rules that you define.\nAnd, this is not the end ! You’re encouraged to look at the different types of common initialization distributions.\n\n\n\n\nhttp://cs231n.github.io/neural-networks-2/#init\nhttps://github.com/udacity/deep-learning"
  },
  {
    "objectID": "posts/Machine Learning/Weight_Initializer.html#initial-weights-and-observing-training-loss",
    "href": "posts/Machine Learning/Weight_Initializer.html#initial-weights-and-observing-training-loss",
    "title": "Weight Intialization",
    "section": "",
    "text": "To see how different weights perform, we’ll test on the same dataset and neural network. That way, we know that any changes in model behavior are due to the weights and not any changing data or model structure. &gt; We’ll instantiate at least two of the same models, with different initial weights and see how the training loss decreases over time, such as in the example below.\n\nSometimes the differences in training loss, over time, will be large and other times, certain weights offer only small improvements.\n\n\nWe’ll train an MLP to classify images from the Fashion-MNIST database to demonstrate the effect of different initial weights. As a reminder, the FashionMNIST dataset contains images of clothing types; classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']. The images are normalized so that their pixel values are in a range [0.0 - 1.0). Run the cell below to download and load the dataset.\n\n\n\n\n\nCode\nimport torch\nimport numpy as np\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n# number of subprocesses to use for data loading\nnum_workers = 0\n# how many samples per batch to load\nbatch_size = 100\n# percentage of training set to use as validation\nvalid_size = 0.2\n\n# convert data to torch.FloatTensor\ntransform = transforms.ToTensor()\n\n# choose the training and test datasets\ntrain_data = datasets.FashionMNIST(root='data', train=True,\n                                   download=True, transform=transform)\ntest_data = datasets.FashionMNIST(root='data', train=False,\n                                  download=True, transform=transform)\n\n# obtain training indices that will be used for validation\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n# prepare data loaders (combine dataset and sampler)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n    sampler=train_sampler, num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n    sampler=valid_sampler, num_workers=num_workers)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n    num_workers=num_workers)\n\n# specify the image classes\nclasses = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n%matplotlib inline\n    \n# obtain one batch of training images\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nimages = images.numpy()\n\n# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(25, 4))\nfor idx in np.arange(20):\n    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n    ax.set_title(classes[labels[idx]])"
  },
  {
    "objectID": "posts/Machine Learning/Weight_Initializer.html#define-the-model-architecture",
    "href": "posts/Machine Learning/Weight_Initializer.html#define-the-model-architecture",
    "title": "Weight Intialization",
    "section": "",
    "text": "We’ve defined the MLP that we’ll use for classifying the dataset.\n\n\n\n\nA 3 layer MLP with hidden dimensions of 256 and 128.\nThis MLP accepts a flattened image (784-value long vector) as input and produces 10 class scores as output. We’ll test the effect of different initial weights on this 3 layer neural network with ReLU activations and an Adam optimizer.\n\nThe lessons you learn apply to other neural networks, including different activations and optimizers."
  },
  {
    "objectID": "posts/Machine Learning/Weight_Initializer.html#initialize-weights",
    "href": "posts/Machine Learning/Weight_Initializer.html#initialize-weights",
    "title": "Weight Intialization",
    "section": "",
    "text": "Let’s start looking at some initial weights. ### All Zeros or Ones If you follow the principle of Occam’s razor, you might think setting all the weights to 0 or 1 would be the best solution. This is not the case.\nWith every weight the same, all the neurons at each layer are producing the same output. This makes it hard to decide which weights to adjust.\nLet’s compare the loss with all ones and all zero weights by defining two models with those constant weights.\nBelow, we are using PyTorch’s nn.init to initialize each Linear layer with a constant weight. The init library provides a number of weight initialization functions that give you the ability to initialize the weights of each layer according to layer type.\nIn the case below, we look at every layer/module in our model. If it is a Linear layer (as all three layers are for this MLP), then we initialize those layer weights to be a constant_weight with bias=0 using the following code: &gt;if isinstance(m, nn.Linear):     nn.init.constant_(m.weight, constant_weight)     nn.init.constant_(m.bias, 0)\nThe constant_weight is a value that you can pass in when you instantiate the model.\n\n\nCode\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# define the NN architecture\nclass Net(nn.Module):\n    def __init__(self, hidden_1=256, hidden_2=128, constant_weight=None):\n        super(Net, self).__init__()\n        # linear layer (784 -&gt; hidden_1)\n        self.fc1 = nn.Linear(28 * 28, hidden_1)\n        # linear layer (hidden_1 -&gt; hidden_2)\n        self.fc2 = nn.Linear(hidden_1, hidden_2)\n        # linear layer (hidden_2 -&gt; 10)\n        self.fc3 = nn.Linear(hidden_2, 10)\n        # dropout layer (p=0.2)\n        self.dropout = nn.Dropout(0.2)\n        \n        # initialize the weights to a specified, constant value\n        if(constant_weight is not None):\n            for m in self.modules():\n                if isinstance(m, nn.Linear):\n                    nn.init.constant_(m.weight, constant_weight)\n                    nn.init.constant_(m.bias, 0)\n    \n            \n    def forward(self, x):\n        # flatten image input\n        x = x.view(-1, 28 * 28)\n        # add hidden layer, with relu activation function\n        x = F.relu(self.fc1(x))\n        # add dropout layer\n        x = self.dropout(x)\n        # add hidden layer, with relu activation function\n        x = F.relu(self.fc2(x))\n        # add dropout layer\n        x = self.dropout(x)\n        # add output layer\n        x = self.fc3(x)\n        return x\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim\n\n\ndef _get_loss_acc(model, train_loader, valid_loader):\n    \"\"\"\n    Get losses and validation accuracy of example neural network\n    \"\"\"\n    n_epochs = 2\n    learning_rate = 0.001\n    \n    # Training loss\n    criterion = nn.CrossEntropyLoss()\n\n    # Optimizer\n    optimizer = optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n\n    # Measurements used for graphing loss\n    loss_batch = []\n\n    for epoch in range(1, n_epochs+1):\n        # initialize var to monitor training loss\n        train_loss = 0.0\n        ###################\n        # train the model #\n        ###################\n        for data, target in train_loader:\n            # clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data)\n            # calculate the batch loss\n            loss = criterion(output, target)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            # record average batch loss \n            loss_batch.append(loss.item())\n             \n    # after training for 2 epochs, check validation accuracy \n    correct = 0\n    total = 0\n    for data, target in valid_loader:\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # get the predicted class from the maximum class score\n        _, predicted = torch.max(output.data, 1)\n        # count up total number of correct labels\n        # for which the predicted and true labels are equal\n\n\n\n\nBelow, we are using compare_init_weights to compare the training and validation loss for the two models we defined above, model_0 and model_1. This function takes in a list of models (each with different initial weights), the name of the plot to produce, and the training and validation dataset loaders. For each given model, it will plot the training loss for the first 100 batches and print out the validation accuracy after 2 training epochs. Note: if you’ve used a small batch_size, you may want to increase the number of epochs here to better compare how models behave after seeing a few hundred images.\nWe plot the loss over the first 100 batches to better judge which model weights performed better at the start of training.\nRun the cell below to see the difference between weights of all zeros against all ones.\n\n\nCode\n# initialize two NN's with 0 and 1 constant weights\nmodel_0 = Net(constant_weight=0)\nmodel_1 = Net(constant_weight=1)\n\n\n\n\nCode\n# put them in list form to compare\nmodel_list = [(model_0, 'All Zeros'),\n              (model_1, 'All Ones')]\n\n\n# plot the loss over the first 100 batches\ncompare_init_weights(model_list, \n                             'All Zeros vs All Ones', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n    9.675% -- All Zeros\n    9.675% -- All Ones\nTraining Loss\n    2.305  -- All Zeros\n  478.745  -- All Ones\n\n\nAs you can see the accuracy is close to guessing for both zeros and ones, around 10%.\nThe neural network is having a hard time determining which weights need to be changed, since the neurons have the same output for each layer. To avoid neurons with the same output, let’s use unique weights. We can also randomly select these weights to avoid being stuck in a local minimum for each run.\nA good solution for getting these random weights is to sample from a uniform distribution.\n\n\n\nA uniform distribution has the equal probability of picking any number from a set of numbers. We’ll be picking from a continuous distribution, so the chance of picking the same number is low. We’ll use NumPy’s np.random.uniform function to pick random numbers from a uniform distribution.\n\n\nOutputs random values from a uniform distribution.\n\n\nThe generated values follow a uniform distribution in the range [low, high). The lower bound minval is included in the range, while the upper bound maxval is excluded.\n\n\n\nlow: The lower bound on the range of random values to generate. Defaults to 0.\nhigh: The upper bound on the range of random values to generate. Defaults to 1.\nsize: An int or tuple of ints that specify the shape of the output array.\n\n\nWe can visualize the uniform distribution by using a histogram. Let’s map the values from np.random_uniform(-3, 3, [1000]) to a histogram using the hist_dist function. This will be 1000 random float values from -3 to 3, excluding the value 3.\n\n\nCode\nhist_dist('Random Uniform (low=-3, high=3)', np.random.uniform(-3, 3, [1000]))\n\n\n\n\n\nThe histogram used 500 buckets for the 1000 values. Since the chance for any single bucket is the same, there should be around 2 values for each bucket. That’s exactly what we see with the histogram. Some buckets have more and some have less, but they trend around 2.\nNow that you understand the uniform function, let’s use PyTorch’s nn.init to apply it to a model’s initial weights.\n\n\n\nLet’s see how well the neural network trains using a uniform weight initialization, where low=0.0 and high=1.0. Below, I’ll show you another way (besides in the Net class code) to initialize the weights of a network. To define weights outside of the model definition, you can: &gt;1. Define a function that assigns weights by the type of network layer, then 2. Apply those weights to an initialized model using model.apply(fn), which applies a function to each model layer.\nThis time, we’ll use weight.data.uniform_ to initialize the weights of our model, directly.\n\n\nCode\n# takes in a module and applies the specified weight initialization\ndef weights_init_uniform(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # apply a uniform distribution to the weights and a bias=0\n        m.weight.data.uniform_(0.0, 1.0)\n        m.bias.data.fill_(0)\n\n\n\n\nCode\n# create a new model with these weights\nmodel_uniform = Net()\nmodel_uniform.apply(weights_init_uniform)\n\n\nNet(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\n  (dropout): Dropout(p=0.2)\n)\n\n\n\n\nCode\n# evaluate behavior \ncompare_init_weights([(model_uniform, 'Uniform Weights')], \n                             'Uniform Baseline', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n   33.933% -- Uniform Weights\nTraining Loss\n    4.697  -- Uniform Weights\n\n\nThe loss graph is showing the neural network is learning, which it didn’t with all zeros or all ones. We’re headed in the right direction!"
  },
  {
    "objectID": "posts/Machine Learning/Weight_Initializer.html#general-rule-for-setting-weights",
    "href": "posts/Machine Learning/Weight_Initializer.html#general-rule-for-setting-weights",
    "title": "Weight Intialization",
    "section": "",
    "text": "The general rule for setting the weights in a neural network is to set them to be close to zero without being too small. &gt;Good practice is to start your weights in the range of \\([-y, y]\\) where \\(y=1/\\sqrt{n}\\)\n(\\(n\\) is the number of inputs to a given neuron).\nLet’s see if this holds true; let’s create a baseline to compare with and center our uniform range over zero by shifting it over by 0.5. This will give us the range [-0.5, 0.5).\n\n\nCode\n# takes in a module and applies the specified weight initialization\ndef weights_init_uniform_center(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # apply a centered, uniform distribution to the weights\n        m.weight.data.uniform_(-0.5, 0.5)\n        m.bias.data.fill_(0)\n\n# create a new model with these weights\nmodel_centered = Net()\nmodel_centered.apply(weights_init_uniform_center)\n\n\nNet(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\n  (dropout): Dropout(p=0.2)\n)\n\n\nThen let’s create a distribution and model that uses the general rule for weight initialization; using the range \\([-y, y]\\), where \\(y=1/\\sqrt{n}\\) .\nAnd finally, we’ll compare the two models.\n\n\nCode\n# takes in a module and applies the specified weight initialization\ndef weights_init_uniform_rule(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # get the number of the inputs\n        n = m.in_features\n        y = 1.0/np.sqrt(n)\n        m.weight.data.uniform_(-y, y)\n        m.bias.data.fill_(0)\n\n# create a new model with these weights\nmodel_rule = Net()\nmodel_rule.apply(weights_init_uniform_rule)\n\n\nNet(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\n  (dropout): Dropout(p=0.2)\n)\n\n\n\n\nCode\n# compare these two models\nmodel_list = [(model_centered, 'Centered Weights [-0.5, 0.5)'), \n              (model_rule, 'General Rule [-y, y)')]\n\n# evaluate behavior \ncompare_init_weights(model_list, \n                             '[-0.5, 0.5) vs [-y, y)', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n   74.875% -- Centered Weights [-0.5, 0.5)\n   84.833% -- General Rule [-y, y)\nTraining Loss\n    0.789  -- Centered Weights [-0.5, 0.5)\n    0.487  -- General Rule [-y, y)\n\n\nThis behavior is really promising! Not only is the loss decreasing, but it seems to do so very quickly for our uniform weights that follow the general rule; after only two epochs we get a fairly high validation accuracy and this should give you some intuition for why starting out with the right initial weights can really help your training process!\n\nSince the uniform distribution has the same chance to pick any value in a range, what if we used a distribution that had a higher chance of picking numbers closer to 0? Let’s look at the normal distribution.\n\n\nUnlike the uniform distribution, the normal distribution has a higher likelihood of picking number close to it’s mean. To visualize it, let’s plot values from NumPy’s np.random.normal function to a histogram.\n\nnp.random.normal(loc=0.0, scale=1.0, size=None)\n\n\nOutputs random values from a normal distribution.\n\n\n\nloc: The mean of the normal distribution.\nscale: The standard deviation of the normal distribution.\nshape: The shape of the output array.\n\n\n\n\nCode\nhist_dist('Random Normal (mean=0.0, stddev=1.0)', np.random.normal(size=[1000]))\n\n\n\n\n\nLet’s compare the normal distribution against the previous, rule-based, uniform distribution.\nBelow, we define a normal distribution that has a mean of 0 and a standard deviation of \\(y=1/\\sqrt{n}\\).\n\n\nCode\n# takes in a module and applies the specified weight initialization\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    # for every Linear layer in a model..\n    if classname.find('Linear') != -1:\n        # get the number of the inputs\n        n = m.in_features\n        y = (1.0/np.sqrt(n))\n        m.weight.data.normal_(0, y)\n        m.bias.data.fill_(0)\n\n\n\n\nCode\n# create a new model with the rule-based, uniform weights\nmodel_uniform_rule = Net()\nmodel_uniform_rule.apply(weights_init_uniform_rule)\n\n# create a new model with the rule-based, NORMAL weights\nmodel_normal_rule = Net()\nmodel_normal_rule.apply(weights_init_normal)\n\n\nNet(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=10, bias=True)\n  (dropout): Dropout(p=0.2)\n)\n\n\n\n\nCode\n# compare the two models\nmodel_list = [(model_uniform_rule, 'Uniform Rule [-y, y)'), \n              (model_normal_rule, 'Normal Distribution')]\n\n# evaluate behavior \ncompare_init_weights(model_list, \n                             'Uniform vs Normal', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n   85.442% -- Uniform Rule [-y, y)\n   85.233% -- Normal Distribution\nTraining Loss\n    0.318  -- Uniform Rule [-y, y)\n    0.333  -- Normal Distribution\n\n\nThe normal distribution gives us pretty similar behavior compared to the uniform distribution, in this case. This is likely because our network is so small; a larger neural network will pick more weight values from each of these distributions, magnifying the effect of both initialization styles. In general, a normal distribution will result in better performance for a model.\n\n\n\n\nLet’s quickly take a look at what happens without any explicit weight initialization.\n\n\nCode\nmodel_no_initialization = Net()\n\n\n\n\nCode\nmodel_list = [(model_no_initialization, 'No Weights')]\n\n# evaluate behavior \ncompare_init_weights(model_list, \n                             'No Weight Initialization', \n                             train_loader,\n                             valid_loader)\n\n\n\n\n\nAfter 2 Epochs:\nValidation Accuracy\n   84.458% -- No Weights\nTraining Loss\n    0.530  -- No Weights\n\n\n\n\n\nSomething really interesting is happening here. You may notice that the red line “no weights” looks a lot like our uniformly initialized weights. It turns out that PyTorch has default weight initialization behavior for every kind of layer. You can see that linear layers are initialized with a uniform distribution (uniform weights and biases) in the module source code.\nHowever, you can also see that the weights taken from a normal distribution are comparable, perhaps even a little better! So, it may still be useful, especially if you are trying to train the best models, to initialize the weights of a model according to rules that you define.\nAnd, this is not the end ! You’re encouraged to look at the different types of common initialization distributions."
  },
  {
    "objectID": "posts/Machine Learning/Weight_Initializer.html#additional-resources",
    "href": "posts/Machine Learning/Weight_Initializer.html#additional-resources",
    "title": "Weight Intialization",
    "section": "",
    "text": "http://cs231n.github.io/neural-networks-2/#init\nhttps://github.com/udacity/deep-learning"
  },
  {
    "objectID": "posts/Machine Learning/data_model_manage_prod.html",
    "href": "posts/Machine Learning/data_model_manage_prod.html",
    "title": "Data and Model Management | Model Monitoring and Logging.",
    "section": "",
    "text": "Whenever a ML model goes to production there are lot many things which has to be take care starting from,\nHere we assume that the model is already in production using some sort of deployment strategy, except how to deploy we talk about every other thing. For Deployment a seperate blog has to be written."
  },
  {
    "objectID": "posts/Machine Learning/data_model_manage_prod.html#code",
    "href": "posts/Machine Learning/data_model_manage_prod.html#code",
    "title": "Data and Model Management | Model Monitoring and Logging.",
    "section": "Code",
    "text": "Code\nWithout a single doubt every programmer knows why we should track our code and to be very precise , Tracking means having knowledge of what changed in the code at what time and by which person and how can we retrieve those changes. The hero of the code tracking is git. and various places where we can keep those changes on the cloud can be Github, Gitlab etc."
  },
  {
    "objectID": "posts/Machine Learning/data_model_manage_prod.html#data",
    "href": "posts/Machine Learning/data_model_manage_prod.html#data",
    "title": "Data and Model Management | Model Monitoring and Logging.",
    "section": "Data",
    "text": "Data\nIn the ML world we are dealing with lots and lots of data where the nature of the received data also sometime it is a streaming data sometimes new data is received in batches and other time may be like a single blob, In the context of ML experiements where with every new experiments we might be generating new data columns doing some different feature processing or with time in the same experiment adding new features.\nAlso when were are putting our models to production we keep updating our models and corresponding Data and feature from one version to another.\nAll of these different scenario asks for some way of tracking the changes in the data.\nWorst way of doing it is put it in git which is okay if data is of small size couple of MBs how about couple of GBs or TBs we need a sophisticated library for this.\nLet me introduce DVC (Data Version Control) dvc integrates very nicely with git and works in tandem and will be used to track changes in the data and storing in some sort of file server it can Amazon s3, Azure storage, GCP, or any self hosted place.\nDVC makes Data Management simplified and helps us maintain a Data Registry.\n\nIn general we see DVC can help us track any type of file or folder, it a generic tool for tracking everything apart from code, so using DVC we can not only track data but also our saved model and can maintain a Model Registry.\nWorking of DVC is quite simple also.\n\nIt uses similar flow as git and is a completely command line tool like git.\nWhenever we ask dvc to track a particular file it tracks it and creates a .dvc metafile. For example 20gb_finance_data.csv when tracked using dvc it will generate 20gb_finance_data.dvc\nNow this .dvc file has to be tracked using git.\nWhen ever at any point of time we checkout a git commit, with code this metafile will also come out and then using dvc we checkout the corresponding data for the .dvc file.\n\n\nso if we see we have not only versioned and tracked our data but also our models, from tracking models I mean tracking weights of a particular model"
  },
  {
    "objectID": "posts/Machine Learning/data_model_manage_prod.html#configuration-management-and-validation",
    "href": "posts/Machine Learning/data_model_manage_prod.html#configuration-management-and-validation",
    "title": "Data and Model Management | Model Monitoring and Logging.",
    "section": "Configuration Management and Validation",
    "text": "Configuration Management and Validation\nIn a production environment lot of values to program comes from the run time and these key value pair has to be stored some where various ways people store it are\n\nPut it in a regular file\nStore it in some database\nStore it in on cloud behind firewalls\nuse tools like Consul, Apache Zookeeper etc\n\nand every choice comes with its on pros and cons.\nUsually JSON or YAML is used for storing config files, YAML being more feature rich.\nPython comes with ConfigParser class which implements a basic configuration language which provides a structure similar to what’s found in Microsoft Windows INI files. You can use this to write Python programs which can be customized by end users easily.\nOne of the most feature rich config management though not much needed in production setting more useful when doing HyperParameter search or Experimenting with different configuration is OmegaConf. It is a hierarchical configuration system, with support for merging configurations from multiple sources (YAML config files, dataclasses/objects and CLI arguments) providing a consistent API regardless of how the configuration was created.\nAn important need of config management is config value validation in run time and there is no better library than Pydantic that is the most widely used data validation library for Python."
  },
  {
    "objectID": "posts/Machine Learning/data_model_manage_prod.html#model-monitoring",
    "href": "posts/Machine Learning/data_model_manage_prod.html#model-monitoring",
    "title": "Data and Model Management | Model Monitoring and Logging.",
    "section": "Model Monitoring",
    "text": "Model Monitoring\nIn terms of Model monitoring there are three aspects which needs to be measured all the time, and special emphasis has to be given to create a Feedback loop to collect user feedback and use it for model improvement.\n\nMeasuring the Resource Cosumption.\n\nAmount of resource consumption (CPU, GPU, Storage)\nSpike in model usage that leads to increase in nodes a.k.a Auto Scaling\nLatency and Throughput\n\nMeasuring the Models Prediction quality\n\nData Drift\nModel Prediction Quality\n\nMeasuring Model Meta Data\n\nFrequency of the training\nModel Versioning\n\n\nData points described in 1 and 3 is of time series nature hence we need a mechanism to store the data in a DB and then show it in a dashboard.\nTIG (Telegraf-InfluxDb-Grafana) comes to the resuce.\n\nTelegraf -&gt; Mechanism to send data to influxdb on specific time intervals. In python we can use Statsd library to send data.\nInflux DB -&gt; Stores the data.\nGrafana -&gt; Helps to visualize data stored in Influx DB.\nThe same can be done with Prometheus and Grafana\n\nPrometheus Server has exporters which pull metrics from different sources and stores it locally which can be then queried using PromQL. Grafana uses PromQL to show the data.\nTelegraf with InfluxDB provides flexibility in data collection and storage, making it well-suited for scalable, long-term retention of metrics data.\nWhen it comes to measuring Data Drift and Model prediction Quality, it is bit different.\nBased on the nature of the problem, Data Drift models that can captures Outliers, Anomalies has to be placed. Model selection is highly dependent on the business, here TIG can be used to measure things.\nIn terms Model predictive quality there is needs to be some ground truth against which we can measure those changes but it unlikely that there will be something like that present, given there is also problem of data drift. What we can do is measure the drop i.e drop in engagment, user count, purchase etc. These events can be measured by paid tools like Amplitude, Google Analytics, Firebase or with some custom solution."
  },
  {
    "objectID": "posts/Machine Learning/data_model_manage_prod.html#logging-and-alerts",
    "href": "posts/Machine Learning/data_model_manage_prod.html#logging-and-alerts",
    "title": "Data and Model Management | Model Monitoring and Logging.",
    "section": "Logging and Alerts",
    "text": "Logging and Alerts\nLogging is a necessity of production.\nPython comes with a feature rich way of generating logs in Logging Module and only in depth tutorial to know everything Logging How To and Logging Cookbook.\nWhen there are multiple services from where logs get generated, There needs to be way to efficiently collecting, aggregating and analyse that data.\nThere are multiple ways of doing it. I am listing a few here\n\nELK stack with an extra Filebeat\nAWS Cloudwatch\nApache Flume\nFluentd\n\nELK stack (Elastic Search , Log Stash, Kibana) -&gt; When the web servers generate logs those logs will get read by Log Stash and we can read those logs from Log Stash directly, but to take a step further we can filter and process those logs using Log Stash Grok and make the unstructed generate log into a structured Log which can be then stored in Elastic Search and visualize on Kibana.\nFilebeat uses a backpressure-sensitive protocol when sending data to Logstash or Elasticsearch to account for higher volumes of data. If Logstash is busy crunching data, it lets Filebeat know to slow down its read. Once the congestion is resolved, Filebeat will build back up to its original pace and keep on shippin’.\nIt is very common to using Elastic search for this purpose apart from using search tool.\n\n\nSame can be done using AWS Cloudwatch, if you entire infrastructure is on AWS. Amazon CloudWatch collects and visualizes real-time logs, metrics, and event data in automated dashboards to streamline your infrastructure and application maintenance.\n\nNow whether to go with ELK stack or fluentd or Cloudwatch or any other tool depends completely on the scale the logs will be generated, infrastructure needs, affordability, value generated out of those logs.\nAs an important measure it is not advised to delete old logs instantly rather archive it on Amazon S3 and then delete after certain amount of time pases.\nWhether we are logging or monitoring every other tools has a built in support for alerts and these alerts has to be generated based on certain criteria the frequency or time till the alerts has to be checked all of them can be customized. Grafana, Kibana, Cloudwatch, Fluentd all of them supports it. We can send alerts to email, slack, discord, sms there are tons of plugin which gives out of the box support for every other communication channel."
  },
  {
    "objectID": "posts/Machine Learning/data_model_manage_prod.html#thank-you",
    "href": "posts/Machine Learning/data_model_manage_prod.html#thank-you",
    "title": "Data and Model Management | Model Monitoring and Logging.",
    "section": "Thank you",
    "text": "Thank you"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html",
    "href": "posts/Machine Learning/metrics.html",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Example\nLet’s take an example of Binary classification where the task is to predict whether we have Dog or not. So in an Image if model predicts Dog that’s a positive class if it predicts no Dog that’s a negative class.\n\n\nA confusion matrix is a table that is used to describe the performance of a classification model.\n\nLet’s consider this example table where N denotes the total number of images our model will predict upon.\nN = 150 Total Number of Dog Images = 100 Total Number of No Dog Images = 50\nIn total 100 Dog Images 1. Model Predicted 60 Correct which is your True Positive. 2. Model Predicted 40 incorrect which is False Negative.(Since It has to predict Dog and It Predicted No Dog which is a False prediction to a Negative class.)\nSimilarly in 50 No Dog Images 1. Model Predicted 30 Correct which is True Negative. 2. Model Predicted 20 Incorrect which is False Positive. (Since It has to predict No Dog and It predicted Dog which is a False prediction to a Positive class.)\nTP-&gt;60 FN-&gt;40 TN-&gt;30 FP-&gt;20\n\n\n\nAccuracy is a basic metric which just tells models overall performance. How many predictions made by model is correct.\n\\[\nAccuracy = \\frac{True Positive + True Negative}{N}\n\\]\n\\[\nAccuracy = \\frac{60+30}{150} = 0.6 = 60\n\\]\nAccuracy gave us a idea about how 60 % prediction was correct. Accuracy can only be a good metric. if all the classes are balanced i.e No of positive sample is approximately equal to No of negative samples. Per class Accuracy can also be calculated to know for which classes model is behaving well.\n\n\n\nMisclassification Rate tells overall how poor model performance is. It just opposite of Accuracy.\n\\[\nmisclassification\\ rate = 1 - Accuracy\n\\]\n\\[\nmisclassifcation\\ rate = \\frac{False Postive + False Negative}{N}\n\\]\n\\[\nmisclassification\\ rate = \\frac{20+40}{150} = 0.4 = 40\\%\n\\]\n\n\n\nPrecision is another metric which tells while predicting how accurately can I predict positive classes .\n\\[\nPositive\\ class\\ prediction  = True\\ positive + False\\ Positive\n\\]\n\\[\nprecision = \\frac{True\\ Positive}{positive\\ class\\ prediction}\n\\]\n\\[\nprecision = \\frac {60}{60 + 20} = 0.75 = 75\\%\n\\]\nWe are only worried about the prediction of one class which is Dog and the positive prediction class will have (Positive class, Negative class) from Actuals. That’s why it is positive predictive value.\nLet’s consider two example where\n\nSpam detection -&gt; Classify whether an email is Spam or Not Spam.\nHere the goal is to accurately classify spam emails. It’s okay to classify a Spam mail as Not Spam mail as it will come in our inbox it does no harm to use. But if we classify as Not Spam Mail as Spam mail then there is a problem because we generally do not open our Spam Box.\nIf you think about it the first case is False Negative and Second case is False Positive and we are okay with False Negative’s but we are not okay with False Positives and our goal is to reduce False Positive.\nSo in Spam detection task precision is a good metric. since it is inversely proportionate to False Positives.\nCancer detection -&gt; Classify whether a person has a cancer or not.\nHere the goal is to accurately classify whether a person has a cancer or Not. It’s okay to classify a person Not having cancer as cancer. But it’s not okay to predict a person having cancer as Not cancer.\nIf you think the first case is False Positive and Second case is False Negative and we are okay with False Positive but not okay with False Negative.\nHence in this particular task Precision plays no role.\n\nHence to reduce False Positives Precision is used. Precision can easily be effected by class Imbalance.\n\n\n\nNegative Predictive Value is another metric which tells while predicting how accurately can I predict Negative classes .\n\\[\nNegative\\ class\\ prediction  = True\\ Negative + False\\ Negative\n\\]\n\\[\nNegative\\ Prediction\\ Value = \\frac{True\\ Negative}{Negative\\ class\\ prediction}\n\\]\n\\[\nnegative\\ prediction\\ value = \\frac {60}{60 + 20} = 0.75 = 75\\%\n\\]\nWe are only worried about the prediction of one class which is Dog and the positive prediction class will have (Positive class, Negative class) from Actuals. That’s why it is positive predictive value.\nLet Suppose we don’t want to have any additional process for screening patients checked as healthy (not cancer) then we may want to make sure that our negative predictive value is high.\n\n\n\nRecall is another metric which tells us while predicting how accurately can it predict positive classes given a set of Actual Positives.\n\\[\nActual\\ positive\\ class = True\\ Positive + False\\ Negative\n\\]\n\\[\nrecall = \\frac{True\\ Positive}{Actual\\ positive\\ class}\n\\]\n\\[\nrecall = \\frac{60}{100} = 0.6 = 60\\%\n\\]\nHere our concern is about Given a set of Positive Samples which is like giving all the Dog images and then making prediction on it. The prediction will have (positive class , Negative class ) from Actual positives. That’s why recall is also True Positive Rate .\nThe reason why it’s called recall is given all the positive sample knowledge how well the model can recall that knowledge to predicted accurately by decreasing the error rate for the Actual positive class.\nGoing back to the Cancer Example it is very clear know that for Cancer Detection we will use Recall as our metric. Recall is good metric to be used for class Imbalance problem.\n\n\n\nSimilar to True positive rate, True Negative rate tells us while predicting how accurately can it predict Negative classes given a set of Actual Negatives.\n\\[\nActual\\ negative\\ class = True\\ Negative + False\\ Positive\n\\]\n\\[\nTrue\\ Negative\\ Rate = \\frac{True\\ Negative}{Actual\\ negative\\ class}\n\\]\n\\[\nTrue\\ Negative\\ Rate = \\frac{60}{100} = 0.6 = 60\\%\n\\]\nHere our concern is about Given a set of Negative Samples which is like giving all the No Dog images and then making prediction on it. The prediction will have (positive class , Negative class ) from Actual Negatives.\nFor the same cancer example True Negative rate will show how many non cancer people are identified as not having cancer.\n\n\n\n\nn many tests, including diagnostic medical tests, sensitivity is the extent to which actual positives are not overlooked (so false negatives are few), and specificity is the extent to which actual negatives are classified as such (so false positives are few). Thus, a highly sensitive test rarely overlooks an actual positive (for example, showing “nothing bad” despite something bad existing); a highly specific test rarely registers a positive classification for anything that is not the target of testing (for example, finding one bacterial species and mistaking it for another closely related one that is the true target); and a test that is highly sensitive and highly specific does both, so it “rarely overlooks a thing that it is looking for” and it “rarely mistakes anything else for that thing.” Because most medical tests do not have sensitivity and specificity values above 99%, “rarely” does not equate to certainty. But for practical reasons, tests with sensitivity and specificity values above 90% have high credibility, albeit usually no certainty, in differential diagnosis.\nSensitivity, therefore, quantifies the avoidance of false negatives and specificity does the same for false positives.\n\n\n\nWhen the model predicts something Falsely to the positive class which then it is contributing to the False Positive rate. we can think of it as False alert. For example if in a production house based on certain machine parameters the model has to predict whether the situation insider the production house is dangerous or not and it has to raise alarm if its dangerous. Now if everything is fine and still the model predicts as dangerous situation then that’s a False alarm which you can say a False Positive Rate.\nFalse Positive Rate is just opposite of True Negative Rate\n\\[\nActual\\ negative\\ class = True\\ Negative + False\\ Positive\n\\]\n\\[\nFalse\\ positive\\ Rate = \\frac{False\\ Positive}{Actual\\ negative\\ class}\n\\]\n\\[\nFalse\\ Positive\\ Rate = 1 - True\\ Negative\\ Rate\n\\]\nThe lower the False Positive Rate the better the model.\n\n\n\nWhen the model doesn’t predict something which it should then it is contributing to the False Negative Rate. We can think it as Miss Rate. For example in Online fraud transaction if the model classifies a Fraud Transaction as a Non Fraud Transaction then the model basically missed to catch that Fraud transaction.\nFalse Negative Rate is just of True Positive Rate\n\\[\nActual\\ positive\\ class = True\\ Positive + False\\ Negative\n\\]\n\\[\nFalse\\ Negative\\ Rate = \\frac{False\\ Negative}{Actual\\ positive\\ class}\n\\]\n\\[\nFalse\\ Negative\\ Rate = 1 - True\\ Positive\\ Rate\n\\]\n\n\n\nFalse Discovery Rate is just opposite of Precision It measures how many predictions out of all positive predictions were incorrect.\n\\[\nPositive\\ class\\ prediction  = True\\ positive + False\\ Positive\n\\]\n\\[\nFalse\\ Discovery\\ Rate = \\frac{False\\ Positive}{positive\\ class\\ prediction}\n\\]\n\\[\nFalse\\ Discovery\\ Rate = 1 - Precision\n\\]\nWhen raising False alert is expensive it is worth looking every Positive prediction then we should optimize for precision.\n\n\n\nFalse Omission Rate is just opposite of Negative Predictive Value\n\\[\nFalse\\ Omission\\ Rate = 1 - Negative\\ Predictive\\ Value\n\\]\n\n\n\nNow that two important metric which is used often is precision and recall and rather then having too look two number F1 score combines precision and recall.\nThe score lies in the range [0,1] with 1 being ideal and 0 being the worst. The two ways to combine Precision and recall is\n\\[\nArithmetic\\ Mean\\\\\nF1\\ score = \\frac{precision + recall}{2}\n\\]\n\\[\nHarmonic\\ Mean\\\\\nF1\\ Score = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}}\n\\]\nThe reason to choose Harmonic mean over Arithmetic mean is precision and recall both have same numerator but different denominators so it makes no sense to average two different things as, fractions are more sensible to average by arithmetic mean when they have the same denominator. Rather we take reciprocal so that the average makes sense.\n\n\n\nIt’s a metric that combines precision and recall, putting 2x emphasis on recall.\n\\[\nF2\\ score = \\frac{1+2}{\\frac{2}{precision} + \\frac{1}{recall}}\n\\]\n\n\n\nF beta score is a general formula for F1 score and F2 score\nWhen choosing beta in your F-beta score the more you care about recall over precision the higher beta you should choose. For example, with F1 score we care equally about recall and precision with F2 score, recall is twice as important to us.\nWith 0&lt;beta&lt;1 we care more about precision\n\\[\nF beta \\ score = \\frac{1+\\beta}{\\frac{\\beta}{precision} + \\frac{1}{recall}}\n\\]\n\n\n\nmicro\nCalculate metrics globally by counting the total number of times each class was correctly predicted and incorrectly predicted. Micro Average captures class-imbalance and will bring down the precision\nFor example in Iris Dataset the model prediction result is given in the table\n\n\n\n\nTP\nFP\n\n\n\n\nSetosa\n45\n5\n\n\nVirgnica\n10\n60\n\n\nVersicolor\n40\n10\n\n\n\n\\[\nmicro\\ precision = \\frac{45 + 10 + 40}{45+ 10 + 40 + 5+60+10 }  = 0.55\n\\]\nmacro\nCalculate metrics for each “class” independently, and find their unweighted mean. This does not take label imbalance into account. In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class.\n\\[\nSetosa\\ precision = \\frac{45}{45+5} =0.9\\\\\nvirgnica\\ precision = \\frac{10}{10 + 60} =0.14\\\\\nversicolor\\ precision = \\frac{40}{40+10} = 0.8\\\\\n\\]\n\\[\nMacro\\ Precision = \\frac{0.9+0.14+0.8}{3} = 0.613\n\\]\nweighted accounts for class imbalance by computing the average of binary metrics in which each class’s score is weighted by its presence in the true data sample.\n\n\n\nPrecision-Recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.\nThe precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\nA system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly.\n\n\n\n\nA receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.\nThe top left corner of the plot is the “ideal” point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better.\nThe “steepness” of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate.\nIt can also be used for Mutli label classification problem.\n\n\n\n\nThe function cohen_kappa_score computes Cohen’s kappa statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.\nThe kappa score is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).\nKappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators.\nFor Kappa score formulae and calculation refer Cohen’s kappa\n\n\n\nThe Hamming loss is the fraction of labels that are incorrectly predicted.\nEvaluation metrics for multi-label classification performance are inherently different from those used in multi-class (or binary) classification, due to the inherent differences of the classification problem. If T denotes the true set of labels for a given sample, and P the predicted set of labels, then the following metrics can be defined on that sample:\nHamming loss: the fraction of the wrong labels to the total number of labels, i.e.\n\\[\nhamming\\ loss  = {\\frac {1}{|N|\\ . |L|}}\\sum_{i=1}^{|N|}\\sum_{j=1}^{|L|}xor (y_{i,j},z_{i,j})\n\\]\nwhere y_ij target and z_ij is the prediction. This is a loss function, so the optimal value is zero.\nHamming Loss computes Hamming distance and In information theory, the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other. In a more general context, the Hamming distance is one of several string metrics for measuring the edit distance between two sequences. It is named after the American mathematician Richard Hamming.\n\n\n\nTill Now For Binary Classification Problem we haven’t encountered any metric which incorporates all 4 parts of the confusion matrix and works good either we have balanced dataset or a Imbalanced one.\nMatthews Correlation Coefficient is the answer It is a more reliable statistical rate which produces high score only if the prediction obtained good results in all 4 parts of the confusion matrix.\nIt computes correlation coefficient between the true class and the predicted class the higher the correlation coefficient the better the model is at prediction.\nThe MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient.\n\\[\nMCC = \\frac{TP \\times TN - FP \\times FN }{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n\\]\nIf there is no False prediction, then the model has +1 as a correlation coefficient since (FP x FN = 0) vice-versa if (TP x TN = 0) then the model has -1 as a correlation coefficient.\nAdvantages of MCC over accuracy and F1 score\n\n\n\nAP summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight:\n\\[\nAP = \\sum_n{(R_n-R_{n-1}) P_n}\n\\]\nwhere Pn and Rn denotes the nth threshold.\nThis metric is also used in Object Detection.\n\nIntution Behind Average Precision\nWikipedia Average Precision\n\n\n\n\nBalanced Accuracy is metric used to deal with Imbalanced dataset. It is the average of Sensitivity and Specificity . In more generic term averaging recall of all classes.\nSensitivity covers the True Positive part and Specificity covers True Negative Part.\n\\[\nBalanced\\ Accuracy = \\frac{sensitivity + specificity}{2}\n\\]\n\n\n\nIn an ideal model, the probability scores of all true 1’s should be greater than the probability scores of ALL true 0’s. Such a model is said to be perfectly concordant and this phenomenon can be measured by Concordance and Discordance.\nSo how to calculate Concordance?\nLet’s consider the following 4 observation’s actual class and predicted probability scores.\n\n\n\nPatient No\nTrue Class\nProbability Score\n\n\n\n\nP1\n1\n0.9\n\n\nP2\n0\n0.42\n\n\nP3\n1\n0.30\n\n\nP4\n1\n0.80\n\n\n\nFrom the above 4 observations, there are 3 possible pairs of 1’s and 0’s. That is, P1-P2, P3-P2 and P4-P2.\nA pair is said to be concordant if the probability score of True 1 is greater than the probability score of True 0.\nP1-P2 =&gt; 0.9 &gt; 0.42 =&gt; Concordant! P3-P2 =&gt; 0.3 &lt; 0.42 =&gt; Discordant! P4-P2 =&gt; 0.8 &gt; 0.42 =&gt; Concordant!\nOut of the 3 pairs, only 2 are concordant. So, the concordance is 2/3 = 0.66 and discordance is 1 - 0.66 = 0.33.\nIn simpler words, we take all possible combinations of true events and non-events. Concordance is the percentage of pairs, where true event’s probability scores are greater than the scores of true non-events.\nFor a perfect model, this will be 100%. So, the higher the concordance, the better is the quality of the model."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#confusion-matrix",
    "href": "posts/Machine Learning/metrics.html#confusion-matrix",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "A confusion matrix is a table that is used to describe the performance of a classification model.\n\nLet’s consider this example table where N denotes the total number of images our model will predict upon.\nN = 150 Total Number of Dog Images = 100 Total Number of No Dog Images = 50\nIn total 100 Dog Images 1. Model Predicted 60 Correct which is your True Positive. 2. Model Predicted 40 incorrect which is False Negative.(Since It has to predict Dog and It Predicted No Dog which is a False prediction to a Negative class.)\nSimilarly in 50 No Dog Images 1. Model Predicted 30 Correct which is True Negative. 2. Model Predicted 20 Incorrect which is False Positive. (Since It has to predict No Dog and It predicted Dog which is a False prediction to a Positive class.)\nTP-&gt;60 FN-&gt;40 TN-&gt;30 FP-&gt;20"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#accuracy",
    "href": "posts/Machine Learning/metrics.html#accuracy",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Accuracy is a basic metric which just tells models overall performance. How many predictions made by model is correct.\n\\[\nAccuracy = \\frac{True Positive + True Negative}{N}\n\\]\n\\[\nAccuracy = \\frac{60+30}{150} = 0.6 = 60\n\\]\nAccuracy gave us a idea about how 60 % prediction was correct. Accuracy can only be a good metric. if all the classes are balanced i.e No of positive sample is approximately equal to No of negative samples. Per class Accuracy can also be calculated to know for which classes model is behaving well."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#misclassification-rate",
    "href": "posts/Machine Learning/metrics.html#misclassification-rate",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Misclassification Rate tells overall how poor model performance is. It just opposite of Accuracy.\n\\[\nmisclassification\\ rate = 1 - Accuracy\n\\]\n\\[\nmisclassifcation\\ rate = \\frac{False Postive + False Negative}{N}\n\\]\n\\[\nmisclassification\\ rate = \\frac{20+40}{150} = 0.4 = 40\\%\n\\]"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#precision-positive-predictive-value",
    "href": "posts/Machine Learning/metrics.html#precision-positive-predictive-value",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Precision is another metric which tells while predicting how accurately can I predict positive classes .\n\\[\nPositive\\ class\\ prediction  = True\\ positive + False\\ Positive\n\\]\n\\[\nprecision = \\frac{True\\ Positive}{positive\\ class\\ prediction}\n\\]\n\\[\nprecision = \\frac {60}{60 + 20} = 0.75 = 75\\%\n\\]\nWe are only worried about the prediction of one class which is Dog and the positive prediction class will have (Positive class, Negative class) from Actuals. That’s why it is positive predictive value.\nLet’s consider two example where\n\nSpam detection -&gt; Classify whether an email is Spam or Not Spam.\nHere the goal is to accurately classify spam emails. It’s okay to classify a Spam mail as Not Spam mail as it will come in our inbox it does no harm to use. But if we classify as Not Spam Mail as Spam mail then there is a problem because we generally do not open our Spam Box.\nIf you think about it the first case is False Negative and Second case is False Positive and we are okay with False Negative’s but we are not okay with False Positives and our goal is to reduce False Positive.\nSo in Spam detection task precision is a good metric. since it is inversely proportionate to False Positives.\nCancer detection -&gt; Classify whether a person has a cancer or not.\nHere the goal is to accurately classify whether a person has a cancer or Not. It’s okay to classify a person Not having cancer as cancer. But it’s not okay to predict a person having cancer as Not cancer.\nIf you think the first case is False Positive and Second case is False Negative and we are okay with False Positive but not okay with False Negative.\nHence in this particular task Precision plays no role.\n\nHence to reduce False Positives Precision is used. Precision can easily be effected by class Imbalance."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#negative-predictive-value",
    "href": "posts/Machine Learning/metrics.html#negative-predictive-value",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Negative Predictive Value is another metric which tells while predicting how accurately can I predict Negative classes .\n\\[\nNegative\\ class\\ prediction  = True\\ Negative + False\\ Negative\n\\]\n\\[\nNegative\\ Prediction\\ Value = \\frac{True\\ Negative}{Negative\\ class\\ prediction}\n\\]\n\\[\nnegative\\ prediction\\ value = \\frac {60}{60 + 20} = 0.75 = 75\\%\n\\]\nWe are only worried about the prediction of one class which is Dog and the positive prediction class will have (Positive class, Negative class) from Actuals. That’s why it is positive predictive value.\nLet Suppose we don’t want to have any additional process for screening patients checked as healthy (not cancer) then we may want to make sure that our negative predictive value is high."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#recall-true-positive-rate-sensitivity",
    "href": "posts/Machine Learning/metrics.html#recall-true-positive-rate-sensitivity",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Recall is another metric which tells us while predicting how accurately can it predict positive classes given a set of Actual Positives.\n\\[\nActual\\ positive\\ class = True\\ Positive + False\\ Negative\n\\]\n\\[\nrecall = \\frac{True\\ Positive}{Actual\\ positive\\ class}\n\\]\n\\[\nrecall = \\frac{60}{100} = 0.6 = 60\\%\n\\]\nHere our concern is about Given a set of Positive Samples which is like giving all the Dog images and then making prediction on it. The prediction will have (positive class , Negative class ) from Actual positives. That’s why recall is also True Positive Rate .\nThe reason why it’s called recall is given all the positive sample knowledge how well the model can recall that knowledge to predicted accurately by decreasing the error rate for the Actual positive class.\nGoing back to the Cancer Example it is very clear know that for Cancer Detection we will use Recall as our metric. Recall is good metric to be used for class Imbalance problem."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#selectivity-true-negative-rate-specificity",
    "href": "posts/Machine Learning/metrics.html#selectivity-true-negative-rate-specificity",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Similar to True positive rate, True Negative rate tells us while predicting how accurately can it predict Negative classes given a set of Actual Negatives.\n\\[\nActual\\ negative\\ class = True\\ Negative + False\\ Positive\n\\]\n\\[\nTrue\\ Negative\\ Rate = \\frac{True\\ Negative}{Actual\\ negative\\ class}\n\\]\n\\[\nTrue\\ Negative\\ Rate = \\frac{60}{100} = 0.6 = 60\\%\n\\]\nHere our concern is about Given a set of Negative Samples which is like giving all the No Dog images and then making prediction on it. The prediction will have (positive class , Negative class ) from Actual Negatives.\nFor the same cancer example True Negative rate will show how many non cancer people are identified as not having cancer."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#sensitivity-vs-specificity",
    "href": "posts/Machine Learning/metrics.html#sensitivity-vs-specificity",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "n many tests, including diagnostic medical tests, sensitivity is the extent to which actual positives are not overlooked (so false negatives are few), and specificity is the extent to which actual negatives are classified as such (so false positives are few). Thus, a highly sensitive test rarely overlooks an actual positive (for example, showing “nothing bad” despite something bad existing); a highly specific test rarely registers a positive classification for anything that is not the target of testing (for example, finding one bacterial species and mistaking it for another closely related one that is the true target); and a test that is highly sensitive and highly specific does both, so it “rarely overlooks a thing that it is looking for” and it “rarely mistakes anything else for that thing.” Because most medical tests do not have sensitivity and specificity values above 99%, “rarely” does not equate to certainty. But for practical reasons, tests with sensitivity and specificity values above 90% have high credibility, albeit usually no certainty, in differential diagnosis.\nSensitivity, therefore, quantifies the avoidance of false negatives and specificity does the same for false positives."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#false-positive-rate-type-i-error",
    "href": "posts/Machine Learning/metrics.html#false-positive-rate-type-i-error",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "When the model predicts something Falsely to the positive class which then it is contributing to the False Positive rate. we can think of it as False alert. For example if in a production house based on certain machine parameters the model has to predict whether the situation insider the production house is dangerous or not and it has to raise alarm if its dangerous. Now if everything is fine and still the model predicts as dangerous situation then that’s a False alarm which you can say a False Positive Rate.\nFalse Positive Rate is just opposite of True Negative Rate\n\\[\nActual\\ negative\\ class = True\\ Negative + False\\ Positive\n\\]\n\\[\nFalse\\ positive\\ Rate = \\frac{False\\ Positive}{Actual\\ negative\\ class}\n\\]\n\\[\nFalse\\ Positive\\ Rate = 1 - True\\ Negative\\ Rate\n\\]\nThe lower the False Positive Rate the better the model."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#false-negative-rate-type---ii-error",
    "href": "posts/Machine Learning/metrics.html#false-negative-rate-type---ii-error",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "When the model doesn’t predict something which it should then it is contributing to the False Negative Rate. We can think it as Miss Rate. For example in Online fraud transaction if the model classifies a Fraud Transaction as a Non Fraud Transaction then the model basically missed to catch that Fraud transaction.\nFalse Negative Rate is just of True Positive Rate\n\\[\nActual\\ positive\\ class = True\\ Positive + False\\ Negative\n\\]\n\\[\nFalse\\ Negative\\ Rate = \\frac{False\\ Negative}{Actual\\ positive\\ class}\n\\]\n\\[\nFalse\\ Negative\\ Rate = 1 - True\\ Positive\\ Rate\n\\]"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#false-discovery-rate",
    "href": "posts/Machine Learning/metrics.html#false-discovery-rate",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "False Discovery Rate is just opposite of Precision It measures how many predictions out of all positive predictions were incorrect.\n\\[\nPositive\\ class\\ prediction  = True\\ positive + False\\ Positive\n\\]\n\\[\nFalse\\ Discovery\\ Rate = \\frac{False\\ Positive}{positive\\ class\\ prediction}\n\\]\n\\[\nFalse\\ Discovery\\ Rate = 1 - Precision\n\\]\nWhen raising False alert is expensive it is worth looking every Positive prediction then we should optimize for precision."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#false-omission-rate",
    "href": "posts/Machine Learning/metrics.html#false-omission-rate",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "False Omission Rate is just opposite of Negative Predictive Value\n\\[\nFalse\\ Omission\\ Rate = 1 - Negative\\ Predictive\\ Value\n\\]"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#f-1-score-beta-1",
    "href": "posts/Machine Learning/metrics.html#f-1-score-beta-1",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Now that two important metric which is used often is precision and recall and rather then having too look two number F1 score combines precision and recall.\nThe score lies in the range [0,1] with 1 being ideal and 0 being the worst. The two ways to combine Precision and recall is\n\\[\nArithmetic\\ Mean\\\\\nF1\\ score = \\frac{precision + recall}{2}\n\\]\n\\[\nHarmonic\\ Mean\\\\\nF1\\ Score = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}}\n\\]\nThe reason to choose Harmonic mean over Arithmetic mean is precision and recall both have same numerator but different denominators so it makes no sense to average two different things as, fractions are more sensible to average by arithmetic mean when they have the same denominator. Rather we take reciprocal so that the average makes sense."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#f-2-score-beta-2",
    "href": "posts/Machine Learning/metrics.html#f-2-score-beta-2",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "It’s a metric that combines precision and recall, putting 2x emphasis on recall.\n\\[\nF2\\ score = \\frac{1+2}{\\frac{2}{precision} + \\frac{1}{recall}}\n\\]"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#f-beta-score",
    "href": "posts/Machine Learning/metrics.html#f-beta-score",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "F beta score is a general formula for F1 score and F2 score\nWhen choosing beta in your F-beta score the more you care about recall over precision the higher beta you should choose. For example, with F1 score we care equally about recall and precision with F2 score, recall is twice as important to us.\nWith 0&lt;beta&lt;1 we care more about precision\n\\[\nF beta \\ score = \\frac{1+\\beta}{\\frac{\\beta}{precision} + \\frac{1}{recall}}\n\\]"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#averaging-parameter",
    "href": "posts/Machine Learning/metrics.html#averaging-parameter",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "micro\nCalculate metrics globally by counting the total number of times each class was correctly predicted and incorrectly predicted. Micro Average captures class-imbalance and will bring down the precision\nFor example in Iris Dataset the model prediction result is given in the table\n\n\n\n\nTP\nFP\n\n\n\n\nSetosa\n45\n5\n\n\nVirgnica\n10\n60\n\n\nVersicolor\n40\n10\n\n\n\n\\[\nmicro\\ precision = \\frac{45 + 10 + 40}{45+ 10 + 40 + 5+60+10 }  = 0.55\n\\]\nmacro\nCalculate metrics for each “class” independently, and find their unweighted mean. This does not take label imbalance into account. In problems where infrequent classes are nonetheless important, macro-averaging may be a means of highlighting their performance. On the other hand, the assumption that all classes are equally important is often untrue, such that macro-averaging will over-emphasize the typically low performance on an infrequent class.\n\\[\nSetosa\\ precision = \\frac{45}{45+5} =0.9\\\\\nvirgnica\\ precision = \\frac{10}{10 + 60} =0.14\\\\\nversicolor\\ precision = \\frac{40}{40+10} = 0.8\\\\\n\\]\n\\[\nMacro\\ Precision = \\frac{0.9+0.14+0.8}{3} = 0.613\n\\]\nweighted accounts for class imbalance by computing the average of binary metrics in which each class’s score is weighted by its presence in the true data sample."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#precision-recall-curve",
    "href": "posts/Machine Learning/metrics.html#precision-recall-curve",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.\nThe precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\nA system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#roc-auc-curve",
    "href": "posts/Machine Learning/metrics.html#roc-auc-curve",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.\nThe top left corner of the plot is the “ideal” point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better.\nThe “steepness” of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate.\nIt can also be used for Mutli label classification problem."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#cohens-kappa",
    "href": "posts/Machine Learning/metrics.html#cohens-kappa",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "The function cohen_kappa_score computes Cohen’s kappa statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.\nThe kappa score is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).\nKappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators.\nFor Kappa score formulae and calculation refer Cohen’s kappa"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#hamming-loss",
    "href": "posts/Machine Learning/metrics.html#hamming-loss",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "The Hamming loss is the fraction of labels that are incorrectly predicted.\nEvaluation metrics for multi-label classification performance are inherently different from those used in multi-class (or binary) classification, due to the inherent differences of the classification problem. If T denotes the true set of labels for a given sample, and P the predicted set of labels, then the following metrics can be defined on that sample:\nHamming loss: the fraction of the wrong labels to the total number of labels, i.e.\n\\[\nhamming\\ loss  = {\\frac {1}{|N|\\ . |L|}}\\sum_{i=1}^{|N|}\\sum_{j=1}^{|L|}xor (y_{i,j},z_{i,j})\n\\]\nwhere y_ij target and z_ij is the prediction. This is a loss function, so the optimal value is zero.\nHamming Loss computes Hamming distance and In information theory, the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other. In a more general context, the Hamming distance is one of several string metrics for measuring the edit distance between two sequences. It is named after the American mathematician Richard Hamming."
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#matthews-correlation-coefficient",
    "href": "posts/Machine Learning/metrics.html#matthews-correlation-coefficient",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Till Now For Binary Classification Problem we haven’t encountered any metric which incorporates all 4 parts of the confusion matrix and works good either we have balanced dataset or a Imbalanced one.\nMatthews Correlation Coefficient is the answer It is a more reliable statistical rate which produces high score only if the prediction obtained good results in all 4 parts of the confusion matrix.\nIt computes correlation coefficient between the true class and the predicted class the higher the correlation coefficient the better the model is at prediction.\nThe MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient.\n\\[\nMCC = \\frac{TP \\times TN - FP \\times FN }{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n\\]\nIf there is no False prediction, then the model has +1 as a correlation coefficient since (FP x FN = 0) vice-versa if (TP x TN = 0) then the model has -1 as a correlation coefficient.\nAdvantages of MCC over accuracy and F1 score"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#average-precision-score",
    "href": "posts/Machine Learning/metrics.html#average-precision-score",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "AP summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight:\n\\[\nAP = \\sum_n{(R_n-R_{n-1}) P_n}\n\\]\nwhere Pn and Rn denotes the nth threshold.\nThis metric is also used in Object Detection.\n\nIntution Behind Average Precision\nWikipedia Average Precision"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#balanced-accuracy",
    "href": "posts/Machine Learning/metrics.html#balanced-accuracy",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "Balanced Accuracy is metric used to deal with Imbalanced dataset. It is the average of Sensitivity and Specificity . In more generic term averaging recall of all classes.\nSensitivity covers the True Positive part and Specificity covers True Negative Part.\n\\[\nBalanced\\ Accuracy = \\frac{sensitivity + specificity}{2}\n\\]"
  },
  {
    "objectID": "posts/Machine Learning/metrics.html#concordance-and-discordance",
    "href": "posts/Machine Learning/metrics.html#concordance-and-discordance",
    "title": "Metrics for Machine Learning",
    "section": "",
    "text": "In an ideal model, the probability scores of all true 1’s should be greater than the probability scores of ALL true 0’s. Such a model is said to be perfectly concordant and this phenomenon can be measured by Concordance and Discordance.\nSo how to calculate Concordance?\nLet’s consider the following 4 observation’s actual class and predicted probability scores.\n\n\n\nPatient No\nTrue Class\nProbability Score\n\n\n\n\nP1\n1\n0.9\n\n\nP2\n0\n0.42\n\n\nP3\n1\n0.30\n\n\nP4\n1\n0.80\n\n\n\nFrom the above 4 observations, there are 3 possible pairs of 1’s and 0’s. That is, P1-P2, P3-P2 and P4-P2.\nA pair is said to be concordant if the probability score of True 1 is greater than the probability score of True 0.\nP1-P2 =&gt; 0.9 &gt; 0.42 =&gt; Concordant! P3-P2 =&gt; 0.3 &lt; 0.42 =&gt; Discordant! P4-P2 =&gt; 0.8 &gt; 0.42 =&gt; Concordant!\nOut of the 3 pairs, only 2 are concordant. So, the concordance is 2/3 = 0.66 and discordance is 1 - 0.66 = 0.33.\nIn simpler words, we take all possible combinations of true events and non-events. Concordance is the percentage of pairs, where true event’s probability scores are greater than the scores of true non-events.\nFor a perfect model, this will be 100%. So, the higher the concordance, the better is the quality of the model."
  },
  {
    "objectID": "posts/Computer Vision/object_detection.html",
    "href": "posts/Computer Vision/object_detection.html",
    "title": "Object Detection",
    "section": "",
    "text": "1. Object Classification- Tells you what the “main subject” of the image is\n2. Object Localization- Predict and draw bounding boxes around on object in an image\n3. Object Detection- Find multiple objects, classify them, and locate where they are in the image.\n\nWhy it is difficult\n\nCan have varying number of objects in an image and we do not know ahead of time how many we would expect in an image.\nChoosing a right crop is not a trivial task as we may encounter any number of images which :\n\ncan be at any place.\ncan be of any aspect ratio.\ncan be of any size.\n\n\n\n\nTypically, there are three steps in an object detection framework.\n\nObject localization component A model or algorithm is used to generate regions of interest or region proposals. These region proposals are a large set of bounding boxes spanning the full image.\n\nSome of the famous approaches:\n\nSelective Search - A clustering based approach which attempts to group pixels and generate proposals based on the generated clusters.\nRegion Proposal using Deep Learning Model (Features extracted from the image to generate regions) - Based on the features from a deep learning model\nBrute Force - Similar to a sliding window that is applied to the image, over several ratios and scales. These regions are generated automatically, without taking into account the image features.\n\n\n\nObject classification component In the second step, visual features are extracted for each of the bounding boxes, they are evaluated and it is determined whether and which objects are present in the proposals based on visual features.\n\nSome of the famous approaches:\n\nUse pre-trained image classification models to extract visual features\nTraditional Computer Vision (filter based approached, histogram methods, etc.)\n\n\nNon maximum suppression In the final post-processing step, reduce the number of detections in a frame to the actual number of objects present to make sure overlapping boxes are combined into a single bounding box. NMS techniques are typically standard across the different detection frameworks, but it is an important step that might require hyper-parameter tweaking based on the scenario.\n\nPredicted \nDesired \n\n\n\n\n\nBounding box is represented using : x_min , y_min , x_max , y_max\nx_min: The x-coordinate of the top-left corner of the bounding box.\ny_min: The y-coordinate of the top-left corner of the bounding box.\nx_max: The x-coordinate of the bottom-right corner of the bounding box.\ny_max: The y-coordinate of the bottom-right corner of the bounding box.\n\nBut pixel values are next to useless if we don't know the actual dimensions of the image. A better way would be to represent all coordinates is in their fractional form.\n\n\nFrom boundary coordinates to centre size coordinates x_min, y_min, x_max, y_max -&gt; c_x,c_y,w,h\nFrom centre size coordinates to bounding box coordinates c_x , c_y ,w , h -&gt; x_min, y_min, x_max, y_max\n\nUsing normalized coordinates (coordinates scaled between 0 and 1) is a common practice in object detection to make bounding box representations independent of image dimensions.\n\n\n\n\nHow well the one box matches the the other box we can compute the IOU (or intersection-over-union, also known as the Jaccard index) between the two bounding boxes.\nSteps to Calculate:\n\nDefine the intersection area as the area where the predicted and ground truth bounding boxes overlap.\nDefine the union area as the total area covered by both the predicted and ground truth bounding boxes.\nThe Jaccard Overlap, which is IOU, is calculated by dividing the intersection by the union. Jaccard Overlap = Intersection / Union\n\n\n\n\n\n\nMean Average Precision (mAP or mAP@0.5 or mAP@0.25) -\n\nIt is a number from 0 to 100 and higher values are typically better\n\n\nPrecision measures the accuracy of predictions. It tells you what percentage of the predicted objects are correct. In object detection, a high precision indicates that the model’s detections are mostly accurate.\nRecall, on the other hand, measures how well the model finds all the actual objects in the image. A high recall means that the model can detect most of the objects present in the image.\n\nHowever the standard metric of Precision or Recall used in image classification problems cannot be directly applied here because we want both the classification and localization of a model need to be evaluated.This is where mAP(Mean Average-Precision) is comes into the picture.\nFor calculating Precision and Recall we need:\nTrue Positives (TP): These are the correct detections made by the model, where the predicted bounding box significantly overlaps with the ground truth box.\nFalse Positives (FP): These are incorrect detections, where the model predicts an object, but it doesn’t overlap significantly with the ground truth box.\nFalse Negatives (FN): These are the objects that the model misses; it fails to detect them.\nLet us see how can we calculate them in the context of Object Detection\nTrue Positive and False Positive Using IOU we can determine if the detection(a Positive) is correct(True) or not(False). Considering a threshold of 0.5 for IOU and 0.5 for confidence score So any score &gt;=0.5 and IOU &gt;=0.5 - True Positive any score &gt;=0.5 and IOU &lt; 0.5 - False Positive.\nUsing above information we can calculate:\n\nPrecision for each class = TP/(TP+FP)\nRecall for each class = TP/(TP+FN)\n\nBut the value of Precision and Recall is very much dependent on threshold assigned to Confidence score and IOU.\n\nFor IOU, either we can decide a fixed threshold like in VOC Dataset or calculate in the range (say 0.5 to .95) in the case of COCO Dataset\nThe confidence factor varies across models, 50% confidence in my model design might probably be equivalent to an 80% confidence in someone else’s model design, which would vary the precision recall curve shape.\n\nTo overcome that problem we go for ... Average Precision Area under the precision-recall curve (PR curve)\nLet us take an oversimplified example where we just have 5 image in which we have 5 object of the same class. We consider 10 predictions and if IOU&gt;=0.5 we call it a correct prediction.\n\nAt rank #3 Precision is the proportion of TP = 2/3 = 0.67.\nRecall is the proportion of TP out of the possible positives = 2/5 = 0.4. If we plot it \nThings to note: Precision will have a zig zag pattern because it goes down with false positives and goes up again with true positives.\nA good classifier will be good at ranking correct images near the top of the list, and be able to retrieve a lot of correct images before retrieving any incorrect: its precision will stay high as recall increases. A poor classifier will have to take a large hit in precision to get higher recall.\nAverage Precision(AP) = \nInterpolated Average Precision (IAP) Reference from the paper\nTo smooth out the zigzag pattern in the Precision Recall Curve caused by small variations in the ranking of examples. Graphically, at each recall level, we replace each precision value with the maximum precision value to the right of that recall level.\n\n\nFurther, there are variations on where to take the samples when computing the interpolated average precision. Some take samples at a fixed 11 points from 0 to 1: {0, 0.1, 0.2, …, 0.9, 1.0}. This is called the 11-point interpolated average precision. Others sample at every k where the recall changes(Area under the Curve)\nInterpolated Average Precision\n\n\nwe divide the recall value from 0 to 1.0 into 11 points — 0, 0.1, 0.2, …, 0.9 and 1.0. Next, we compute the average of maximum precision value for these 11 recall values.\nIn our example, AP = (5 × 1.0 + 4 × 0.57 + 2 × 0.5)/11\nIssues with Interpolated AP:\n\nIt is less precise due to interpolation.\nit lost the capability in measuring the difference for methods with low AP. Therefore, a new AP calculation is introduced after 2008 for PASCAL VOC.\n\nAP (Area under curve AUC) By interpolating all points, the Average Precision (AP) can be interpreted as an approximated AUC of the Precision - Recall curve. The intention is to reduce the impact of the wiggles in the curve.\nInstead of sampling at fixed values, sample the curve at all unique recall values (r₁, r₂, …), whenever the maximum precision value drops. With this change, we are measuring the exact area under the precision-recall curve after the zigzags are removed.Hence No approximation or interpolation is needed\n\n\n\n\n\n\n\nCOCO (Common Objects in Context):\n\nCOCO is one of the most widely used object detection datasets.\nIt contains a diverse set of images with 80 object categories.\nThe dataset includes over 200,000 labeled images for object detection and segmentation tasks.\nAnnotations include object bounding boxes, object category labels, and segmentation masks.\nCOCO also provides a set of evaluation metrics, making it suitable for benchmarking and comparing object detection algorithms.\n\nPASCAL VOC (Visual Object Classes):\n\nThe PASCAL VOC dataset is a classic benchmark in computer vision.\nIt consists of 20 object categories, and the dataset is divided into train, validation, and test sets.\nThe annotations include bounding boxes and object category labels.\nPASCAL VOC has been widely used for object detection, classification, and segmentation tasks.\n\nImageNet Object Detection Challenge:\n\nImageNet is primarily known for its large-scale image classification dataset, but it also hosts object detection challenges.\nThe dataset contains thousands of object categories and millions of images.\nIt includes bounding box annotations for object detection tasks.\n\nOpen Images Dataset:\n\nOpen Images is a dataset with millions of images and thousands of object categories.\nIt provides annotations for object detection, image classification, and visual relationship detection.\nThe dataset is diverse and covers a wide range of everyday objects and scenes.\n\nKITTI Dataset:\n\nThe KITTI dataset is focused on autonomous driving and contains images and LiDAR data.\nIt includes annotations for various tasks, including object detection, object tracking, and road segmentation.\nObject detection annotations in KITTI cover categories such as cars, pedestrians, and cyclists.\n\nYouTube-BoundingBoxes Dataset:\n\nThis dataset contains object detection annotations for video frames extracted from YouTube videos.\nIt includes various object categories and is suitable for object detection in video content.\n\n\n\n\n\n\n\n\nYolo paper did a very simplifies a job. So what do want to predict\n\nBounding Boxes - 4 parameters\nFor each bounding box what is the object inside\n(Optional) Confidence score for those boxes\n\n\n\n\nAs we can see we have an input image of size 448*448 we do all convolution operation and convert it 7*7. Now we have got 49 grids. we predict the bounding box co-ordinate from these grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object.\n\n\n\n\nLocalization Loss (Box Coordinates Loss): This loss measures the error in predicting the coordinates of the bounding boxes. YOLO predicts the center coordinates (x, y), width (w), and height (h) of the bounding boxes. The loss is typically computed using Mean Squared Error (MSE) between the predicted box coordinates and the ground truth box coordinates for the object. The localization loss is usually represented as:\nLocalization Loss = λ_coord * ∑[over all grid cells] ∑[over all bounding boxes for the cell] [(x - x_true)^2 + (y - y_true)^2 + (sqrt(w) - sqrt(w_true))^2 + (sqrt(h) - sqrt(h_true))^2]\nHere, λ_coord is a weight to balance the importance of the localization loss.\nConfidence Loss (Objectness Loss): This loss measures the confidence of the model in predicting whether an object exists within a grid cell and how well the predicted bounding box overlaps with the ground truth box. It is computed using the binary cross-entropy loss. The confidence loss is typically represented as:\nConfidence Loss = ∑[over all grid cells] ∑[over all bounding boxes for the cell] [I_obj * (C - C_true)^2 + I_noobj * (C - C_true)^2]\nHere, I_obj is an indicator function that is 1 if an object exists in the cell and 0 otherwise. I_noobj is an indicator function that is 1 if no object exists in the cell and 0 otherwise. C is the predicted confidence score, and C_true is the ground truth confidence score.\nClass Loss: This loss measures the error in predicting the class of the object present in each grid cell. YOLO typically uses categorical cross-entropy loss to compute the class loss. It is represented as:\nClass Loss = λ_class * ∑[over all grid cells] ∑[over all bounding boxes for the cell] [I_obj * ∑(C_i - C_i_true)^2]\nHere, λ_class is a weight to balance the importance of the class loss. C_i is the predicted class probability for class i, and C_i_true is the ground truth class label.\nThe total YOLO loss is the sum of the localization loss, confidence loss, and class loss:\nTotal Loss = Localization Loss + Confidence Loss + Class Loss\n\n\n\nYOLO imposes strong spatial constraints on bounding box predictions since each grid cell only predicts two boxes and can only have one class. This spatial constraint limits the number of nearby objects that our model can predict.\nModel struggles with small objects that appear in groups, such as flocks of birds. Since model learns to predict bounding boxes from data, it struggles to generalize to objects in new or unusual aspect ratios or configurations. Our model also uses relatively coarse features for predicting bounding boxes since the architecture has multiple downsampling layers from the input image.\n\n\n\n\nWhat better SSD does, it fixes the limitation of yolo by making detection at multiple scale. So rather than using final feature map to generate bounding boxes, it samples from different layers in between the architecture and generate recommendation\nTo achieve high detection accuracy they produce predictions of different scales from feature maps of different scales, and explicitly separate predictions by aspect ratio.\nThese design features lead to simple end-to-end training and high accuracy, even on low resolution input images, further improving the speed vs accuracy trade-off.\n\n\n\n\nDefault Boxes (or Prior Boxes):\nDefault boxes, often referred to as “prior boxes,” are a set of pre-defined bounding boxes with specific sizes and aspect ratios that are placed at various positions on the feature maps generated by different layers of the convolutional neural network (CNN). These default boxes are used to predict the locations and sizes of objects in the input image. The network learns to adjust these default boxes during training to better match the actual objects in the image. By having default boxes at multiple scales and aspect ratios, SSD is capable of detecting objects of different sizes and proportions efficiently.\n\n\n\n\nThe primary goal of the objective function (or loss function) in SSD is to train the model to make accurate predictions about object locations and classes in an input image. The objective function is a combination of localization loss and confidence loss:\n\nLocalization Loss: This component of the objective function measures how accurately the model predicts the locations (coordinates) of objects in the image. It’s typically calculated using metrics like Smooth L1 loss. The localization loss is minimized when the model’s predicted bounding box coordinates are close to the ground-truth coordinates of the objects.\nConfidence Loss: The confidence loss is concerned with how well the model classifies objects and background regions. It involves predicting the objectness score for each default box (prior box) to determine whether it contains an object or not. Cross-entropy loss is commonly used for this purpose. The confidence loss is reduced when the model assigns higher scores to true positive predictions and lower scores to false positives.\n\nHard Negative Mining and NMS in SSD:\nNow, let’s see how SSD utilizes hard negative mining and Non-Maximum Suppression:\n\nHard Negative Mining:\n\nHard negative mining is used to address the problem of unbalanced datasets, where there are many more background regions than actual objects in the image. This can lead to a biased model that tends to predict most regions as background.\nDuring training, hard negative mining identifies and focuses on challenging background regions that the model has difficulty distinguishing from actual objects. These challenging negatives are typically the false positive predictions with the highest confidence scores.\nBy emphasizing these difficult-to-classify background regions, the model learns to make more accurate predictions about what constitutes an object and what doesn’t. The challenging negatives help reduce the false negative rate and improve overall accuracy.\n\n\n\n\n\n\n\n\n\n\n\n\nR-CNN Working\n\nTake an input Image\nUse Selective Search Algorithm to generate approximately ~2k proposals Selective Search:\n\nGenerate initial sub-segmentation, we generate many candidate regions\nUse greedy algorithm to recursively combine similar regions into larger ones\nUse the generated regions to produce the final candidate region proposals`\n\nWarp all the proposals into a fix size proposals which will be input to the convolutions\nFeed Each warped proposals(~2k) into Convolutions Network which gives 4096-dimensional Feature Vector\nEach of the feature vector is send to SVMs for Classification of a object with in a region proposal\nThe Networks all gives 4 values which predicts the offsets of the predicted bounding compared to Ground truth.\n\nPros:\n\nIt led the foundation for Two Stage Detectors\nR-CNN achieves a mean-average precision (mAP) of53.7% on PASCAL VOC 2010.\n\nCons:\n\nSelective Search Algorithm and Convolution operation on each proposal makes training, time and memory consuming.\nInference is Extremely slow as it takes around 47 seconds for each test image.\nThe selective search algorithm is a fixed algorithm. Therefore, no learning is happening at that stage. This could lead to the generation of bad candidate region proposals.\n\n\n\n\n\n\nHere In Fast R-CNN few of previous drawbacks of R-CNN are solved\n\nUsing a Single Convolution Networks into which we pass the entire image which generates a feature map.\nThe Region of Interest(RoI) generated using selective search algorithm is then projected on the feature map.\nThe RoI is generated on the scale of original image but the feature map spatial dimension is small in comparison to RoI so, the mapping is done by converting RoI to feature map scale.\nBecause different size RoIs are cropped from the feature they are feed into RoI pool layer which performs a pooling operation and converts RoI into a fixed size feature map.\nPooled feature map are then feed into two separate branches one which does classification and other Regression.\n\nFor Classification :- Cross Entropy Loss\nFor Regression :- SmoothL1Loss\nSmoothL1Loss -&gt; It combines both L1 Loss and L2 Loss\n\nPros: The reason “Fast R-CNN” is faster than R-CNN is because you don’t have to feed 2000 region proposals to the convolutional neural network every time. Instead, the convolution operation is done only once per image and a feature map is generated from it.\n Cons: When you look at the performance of Fast R-CNN during testing time, including region proposals slows down the algorithm significantly when compared to not using region proposals. Therefore, region proposals become bottlenecks in Fast R-CNN algorithm affecting its performance.\n\n\n\n\nBoth of the above algorithms(R-CNN & Fast R-CNN) uses selective search to find out the region proposals. Selective search is a slow and time-consuming process affecting the performance of the network. Hence In Faster R-CNN the authors of paper introduced Regional Proposal Network (RPN)\nThe RPN produces two main outputs:\nObjectness Score: The Objectness Score predicts whether an object exists within a Region of Interest (RoI). It essentially evaluates whether a particular region contains an object or not. This output comprises 2K predictions, where K represents the number of anchor boxes.\nBounding Boxes: The Bounding Boxes output predicts the coordinates for the bounding boxes. This information is vital for precisely localizing the object within the RoI. This output contains 4K coordinate predictions corresponding to the anchor boxes.\nUsing these outputs, the RPN generates RoIs, which are regions that are likely to contain objects of interest. These RoIs serve as the basis for object detection. This RPN is trained separately.\nBounding Boxes :- It Predicts bounding boxes with 4K coordinates.\nUsing RPN we generate RoI.\nAfter RPN gives RoI it's all similar to Fast R-CNN.\n\n\n\n\n\n\nFaster R-CNN Architecture Details\nThe Faster R-CNN architecture can be broken down into several key components:\nBackbone Network (VGG16): Faster R-CNN uses the VGG16 model as the backbone network to extract features from the input image. This network provides a hierarchical representation of the image, which is crucial for object detection.\nProposal Creator: This component creates RoIs using anchor boxes. Anchor boxes are predefined boxes of various sizes and aspect ratios, which the RPN uses to propose regions of interest.\nProposal Target Creator: After generating a set of RoIs, the Proposal Target Creator subsamples the top RoIs and assigns targets to them. This process is essential for efficient training and ensures that the model focuses on the most informative RoIs.\nAnchor Target Generator: The Anchor Target Generator is responsible for generating targets for the anchor boxes. These targets are used to calculate the RPN loss and train the RPN network. It plays a critical role in fine-tuning the anchor box predictions.\nLoss Calculation: Faster R-CNN calculates four distinct loss values: RPN_reg_loss (Regression loss for RPN), RPN_cls_loss (Classification loss for RPN), RoI_reg_loss (Regression loss for RoIs), and RoI_cls_loss (Classification loss for RoIs). These losses quantify the errors in object localization and classification. To compute the total loss, all four of these losses are added together.\n\n\n\n\n\n\n\nhttps://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/"
  },
  {
    "objectID": "posts/Computer Vision/object_detection.html#general-object-detection-framework",
    "href": "posts/Computer Vision/object_detection.html#general-object-detection-framework",
    "title": "Object Detection",
    "section": "",
    "text": "Typically, there are three steps in an object detection framework.\n\nObject localization component A model or algorithm is used to generate regions of interest or region proposals. These region proposals are a large set of bounding boxes spanning the full image.\n\nSome of the famous approaches:\n\nSelective Search - A clustering based approach which attempts to group pixels and generate proposals based on the generated clusters.\nRegion Proposal using Deep Learning Model (Features extracted from the image to generate regions) - Based on the features from a deep learning model\nBrute Force - Similar to a sliding window that is applied to the image, over several ratios and scales. These regions are generated automatically, without taking into account the image features.\n\n\n\nObject classification component In the second step, visual features are extracted for each of the bounding boxes, they are evaluated and it is determined whether and which objects are present in the proposals based on visual features.\n\nSome of the famous approaches:\n\nUse pre-trained image classification models to extract visual features\nTraditional Computer Vision (filter based approached, histogram methods, etc.)\n\n\nNon maximum suppression In the final post-processing step, reduce the number of detections in a frame to the actual number of objects present to make sure overlapping boxes are combined into a single bounding box. NMS techniques are typically standard across the different detection frameworks, but it is an important step that might require hyper-parameter tweaking based on the scenario.\n\nPredicted \nDesired"
  },
  {
    "objectID": "posts/Computer Vision/object_detection.html#concepts",
    "href": "posts/Computer Vision/object_detection.html#concepts",
    "title": "Object Detection",
    "section": "",
    "text": "Bounding box is represented using : x_min , y_min , x_max , y_max\nx_min: The x-coordinate of the top-left corner of the bounding box.\ny_min: The y-coordinate of the top-left corner of the bounding box.\nx_max: The x-coordinate of the bottom-right corner of the bounding box.\ny_max: The y-coordinate of the bottom-right corner of the bounding box.\n\nBut pixel values are next to useless if we don't know the actual dimensions of the image. A better way would be to represent all coordinates is in their fractional form.\n\n\nFrom boundary coordinates to centre size coordinates x_min, y_min, x_max, y_max -&gt; c_x,c_y,w,h\nFrom centre size coordinates to bounding box coordinates c_x , c_y ,w , h -&gt; x_min, y_min, x_max, y_max\n\nUsing normalized coordinates (coordinates scaled between 0 and 1) is a common practice in object detection to make bounding box representations independent of image dimensions.\n\n\n\n\nHow well the one box matches the the other box we can compute the IOU (or intersection-over-union, also known as the Jaccard index) between the two bounding boxes.\nSteps to Calculate:\n\nDefine the intersection area as the area where the predicted and ground truth bounding boxes overlap.\nDefine the union area as the total area covered by both the predicted and ground truth bounding boxes.\nThe Jaccard Overlap, which is IOU, is calculated by dividing the intersection by the union. Jaccard Overlap = Intersection / Union\n\n\n\n\n\n\nMean Average Precision (mAP or mAP@0.5 or mAP@0.25) -\n\nIt is a number from 0 to 100 and higher values are typically better\n\n\nPrecision measures the accuracy of predictions. It tells you what percentage of the predicted objects are correct. In object detection, a high precision indicates that the model’s detections are mostly accurate.\nRecall, on the other hand, measures how well the model finds all the actual objects in the image. A high recall means that the model can detect most of the objects present in the image.\n\nHowever the standard metric of Precision or Recall used in image classification problems cannot be directly applied here because we want both the classification and localization of a model need to be evaluated.This is where mAP(Mean Average-Precision) is comes into the picture.\nFor calculating Precision and Recall we need:\nTrue Positives (TP): These are the correct detections made by the model, where the predicted bounding box significantly overlaps with the ground truth box.\nFalse Positives (FP): These are incorrect detections, where the model predicts an object, but it doesn’t overlap significantly with the ground truth box.\nFalse Negatives (FN): These are the objects that the model misses; it fails to detect them.\nLet us see how can we calculate them in the context of Object Detection\nTrue Positive and False Positive Using IOU we can determine if the detection(a Positive) is correct(True) or not(False). Considering a threshold of 0.5 for IOU and 0.5 for confidence score So any score &gt;=0.5 and IOU &gt;=0.5 - True Positive any score &gt;=0.5 and IOU &lt; 0.5 - False Positive.\nUsing above information we can calculate:\n\nPrecision for each class = TP/(TP+FP)\nRecall for each class = TP/(TP+FN)\n\nBut the value of Precision and Recall is very much dependent on threshold assigned to Confidence score and IOU.\n\nFor IOU, either we can decide a fixed threshold like in VOC Dataset or calculate in the range (say 0.5 to .95) in the case of COCO Dataset\nThe confidence factor varies across models, 50% confidence in my model design might probably be equivalent to an 80% confidence in someone else’s model design, which would vary the precision recall curve shape.\n\nTo overcome that problem we go for ... Average Precision Area under the precision-recall curve (PR curve)\nLet us take an oversimplified example where we just have 5 image in which we have 5 object of the same class. We consider 10 predictions and if IOU&gt;=0.5 we call it a correct prediction.\n\nAt rank #3 Precision is the proportion of TP = 2/3 = 0.67.\nRecall is the proportion of TP out of the possible positives = 2/5 = 0.4. If we plot it \nThings to note: Precision will have a zig zag pattern because it goes down with false positives and goes up again with true positives.\nA good classifier will be good at ranking correct images near the top of the list, and be able to retrieve a lot of correct images before retrieving any incorrect: its precision will stay high as recall increases. A poor classifier will have to take a large hit in precision to get higher recall.\nAverage Precision(AP) = \nInterpolated Average Precision (IAP) Reference from the paper\nTo smooth out the zigzag pattern in the Precision Recall Curve caused by small variations in the ranking of examples. Graphically, at each recall level, we replace each precision value with the maximum precision value to the right of that recall level.\n\n\nFurther, there are variations on where to take the samples when computing the interpolated average precision. Some take samples at a fixed 11 points from 0 to 1: {0, 0.1, 0.2, …, 0.9, 1.0}. This is called the 11-point interpolated average precision. Others sample at every k where the recall changes(Area under the Curve)\nInterpolated Average Precision\n\n\nwe divide the recall value from 0 to 1.0 into 11 points — 0, 0.1, 0.2, …, 0.9 and 1.0. Next, we compute the average of maximum precision value for these 11 recall values.\nIn our example, AP = (5 × 1.0 + 4 × 0.57 + 2 × 0.5)/11\nIssues with Interpolated AP:\n\nIt is less precise due to interpolation.\nit lost the capability in measuring the difference for methods with low AP. Therefore, a new AP calculation is introduced after 2008 for PASCAL VOC.\n\nAP (Area under curve AUC) By interpolating all points, the Average Precision (AP) can be interpreted as an approximated AUC of the Precision - Recall curve. The intention is to reduce the impact of the wiggles in the curve.\nInstead of sampling at fixed values, sample the curve at all unique recall values (r₁, r₂, …), whenever the maximum precision value drops. With this change, we are measuring the exact area under the precision-recall curve after the zigzags are removed.Hence No approximation or interpolation is needed"
  },
  {
    "objectID": "posts/Computer Vision/object_detection.html#dataset",
    "href": "posts/Computer Vision/object_detection.html#dataset",
    "title": "Object Detection",
    "section": "",
    "text": "COCO (Common Objects in Context):\n\nCOCO is one of the most widely used object detection datasets.\nIt contains a diverse set of images with 80 object categories.\nThe dataset includes over 200,000 labeled images for object detection and segmentation tasks.\nAnnotations include object bounding boxes, object category labels, and segmentation masks.\nCOCO also provides a set of evaluation metrics, making it suitable for benchmarking and comparing object detection algorithms.\n\nPASCAL VOC (Visual Object Classes):\n\nThe PASCAL VOC dataset is a classic benchmark in computer vision.\nIt consists of 20 object categories, and the dataset is divided into train, validation, and test sets.\nThe annotations include bounding boxes and object category labels.\nPASCAL VOC has been widely used for object detection, classification, and segmentation tasks.\n\nImageNet Object Detection Challenge:\n\nImageNet is primarily known for its large-scale image classification dataset, but it also hosts object detection challenges.\nThe dataset contains thousands of object categories and millions of images.\nIt includes bounding box annotations for object detection tasks.\n\nOpen Images Dataset:\n\nOpen Images is a dataset with millions of images and thousands of object categories.\nIt provides annotations for object detection, image classification, and visual relationship detection.\nThe dataset is diverse and covers a wide range of everyday objects and scenes.\n\nKITTI Dataset:\n\nThe KITTI dataset is focused on autonomous driving and contains images and LiDAR data.\nIt includes annotations for various tasks, including object detection, object tracking, and road segmentation.\nObject detection annotations in KITTI cover categories such as cars, pedestrians, and cyclists.\n\nYouTube-BoundingBoxes Dataset:\n\nThis dataset contains object detection annotations for video frames extracted from YouTube videos.\nIt includes various object categories and is suitable for object detection in video content."
  },
  {
    "objectID": "posts/Computer Vision/object_detection.html#one-stage-detector",
    "href": "posts/Computer Vision/object_detection.html#one-stage-detector",
    "title": "Object Detection",
    "section": "",
    "text": "Yolo paper did a very simplifies a job. So what do want to predict\n\nBounding Boxes - 4 parameters\nFor each bounding box what is the object inside\n(Optional) Confidence score for those boxes\n\n\n\n\nAs we can see we have an input image of size 448*448 we do all convolution operation and convert it 7*7. Now we have got 49 grids. we predict the bounding box co-ordinate from these grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object.\n\n\n\n\nLocalization Loss (Box Coordinates Loss): This loss measures the error in predicting the coordinates of the bounding boxes. YOLO predicts the center coordinates (x, y), width (w), and height (h) of the bounding boxes. The loss is typically computed using Mean Squared Error (MSE) between the predicted box coordinates and the ground truth box coordinates for the object. The localization loss is usually represented as:\nLocalization Loss = λ_coord * ∑[over all grid cells] ∑[over all bounding boxes for the cell] [(x - x_true)^2 + (y - y_true)^2 + (sqrt(w) - sqrt(w_true))^2 + (sqrt(h) - sqrt(h_true))^2]\nHere, λ_coord is a weight to balance the importance of the localization loss.\nConfidence Loss (Objectness Loss): This loss measures the confidence of the model in predicting whether an object exists within a grid cell and how well the predicted bounding box overlaps with the ground truth box. It is computed using the binary cross-entropy loss. The confidence loss is typically represented as:\nConfidence Loss = ∑[over all grid cells] ∑[over all bounding boxes for the cell] [I_obj * (C - C_true)^2 + I_noobj * (C - C_true)^2]\nHere, I_obj is an indicator function that is 1 if an object exists in the cell and 0 otherwise. I_noobj is an indicator function that is 1 if no object exists in the cell and 0 otherwise. C is the predicted confidence score, and C_true is the ground truth confidence score.\nClass Loss: This loss measures the error in predicting the class of the object present in each grid cell. YOLO typically uses categorical cross-entropy loss to compute the class loss. It is represented as:\nClass Loss = λ_class * ∑[over all grid cells] ∑[over all bounding boxes for the cell] [I_obj * ∑(C_i - C_i_true)^2]\nHere, λ_class is a weight to balance the importance of the class loss. C_i is the predicted class probability for class i, and C_i_true is the ground truth class label.\nThe total YOLO loss is the sum of the localization loss, confidence loss, and class loss:\nTotal Loss = Localization Loss + Confidence Loss + Class Loss\n\n\n\nYOLO imposes strong spatial constraints on bounding box predictions since each grid cell only predicts two boxes and can only have one class. This spatial constraint limits the number of nearby objects that our model can predict.\nModel struggles with small objects that appear in groups, such as flocks of birds. Since model learns to predict bounding boxes from data, it struggles to generalize to objects in new or unusual aspect ratios or configurations. Our model also uses relatively coarse features for predicting bounding boxes since the architecture has multiple downsampling layers from the input image.\n\n\n\n\nWhat better SSD does, it fixes the limitation of yolo by making detection at multiple scale. So rather than using final feature map to generate bounding boxes, it samples from different layers in between the architecture and generate recommendation\nTo achieve high detection accuracy they produce predictions of different scales from feature maps of different scales, and explicitly separate predictions by aspect ratio.\nThese design features lead to simple end-to-end training and high accuracy, even on low resolution input images, further improving the speed vs accuracy trade-off.\n\n\n\n\nDefault Boxes (or Prior Boxes):\nDefault boxes, often referred to as “prior boxes,” are a set of pre-defined bounding boxes with specific sizes and aspect ratios that are placed at various positions on the feature maps generated by different layers of the convolutional neural network (CNN). These default boxes are used to predict the locations and sizes of objects in the input image. The network learns to adjust these default boxes during training to better match the actual objects in the image. By having default boxes at multiple scales and aspect ratios, SSD is capable of detecting objects of different sizes and proportions efficiently.\n\n\n\n\nThe primary goal of the objective function (or loss function) in SSD is to train the model to make accurate predictions about object locations and classes in an input image. The objective function is a combination of localization loss and confidence loss:\n\nLocalization Loss: This component of the objective function measures how accurately the model predicts the locations (coordinates) of objects in the image. It’s typically calculated using metrics like Smooth L1 loss. The localization loss is minimized when the model’s predicted bounding box coordinates are close to the ground-truth coordinates of the objects.\nConfidence Loss: The confidence loss is concerned with how well the model classifies objects and background regions. It involves predicting the objectness score for each default box (prior box) to determine whether it contains an object or not. Cross-entropy loss is commonly used for this purpose. The confidence loss is reduced when the model assigns higher scores to true positive predictions and lower scores to false positives.\n\nHard Negative Mining and NMS in SSD:\nNow, let’s see how SSD utilizes hard negative mining and Non-Maximum Suppression:\n\nHard Negative Mining:\n\nHard negative mining is used to address the problem of unbalanced datasets, where there are many more background regions than actual objects in the image. This can lead to a biased model that tends to predict most regions as background.\nDuring training, hard negative mining identifies and focuses on challenging background regions that the model has difficulty distinguishing from actual objects. These challenging negatives are typically the false positive predictions with the highest confidence scores.\nBy emphasizing these difficult-to-classify background regions, the model learns to make more accurate predictions about what constitutes an object and what doesn’t. The challenging negatives help reduce the false negative rate and improve overall accuracy."
  },
  {
    "objectID": "posts/Computer Vision/object_detection.html#two-stage-detectors",
    "href": "posts/Computer Vision/object_detection.html#two-stage-detectors",
    "title": "Object Detection",
    "section": "",
    "text": "R-CNN Working\n\nTake an input Image\nUse Selective Search Algorithm to generate approximately ~2k proposals Selective Search:\n\nGenerate initial sub-segmentation, we generate many candidate regions\nUse greedy algorithm to recursively combine similar regions into larger ones\nUse the generated regions to produce the final candidate region proposals`\n\nWarp all the proposals into a fix size proposals which will be input to the convolutions\nFeed Each warped proposals(~2k) into Convolutions Network which gives 4096-dimensional Feature Vector\nEach of the feature vector is send to SVMs for Classification of a object with in a region proposal\nThe Networks all gives 4 values which predicts the offsets of the predicted bounding compared to Ground truth.\n\nPros:\n\nIt led the foundation for Two Stage Detectors\nR-CNN achieves a mean-average precision (mAP) of53.7% on PASCAL VOC 2010.\n\nCons:\n\nSelective Search Algorithm and Convolution operation on each proposal makes training, time and memory consuming.\nInference is Extremely slow as it takes around 47 seconds for each test image.\nThe selective search algorithm is a fixed algorithm. Therefore, no learning is happening at that stage. This could lead to the generation of bad candidate region proposals.\n\n\n\n\n\n\nHere In Fast R-CNN few of previous drawbacks of R-CNN are solved\n\nUsing a Single Convolution Networks into which we pass the entire image which generates a feature map.\nThe Region of Interest(RoI) generated using selective search algorithm is then projected on the feature map.\nThe RoI is generated on the scale of original image but the feature map spatial dimension is small in comparison to RoI so, the mapping is done by converting RoI to feature map scale.\nBecause different size RoIs are cropped from the feature they are feed into RoI pool layer which performs a pooling operation and converts RoI into a fixed size feature map.\nPooled feature map are then feed into two separate branches one which does classification and other Regression.\n\nFor Classification :- Cross Entropy Loss\nFor Regression :- SmoothL1Loss\nSmoothL1Loss -&gt; It combines both L1 Loss and L2 Loss\n\nPros: The reason “Fast R-CNN” is faster than R-CNN is because you don’t have to feed 2000 region proposals to the convolutional neural network every time. Instead, the convolution operation is done only once per image and a feature map is generated from it.\n Cons: When you look at the performance of Fast R-CNN during testing time, including region proposals slows down the algorithm significantly when compared to not using region proposals. Therefore, region proposals become bottlenecks in Fast R-CNN algorithm affecting its performance.\n\n\n\n\nBoth of the above algorithms(R-CNN & Fast R-CNN) uses selective search to find out the region proposals. Selective search is a slow and time-consuming process affecting the performance of the network. Hence In Faster R-CNN the authors of paper introduced Regional Proposal Network (RPN)\nThe RPN produces two main outputs:\nObjectness Score: The Objectness Score predicts whether an object exists within a Region of Interest (RoI). It essentially evaluates whether a particular region contains an object or not. This output comprises 2K predictions, where K represents the number of anchor boxes.\nBounding Boxes: The Bounding Boxes output predicts the coordinates for the bounding boxes. This information is vital for precisely localizing the object within the RoI. This output contains 4K coordinate predictions corresponding to the anchor boxes.\nUsing these outputs, the RPN generates RoIs, which are regions that are likely to contain objects of interest. These RoIs serve as the basis for object detection. This RPN is trained separately.\nBounding Boxes :- It Predicts bounding boxes with 4K coordinates.\nUsing RPN we generate RoI.\nAfter RPN gives RoI it's all similar to Fast R-CNN."
  },
  {
    "objectID": "posts/Computer Vision/object_detection.html#understanding-basic-faster-r-cnn-architecture",
    "href": "posts/Computer Vision/object_detection.html#understanding-basic-faster-r-cnn-architecture",
    "title": "Object Detection",
    "section": "",
    "text": "Faster R-CNN Architecture Details\nThe Faster R-CNN architecture can be broken down into several key components:\nBackbone Network (VGG16): Faster R-CNN uses the VGG16 model as the backbone network to extract features from the input image. This network provides a hierarchical representation of the image, which is crucial for object detection.\nProposal Creator: This component creates RoIs using anchor boxes. Anchor boxes are predefined boxes of various sizes and aspect ratios, which the RPN uses to propose regions of interest.\nProposal Target Creator: After generating a set of RoIs, the Proposal Target Creator subsamples the top RoIs and assigns targets to them. This process is essential for efficient training and ensures that the model focuses on the most informative RoIs.\nAnchor Target Generator: The Anchor Target Generator is responsible for generating targets for the anchor boxes. These targets are used to calculate the RPN loss and train the RPN network. It plays a critical role in fine-tuning the anchor box predictions.\nLoss Calculation: Faster R-CNN calculates four distinct loss values: RPN_reg_loss (Regression loss for RPN), RPN_cls_loss (Classification loss for RPN), RoI_reg_loss (Regression loss for RoIs), and RoI_cls_loss (Classification loss for RoIs). These losses quantify the errors in object localization and classification. To compute the total loss, all four of these losses are added together."
  },
  {
    "objectID": "posts/Computer Vision/object_detection.html#references",
    "href": "posts/Computer Vision/object_detection.html#references",
    "title": "Object Detection",
    "section": "",
    "text": "https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/"
  },
  {
    "objectID": "posts/Python/parallel_processing.html",
    "href": "posts/Python/parallel_processing.html",
    "title": "Parallelism and Concurrency",
    "section": "",
    "text": "Let’s start from the bottom we have 3 things Socket, Processor, Core.\nEach socket holds 1 or more Processor and each processor can hold 1 or more cores.\n\nRun command lscpu on a Linux machine to see these values for your own system.\nSocket -&gt; It Connects processor to motherboard.\nProcessor -&gt; As a whole we generally refer it as place where all calculation happens. Just to be specific we are talking about Micro-Processors and here we use both the terms interchangeably.\nCores -&gt; They are units which do all the processing. Each one of them has their own set of registers, instruction set and operate individually.\n\nA very important point to note here is at one give point of time t one and only 1 task can be executed.\n\nSo when there are n cores only n tasks can be executed all together.\nFor an analogy in Sprint event if there are 5 lanes on the track then only 5 people can run together."
  },
  {
    "objectID": "posts/Python/parallel_processing.html#socket-processor-and-core",
    "href": "posts/Python/parallel_processing.html#socket-processor-and-core",
    "title": "Parallelism and Concurrency",
    "section": "",
    "text": "Let’s start from the bottom we have 3 things Socket, Processor, Core.\nEach socket holds 1 or more Processor and each processor can hold 1 or more cores.\n\nRun command lscpu on a Linux machine to see these values for your own system.\nSocket -&gt; It Connects processor to motherboard.\nProcessor -&gt; As a whole we generally refer it as place where all calculation happens. Just to be specific we are talking about Micro-Processors and here we use both the terms interchangeably.\nCores -&gt; They are units which do all the processing. Each one of them has their own set of registers, instruction set and operate individually.\n\nA very important point to note here is at one give point of time t one and only 1 task can be executed.\n\nSo when there are n cores only n tasks can be executed all together.\nFor an analogy in Sprint event if there are 5 lanes on the track then only 5 people can run together."
  },
  {
    "objectID": "posts/Python/parallel_processing.html#run-type-and-task-type",
    "href": "posts/Python/parallel_processing.html#run-type-and-task-type",
    "title": "Parallelism and Concurrency",
    "section": "Run Type and Task Type",
    "text": "Run Type and Task Type\nLet’s go a little deeper and inspect what is inside core but before there are few terminologies to understand.\n\nRun Type\n\nParallelism - A given point of time t, n tasks are getting executed simultaneously by n processing units i.e 10 people are painting 10 walls independently.\nConcurrency- A given point of time t 1 task is getting executing by 1 processing unit from a pool of tasks p. i.e a person cooking 5 dish at a time in a way that while his attention is on dish1, progress is happening on other dishes in background and when the person look at dish3 still work is going on on other four dishes.\n\n\n\nTask Type\n\nCompute Bound - Any type of task where we have to do lot mathematical computation, that is compute bound task, i.e if we do matrix multiplication of 2 big matrixes, the processing unit is doing multiplication and addition one after another without waiting, to pull out an analogy imagine water coming out of a tap, non stop.\nIO Bound - Input/Output tasks these kinds of tasks are where computer is waiting to either receive an input or an output. for example when we are downloading some file, doing database call, writing data to a file. Internally computer waiting for the result to come to further process and while it is waiting for the database to give a response it is not sitting idle and simply waiting, it prefers to go do something else. To pull out an analogy rather than waiting for the bucket to fill up from the tap let’s go and do something else until it fills.\n\nNow let’s take an example of a system where we have 3 processing units\n\nIf the task in hand is compute based and if can break it in 3 independent sub-tasks then we can process them in parallel.\nIf the task is IO Bound i.e I have to do 10 independent db calls then using just 1 processing unit I can process these concurrently. This is because the bottleneck in this scenario is typically the time it takes to wait for the database responses, not the processing power of the CPU.\nOne question which can be raised here is 2 processing units are idle here why not execute parallel concurrent where 10 tasks are divided into 3 processing units in parallel and within each unit concurrent then I must say concurrency is way much powerful not only 10 but we can doing 100 or 1000 concurrent db calls. Parallel concurrent makes sense only in very specific use cases.\nSo it is very clear now\nCompute Bound task -&gt; Parallelism  IO Bound Task -&gt; Concurrency\n\n\nExecution Type\nSynchronous and asynchronous execution are two different ways of managing and controlling the flow of tasks or operations in a program or system:\n\nSynchronous - In synchronous execution, tasks are executed one after the other in a sequential manner. Each task must complete before the next one starts. Hence we call this type execution Blocking. it typically blocks the execution of the entire program or thread. In other words, the program waits for a task to finish before moving on to the next one. For Example- Consider a simple synchronous function that reads a file line by line and processes each line. It reads one line, processes it, and then moves on to the next line, blocking the program until the file processing is complete.\nAsynchronous - In asynchronous execution, tasks can overlap and run concurrently. A task can start, and while it’s waiting for a resource (e.g., I/O operation, network request), the program can continue executing other tasks without waiting for the first task to complete. Hence we call this type execution Non-Blocking. When a task initiates an operation that might take time (e.g., waiting for a network response), it doesn’t block the entire program. Instead, it allows other tasks to proceed."
  },
  {
    "objectID": "posts/Python/parallel_processing.html#process-and-threads",
    "href": "posts/Python/parallel_processing.html#process-and-threads",
    "title": "Parallelism and Concurrency",
    "section": "Process and Threads",
    "text": "Process and Threads\nNow what is inside a core answer is Threads. Generally each core has 2 threads and inside the core actual execution of a task happens by the thread.\nNow if till now everything has been crystal clear it should be an easy answer that even if there are two threads inside a core at a given point of time only 1 thread executes.\nSo what these threads do to execute together they execute concurrently by doing a context switch.\n\nDuring the context switch the all the information about the threads has to be saved and reloaded and yes we spend cpu cycles doing that. Also the context switch depends on lot many things like the priority of the task, kernel scheduling policies. Often it happens in fixed interval of time of few milliseconds.\nWhen it comes to actual implementation of things there are many ways of doing execution, the current way of doing things is using Process - Thread Model\nProcesses are often described as containers of threads because they provide a high-level organizational structure for managing multiple threads of execution within an operating system.\n\n\nProcess -\n\nIndependence: Processes are independent execution units. Each process has its own memory space, code, data, and system resources. This isolation ensures that one process cannot directly access or interfere with the memory of another process.\nCommunication: Inter-process communication (IPC) is more challenging and typically involves mechanisms like message passing or using shared resources (e.g., files, pipes, sockets). IPC can be more complex to implement.\nResource Overhead: Processes have a higher resource overhead compared to threads because they require separate memory and resources. Creating and managing processes is generally more resource-intensive.\nFault Isolation: Due to their independence, if one process crashes or experiences a fault, it doesn’t necessarily affect other processes. Processes are good for building robust systems.\n\nThreads -\n\nShared Resources: Threads within the same process share the same memory space, code, and data. They can communicate and exchange data more easily.\nCommunication: Threads can communicate using shared memory, making IPC simpler compared to processes. However, shared memory requires careful synchronization to avoid data conflicts.\nResource Overhead: Threads have lower resource overhead than processes since they share resources with other threads in the same process. Creating and managing threads is less resource-intensive.\nFault Tolerance: A crash or error in one thread can potentially affect other threads within the same process since they share resources. Careful programming and synchronization mechanisms are required to avoid issues like race conditions and deadlocks.\n\n\nIn summary, processes provide stronger isolation but come with higher resource overhead, making them suitable for robustness and scalability across multiple processors or machines. Threads, on the other hand, share resources and memory, making them more lightweight and suitable for tasks that benefit from parallelism within a single process. The choice between processes and threads depends on the specific requirements of your application and the trade-offs you are willing to make in terms of resource usage and complexity.\nOne important distinction that has to be made here is Physical Memory vs virtual Memory vs Logical Memory. Physical memory is the actual hardware where data resides, virtual memory is a technique used to extend the available memory beyond physical RAM, and logical memory is the addressing and organizational system used by programs to interact with memory, abstracting the underlying physical and virtual memory structures.\nTherefore, Process doesn’t exists in reality it is an abstraction created and managed using virtual memory. Threads are not."
  },
  {
    "objectID": "posts/Python/parallel_processing.html#parallel-processing",
    "href": "posts/Python/parallel_processing.html#parallel-processing",
    "title": "Parallelism and Concurrency",
    "section": "Parallel Processing",
    "text": "Parallel Processing\nWhenever we talk about parallel processing, how it is done is highly dependent on OS and programming language implementation. But the above explained core foundation remains same.\nLet’s take an example we have 4 cores with each core 2 threads hence total 8 threads.\nIn Most of the programming languages like Java or C++ we have Multi-Threading model.\nSo if we have 4 cores and 8 threads and we do multi-threading invoking 4 threads, ideally all should run parallel (Kernel does this scheduling hence ideally.)\nIf we invoke 8 threads then only 4 can run in parallel other 4 we are doing context switch.\nwhat happens if we create 100 threads then also 4 will run in parallel other 96 will do context switch. Couple of points to not here\n\nwe would be wasting lot many cpu cycles in just doing context switch.\nCPU threads are not equivalent to threads we create in programming language.\nOne benefit on doing multi-threading this way is all threads are controlled by single process hence they are light weight.\n\nSame in python if we see we have a GIL lock which doesn’t allow multiple threads to execute together in parallel. Only 1 threads executes and we just do context switch.\nOne thing to note here is python threads are your cpu threads.\nNow question at hand is how to do Parallel Execution in python - Multi-Processing GIL works on thread level not on Process level.\nNow if we have 4 cores then we can invoke 4 processes which will run in parallel. why people don’t like multi-processing because of Resource Overhead.\nHence we can see in Multi-Threading model single process controls multiple threads. In multi-Processing model we have multiple process running with in which threads are executing stuff.\nIn python and in all other languages there is construct of Pool, when multi-threading or multiprocessing is involved, directly using those API are not recommended in most of the occasion we should be using it from the pool, ProcessPoolExecutor and ThreadPoolExecutor are two classes in python which facilitates this."
  },
  {
    "objectID": "posts/Python/parallel_processing.html#concurrency",
    "href": "posts/Python/parallel_processing.html#concurrency",
    "title": "Parallelism and Concurrency",
    "section": "Concurrency",
    "text": "Concurrency\nTill now we have seen for doing parallel programming, we use Multi-threading model in Other Language and Multi-processing in python. In python multi-threading model acts like concurrency because of GIL.\nThen a natural question is why to do concurrency using multi-threading model why not using single-thread model.\nIn python we can see we have two ways of doing concurrency one using Multi-Threading Model other is Async-Await model.\nNow Async-Await does concurrent execution on a single thread. They perform Co-operative MultiTasking hence they are called co-routines.\nasync module in python let’s you do that. its working is quite simple and implementation is quite expressive and impressive in python.\n\nThere are multiple components to the async module.\n\nTask - Tasks represent individual units of work that can be executed asynchronously.\nEvent Loop - The event loop schedules and coordinates the execution of these tasks, making it possible to run multiple tasks concurrently. So a event loop has a Queue where tasks are stored, a scheduler to schedule tasks according to their priority. The reason for calling it a loop because it is literally a infinite while loop which runs until there is no more task run.\nEvent Handlers and Callbacks - let’s pick a task t1 from the event loop we check whether we have the result if we have we process it otherwise we schedule the task t1 again to the event loop using callbacks to check again in the future. so we go to task t2 we do the same and keep repeating for all the tasks.\nselectors : We are wasting lot of cpu cycles in just checking rather than checking one by one all the tasks in the event loop for result. we ask the OS which sockets are ready for reading and writing. Clearly, the OS has this information. When a new packet arrives on a network interface, the OS gets notified, decodes the packet, determines the socket to which the packet belongs and wakes up the processes that do a blocking read on that socket. But a process doesn’t need to read from the socket to get notified. It can use an I/O multiplexing mechanism such as select(), poll() or epoll() to tell the OS that it’s interested in reading from or writing to some socket. When the socket becomes ready, the OS will wake up such processes as well.\nPromises and Futures: Promises in Javascript and futures in python are abstractions that represent the result of an asynchronous operation that may not be available immediately. They allow you to work with values that will be resolved in the future, and you can await them to retrieve the result when it’s ready.\n\nTwo very python specific thing in asyncio are\n\nCoroutines: Coroutines are special types of functions that can be paused and resumed during their execution. They are defined using the async keyword and can be used to represent asynchronous tasks. Coroutines are a fundamental building block in async programming and allow for cooperative multitasking.\nasync/await keyword -&gt; A function with def is normal function a function with async def becomes a coroutine. await keyword can only be called inside an async function i.e coroutine. so whenever a co-routine encounters an await keyword it pauses the execution of the coroutine, and starts the executing asynchronous operations once all the operation is complete the coroutine execution resumes. Now here asynchronous operations are called awaitables and we are awaiting on an awaitable using await keyword.\n\nHence we can conclude Threading is for working in parallel, and async is for waiting in parallel."
  },
  {
    "objectID": "posts/Python/parallel_processing.html#thank-you.",
    "href": "posts/Python/parallel_processing.html#thank-you.",
    "title": "Parallelism and Concurrency",
    "section": "Thank you.",
    "text": "Thank you."
  },
  {
    "objectID": "posts/Python/functional_programming.html",
    "href": "posts/Python/functional_programming.html",
    "title": "Functional Programming capabilites of python",
    "section": "",
    "text": "Lambda and Comphresions\nFunctions in python are first class Objects that means you can assign them to variable, store them in data structure, pass them as a parameter to other functions and even return them from other function\n\n\nCode\n# addition function \ndef add(x, y):\n    return x+y\nprint (f\" function is add(5,6) = {add(5,6)}\")\n# you can assign them to other variable\nmyAdd = add\n# wait you can also delete the add function and  the myAdd still points to underlying function\ndel add\nprint (f\" function is myAdd(5,6) = {myAdd(5,6)}\")\n#functions have their own set of attributes \nprint(f\"{myAdd.__name__}\")\n# to see a complete list of attributes of a function type dir(myAdd) in console\n\n\n function is add(5,6) = 11\n function is myAdd(5,6) = 11\nadd\n\n\n\n\nCode\n# functions as data structures\nList_Funcs = [str.upper , str.lower , str.title]\nfor f in List_Funcs:\n    print (f , f(\"aI6-saturdays\"))\n\n\n&lt;method 'upper' of 'str' objects&gt; AI6-SATURDAYS\n&lt;method 'lower' of 'str' objects&gt; ai6-saturdays\n&lt;method 'title' of 'str' objects&gt; Ai6-Saturdays\n\n\nSo lambdas are a sweet Little anonymous Single-Expression functions\n\n\nCode\nadd_lambda = lambda x , y : x+y # lambda automaticaly returns the value after colon\nprint(f\"lambda value add_lambda(2,3)= {add_lambda(2,3)}\") #you call lambda function as normal functions\n\n\nlambda value add_lambda(2,3)= 5\n\n\nYou :- But Wait it’s an anonymous function and how can you give it a name\nStackOverflow :- Relax, Searching for another example\n\n\nCode\ndef someFunc(func):\n    quote = func(\"We will democratize AI \")\n    return quote\n# here the lambda function is passes to a normal function \n# the lambda here is anonymous and the parameter my_sentence = We will democratize AI \n# so we are adding some text of ours and returning the string\nsomeFunc(lambda my_sentence: my_sentence+\"by teaching everyone AI\")\n\n\n'We will democratize AI by teaching everyone AI'\n\n\n\n\nCode\n# here is one more example\ntuples = [(1, 'd'), (2, 'b'), (4, 'a'), (3, 'c')]\nsorted(tuples, key=lambda x: x[1])\n\n\n\n\nCode\n# list comphrensions\nl = [ x for x in range(20)]\neven_list = [x for x in l if x%2==0]\neven_list_with_Zero = [x if x%2==0 else 0 for x in l ]\nl , even_list ,even_list_with_Zero\n\n\n([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n [0, 2, 4, 6, 8, 10, 12, 14, 16, 18],\n [0, 0, 2, 0, 4, 0, 6, 0, 8, 0, 10, 0, 12, 0, 14, 0, 16, 0, 18, 0])\n\n\n\n\nCode\n# dictionary comphrension\nd = {x: x**2 for x in range(2,6)}\nflip_key_value = {value:key for key,value in d.items()}\nd , flip_key_value\n\n\n({2: 4, 3: 9, 4: 16, 5: 25}, {4: 2, 9: 3, 16: 4, 25: 5})\n\n\n\n\nDecorators\nPython’s decorators allow you to extend and modify the behavior of a callable (functions, methods, and classes) without permanently modifying the callable itself.\nAny sufficiently generic functionality you can tack on to an existing class or function’s behavior makes a great use case for decoration. This includes the following:\n\nlogging\nenforcing access control and authentication\ninstrumentation and timing functions\nrate-limiting\ncaching, and more\n\nImagine that you some 50 functions in your code. Now that all functions are working you being a great programmer thought of optimizing each function by checking the amount of time it takes and also you need to log the input/output of few functions. what are you gonna do ?\nWithout decorators you might be spending the next three days modifying each of those 50 functions and clutter them up with your manual logging calls. Fun times, right?\n\n\nCode\ndef my_decorator(func):\n    return func  # It's simple right it takes a function as it's parameter and returns it\ndef someFunc():\n    return \"Deep learning is fun\"\nsomeFunc = my_decorator(someFunc) # it is similar to i = i + 1\nprint(f\" someFunc value  = {someFunc()}\")\n\n\n someFunc value  = Deep learning is fun\n\n\n\n\nCode\n# now just to add syntatic sugar to the code so that we can brag how easy and terse python code is \n# we gonna write this way\ndef my_decorator(func):\n    return func \n\n@my_decorator  # the awesomeness of this block of code lies here which can be used as toggle switch\ndef someFunc():\n    return \"Deep learning is fun\"\n\nprint(f\" someFunc value  = {someFunc()}\")\n\n\n someFunc value  = Deep learning is fun\n\n\nStackoverflow :- Now that you got a little taste of Decorators let’s write another decorator that actually does something and modifies the behavior of the decorated function.\n\n\nCode\n# This blocks contains and actual implementation of decorator\nimport time\n#import functools\n\ndef myTimeItDeco(func):\n    #@functools.wraps(func)\n    def wrapper(*args,**kwargs):\n        starttime = time.time()\n        call_of_func = func(*args,**kwargs) # this works because you can function can be nested and they remember the state\n        function_modification = call_of_func.upper()\n        endtime = time.time()\n        return f\" Executed output is {function_modification} and time is {endtime-starttime} \"\n    return wrapper\n\n@myTimeItDeco\ndef myFunc(arg1,arg2,arg3): # some arguments of no use to show how to pass them in code\n    \"\"\"Documentation of a obfuscate function\"\"\"\n    time.sleep(2) # just to show some complex calculation\n    return \"You had me at Hello world\"\n\nmyFunc(1,2,3) , myFunc.__doc__ , myFunc.__name__ \n\n\n(' Executed output is YOU HAD ME AT HELLO WORLD and time is 2.0032527446746826 ',\n None,\n 'wrapper')\n\n\nYou :- Why didn’t I got the doc and the name of my function. Hmmm….\nStackOverflow :- Great Programmers use me as there debugging tool, so use It.\nHints functools.wrap\nStackOverflow : - Applying Multiple Decorators to a Function (This is really fascinating as it’s gonna confuse you)\n\n\nCode\ndef strong(func):\n    def wrapper():\n        return '&lt;strong&gt;' + func() + '&lt;/strong&gt;'\n    return wrapper\n\ndef emphasis(func):\n    def wrapper():\n        return '&lt;em&gt;' + func() + '&lt;/em&gt;'\n    return wrapper\n\n@strong\n@emphasis\ndef greet():\n    return 'Hello!'\n\n\ngreet()\n# this is your assignment to understand it hints strong(emphasis(greet))()\n\n\n'&lt;strong&gt;&lt;em&gt;Hello!&lt;/em&gt;&lt;/strong&gt;'\n\n\n\n\nCode\n#Disclaimer Execute this at your own risk \n#Only  80's kids will remember this\nimport dis\ndis.dis(greet)\n\n\n\n\nContext Managers\n\n\nCode\n# let's open  a file and write some thing into it\nfile = open('hello.txt', 'w')\ntry:\n    file.write('Some thing')\nfinally:\n    file.close()\n\n\nYou :- ok now that I have wrote something into the file I want to read it, but try and finally again, it suck’s. There should be some other way around\nStackOverflow :- Context manger for your Rescue\n\n\nCode\nwith open(\"hello.txt\") as file:\n    print(file.read())\n\n\nSome thing\n\n\nYou :- That’s pretty easy but what is with\nStackoverflow :- It helps python programmers like you to simplify some common resource management patterns by abstracting their functionality and allowing them to be factored out and reused.\nSo in this case you don’t have to open and close file all done for you automatically.\n\n\nCode\n# A good pattern for with use case is this \nsome_lock = threading.Lock()\n\n# Harmful:\nsome_lock.acquire()\n    try:\n        # Do something complicated because you are coder and you love to do so ...\n    finally:\n        some_lock.release()\n\n# Better :\nwith some_lock:\n    # Do something awesome Because you are a Data Scientist...\n\n\nYou :- But I want to use with for my own use case how do I do it?\nStackOverflow :- Use Data Models and relax\n\n\nCode\n# Python is language full of hooks and protocol\n# Here MyContextManger abides context manager protocol\nclass MyContextManger:\n    \n    def __init__(self, name):\n        self.name=name\n# with statement automatically calls __enter__ and __exit__ methods        \n    def __enter__(self):  ## Acquire the lock do the processing in this method\n        self.f = open(self.name,\"r\")\n        return self.f\n    \n    def __exit__(self,exc_type,exc_val,exc_tb): ## release the lock and free allocated resources in this method\n        if self.f:\n            self.f.close()\n            \nwith MyContextManger(\"hello.txt\") as f:\n    print(f.read())\n\n\nSome thing\n\n\nYou :- It works but what are those parameters in exit method\nStackoverflow :- Google It !\nYou :- But writing a class in python is hectic, I want to do functional Programming\nStackOverflow :- Use Decorators\n\n\nCode\nfrom contextlib import contextmanager\n\n@contextmanager\ndef mySimpleContextManager(name):\n    try:\n        f = open(name, 'r')\n        yield f\n    finally:\n        f.close()\n        \nwith mySimpleContextManager(\"hello.txt\")  as f:\n    print(f.read())\n\n\nSome thing\n\n\nYou :- Ok, that’s what I call a pythonic code but what is yeild\nStackoverflow :- Hang On!\n\n\nIterators and Generators\nAn iterator is an object representing a stream of data; this object returns the data one element at a time. A Python iterator must support a method called next() that takes no arguments and always returns the next element of the stream. If there are no more elements in the stream, next() must raise the StopIteration exception. Iterators don’t have to be finite, though; it’s perfectly reasonable to write an iterator that produces an infinite stream of data.\nThe built-in iter() function takes an arbitrary object and tries to return an iterator that will return the object’s contents or elements, raising TypeError if the object doesn’t support iteration. Several of Python’s built-in data types support iteration, the most common being lists and dictionaries. An object is called iterable if you can get an iterator for it.\n\n\nCode\nl = [1,2,3]\nit = l.__iter__()  ## same as iter(l)\nit.__next__() ## gives 1\nnext(it) ## gives 2\nnext(it) ## gives 3\nnext(it) ## gives error StopIteration\n\n\nStopIteration: \n\n\n\n\nCode\n#lets replicate the simple range method\nclass MyRange:\n    def __init__(self,start,stop):\n        self.start = start -1\n        self.stop = stop\n    def __iter__(self):\n        return self\n    \n    def __next__(self):\n        self.start = self.start+1\n        if self.start&lt;self.stop:\n            return self.start\n        else:\n            raise StopIteration()\n                \nfor i in MyRange(2,10):\n    print(i)\n\n\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nYou :- Again a class\nStackOverflow :- OK here’s a easy way Use Generators\nThey Simplify writing Iterators, kind of iterable you can only iterate over once. Generators do not store the values in memory, they generate the values on the fly so no storage is required. So you ask one value it will generate and spit it out\n\n\nCode\ndef myRange(start,stop):\n    while True:\n        if start&lt;stop:\n            yield start\n            start = start+1\n        else:\n            return \nfor i in myRange(2,10):\n    print(i)\n\n\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nYou’re doubtless familiar with how regular function calls work in Python or C. When you call a function, it gets a private namespace where its local variables are created. When the function reaches a return statement, the local variables are destroyed and the value is returned to the caller. A later call to the same function creates a new private namespace and a fresh set of local variables. But, what if the local variables weren’t thrown away on exiting a function? What if you could later resume the function where it left off? This is what generators provide; they can be thought of as resumable functions.\nAny function containing a yield keyword is a generator function; this is detected by Python’s bytecode compiler which compiles the function specially as a result.\nWhen you call a generator function, it doesn’t return a single value; instead it returns a generator object that supports the iterator protocol. On executing the yield expression, the generator outputs the value start , similar to a return statement. The big difference between yield and a return statement is that on reaching a yield the generator’s state of execution is suspended and local variables are preserved. On the next call to the generator’s next() method, the function will resume executing.\n\n\nCode\n# generator comphresnsive\nl = ( x for x in range(20))\nl\n\n\n&lt;generator object &lt;genexpr&gt; at 0x7f6f905e0fc0&gt;\n\n\n\n\nCode\n[*l]\n\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n\n\nLet’s look in more detail at built-in functions often used with iterators.\nTwo of Python’s built-in functions, map() and filter() duplicate the features of generator expressions:\nmap(f, iterA, iterB, …) returns an iterator over the sequence\nf(iterA[0], iterB[0]), f(iterA[1], iterB[1]), f(iterA[2], iterB[2]), ....\nfilter(predicate, iter) returns an iterator over all the sequence elements that meet a certain condition, and is similarly duplicated by list comprehensions. A predicate is a function that returns the truth value of some condition; for use with filter(), the predicate must take a single value.\n\n\nCode\n# why did it returned an empty list think?\n[*map(lambda x :x **2 , l)]\n\n\n[]\n\n\n\n\nCode\n[*filter(lambda x :x%2!=0,l)]\n\n\n&lt;filter at 0x7f6f9057f9e8&gt;\n\n\nzip(iterA, iterB, …) takes one element from each iterable and returns them in a tuple:\n\n\nCode\nz = zip(['a', 'b', 'c'], (1, 2, 3))\nfor x , y in z:\n    print (x,y)\n\n\na 1\nb 2\nc 3\n\n\n\n\nFunctools and Itertools (This is going to blow your mind)\nThese two python modules are super helpful in writing Efficient Functional Code\n\n\nCode\n# reduce\nfrom functools import reduce\nl = (x for x in range(1,10))\nreduce(lambda x,y : x+y , l)\n\n\n45\n\n\nFor programs written in a functional style, you’ll sometimes want to construct variants of existing functions that have some of the parameters filled in. Consider a Python function f(a, b, c); you may wish to create a new function g(b, c) that’s equivalent to f(1, b, c); you’re filling in a value for one of f()’s parameters. This is called “partial function application”.\nThe constructor for partial() takes the arguments (function, arg1, arg2, …, kwarg1=value1, kwarg2=value2). The resulting object is callable, so you can just call it to invoke function with the filled-in arguments.\n\n\nCode\nfrom functools import partial\n\ndef log(message, subsystem):\n    \"\"\"Write the contents of 'message' to the specified subsystem.\"\"\"\n    print('%s: %s' % (subsystem, message))\n    ...\n\nserver_log = partial(log, subsystem='server')\nserver_log('Unable to open socket')\n\n\nserver: Unable to open socket\n\n\n\n\nCode\nfrom itertools import islice ,takewhile,dropwhile\n\n# here is a very simple implementation of the fibonacci sequence \ndef fib(x=0 , y=1):\n    while True:\n        yield x\n        x , y = y , x+y\n\n\n\n\nCode\nlist(islice(fib(),10))\n\n\n[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n\n\n\n\nCode\nlist(takewhile(lambda x : x &lt; 5  , islice(fib(),10)))\n\n\n[0, 1, 1, 2, 3]\n\n\n\n\nCode\nlist(dropwhile(lambda x : x &lt; 5  , islice(fib(),10)))\n\n\n[5, 8, 13, 21, 34]\n\n\n\n\nCode\nlist(dropwhile(lambda x :x&lt;11 , takewhile(lambda x : x &lt; 211  , islice(fib(),15))))\n\n\n[13, 21, 34, 55, 89, 144]\n\n\nTo read more about itertools https://docs.python.org/3.6/howto/functional.html#creating-new-iterators\n\n\nSome Python Tricks\n\n\nCode\n#normal calculator prorgramm\ndef calculator(operator , x , y):\n    if operator==\"add\":\n        return x+y\n    elif operator==\"sub\":\n        return x-y\n    elif operator == \"div\":\n        return x/y\n    elif operator==\"mul\":\n        return x*y\n    else :\n        return \"unknow\"\ncalculator(\"add\",2,3)\n\n\n5\n\n\n\n\nCode\n#Pythonic way \ncalculatorDict = {\n    \"add\":lambda x,y:x+y,\n    \"sub\":lambda x,y:x-y,\n    \"mul\":lambda x,y:x*y,\n    \"div\":lambda x,y:x/y\n}\ncalculatorDict.get(\"add\",lambda x , y:None)(2,3)\n\n\n5\n\n\n\n\nCode\n# because we are repeating x,y in all lambda so better approach\ndef calculatorCorrected(operator,x,y):\n    return {\n    \"add\":lambda :x+y,\n    \"sub\":lambda :x-y,\n    \"mul\":lambda :x*y,\n    \"div\":lambda :x/y\n}.get(operator , lambda :\"None\")()\n\ncalculatorCorrected(\"add\",2,3)\n\n\n5\n\n\n\n\nCode\n# How to merge multiple dictionaries\nx = {1:2,3:4} \ny = {3:5,6:7}\n{**x,**y}\n\n\n{1: 2, 3: 5, 6: 7}\n\n\n\n\nCode\n# how to merge multiple list you gussed it correct \na = [1,2,3]\nb=[2,3,4,5]\n[*a,*b]\n\n\n[1, 2, 3, 2, 3, 4, 5]\n\n\n\n\nCode\n# Named tuple\nfrom collections import namedtuple\nfrom sys import getsizeof\nvector = namedtuple(\"Vector\" , [\"x\",\"y\",\"z\",\"k\"])(11,12,212,343)\nvector,vector[0], vector.y # can be accessed lke list and dic\n\n\n(Vector(x=11, y=12, z=212, k=343), 11, 12)\n\n\n\n\nCode\n# how to manage a dictionary with count\nfrom collections import Counter\npubg_level3_bag = Counter()\nkill = {\"kar98\":1 , \"7.76mm\":60}\npubg_level3_bag.update(kill)\nprint(pubg_level3_bag)\nmore_kill = {\"7.76mm\":30 , \"scarl\":1 , \"5.56mm\":30}\npubg_level3_bag.update(more_kill)\nprint(pubg_level3_bag)\n\n\nCounter({'7.76mm': 60, 'kar98': 1})\nCounter({'7.76mm': 90, '5.56mm': 30, 'kar98': 1, 'scarl': 1})\n\n\n\n\nCode\n# don't remove element from the front of a list in python use instead deque\nfrom collections import deque\n\n\n\n\nCode\n# for Datastructure with locking functionality use queue module in python\n\n\n\n\nCode\n# how to check if the data structure is iterable\nfrom collections import Iterable\nisinstance([1,2,3] , Iterable)\n\n\nTrue\n\n\n\n\nRefer these To be GREAT IN PYTHON\n\nPython Doc\n\n\nReal Python\n\n\nYoutube PyData\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Optimization/optimisers_in_dl.html",
    "href": "posts/Optimization/optimisers_in_dl.html",
    "title": "Optimizers in - Deep Learning",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import make_circles\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import dataset ,dataloader\nfrom termcolor import colored\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.animation as animation\nfrom matplotlib import colors as mcolors\n\nfrom IPython.display import HTML\n\ndevice = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEED=1\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True"
  },
  {
    "objectID": "posts/Optimization/optimisers_in_dl.html#sgd",
    "href": "posts/Optimization/optimisers_in_dl.html#sgd",
    "title": "Optimizers in - Deep Learning",
    "section": "SGD",
    "text": "SGD\n\\[\ndata = data - lr* grad\n\\]\n\n\nCode\ndef SGD(vector):\n    vectors = [vector.squeeze(0).tolist()]\n    for i in range(EPOCHS):\n        z = LOSS_FUNC(vector)\n        z.backward()\n        with torch.no_grad():\n            vector.data = vector.data - LR *  vector.grad\n            vector.grad.zero_()\n            vectors.append(vector.squeeze(0).tolist())\n    return np.array(vectors)\n\n\n\n\nCode\nanim = Animation(SGD)(VECTOR)\nHTML(anim)\n\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/Optimization/optimisers_in_dl.html#sgd-with-momentum",
    "href": "posts/Optimization/optimisers_in_dl.html#sgd-with-momentum",
    "title": "Optimizers in - Deep Learning",
    "section": "SGD with Momentum",
    "text": "SGD with Momentum\nThe real momentum update looks like\n\\[\nvelocity = viscosity * velocity - lr* grad \\\\\ndata = data - velocity\n\\]\nBut In pytorch they do the other way\n\\[\nvelocity = viscosity * velocity + grad \\\\\ndata = data - lr * velocity\n\\]\n\n\nCode\ndef SGD_Momentum(vector):\n    viscosity = 0.9\n    velocity = torch.zeros_like(vector)\n    vectors = [vector.squeeze(0).tolist()]\n\n    for i in range(EPOCHS):\n        z = LOSS_FUNC(vector)\n        z.backward()\n        with torch.no_grad():\n            velocity = viscosity * velocity + vector.grad\n            vector.data = vector.data - LR *  velocity\n            vector.grad.zero_()\n            vectors.append(vector.squeeze(0).tolist())\n    return np.array(vectors)\n\n\n\n\nCode\nanim = Animation(SGD,SGD_Momentum)(VECTOR)\nHTML(anim)\n\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/Optimization/optimisers_in_dl.html#sgd-with-nestrov-momentum",
    "href": "posts/Optimization/optimisers_in_dl.html#sgd-with-nestrov-momentum",
    "title": "Optimizers in - Deep Learning",
    "section": "SGD with Nestrov Momentum",
    "text": "SGD with Nestrov Momentum\n\nNesterov momentum. Instead of evaluating gradient at the current position (red circle), we know that our momentum is about to carry us to the tip of the green arrow. With Nesterov momentum we therefore instead evaluate the gradient at this “looked-ahead” position."
  },
  {
    "objectID": "posts/Optimization/optimisers_in_dl.html#adagrad",
    "href": "posts/Optimization/optimisers_in_dl.html#adagrad",
    "title": "Optimizers in - Deep Learning",
    "section": "AdaGrad",
    "text": "AdaGrad\n\\[\ncache = cache + (grad^2) \\\\\ndata = data - \\frac {(lr * grad)}{( \\sqrt {cache} + eps)}\n\\]\nIt is a adaptive learning method where we are constantly annealing the lr.\nThe eps is a extremely small value to smoothen the denominator.\n\n\nCode\n\ndef adaGrad(vector):\n    lr=0.5\n    cache = torch.zeros_like(vector)\n    vectors = [vector.squeeze(0).tolist()]\n    eps = 1e-10 # can be any small value\n\n    for i in range(EPOCHS):\n        z = LOSS_FUNC(vector)\n        z.backward()\n        with torch.no_grad():\n\n            cache +=  (vector.grad ** 2)  \n            vector.data = vector.data - (lr *  vector.grad / (torch.sqrt(cache) + eps)) \n            vector.grad.zero_()\n            vectors.append(vector.squeeze(0).tolist())\n    \n    return np.array(vectors)\n\n\n\n\nCode\nanim = Animation(SGD,SGD_Momentum,adaGrad)(VECTOR)\nHTML(anim)\n\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/Optimization/optimisers_in_dl.html#rmsprop",
    "href": "posts/Optimization/optimisers_in_dl.html#rmsprop",
    "title": "Optimizers in - Deep Learning",
    "section": "RMSProp",
    "text": "RMSProp\nIt solved AdaGrad problem of deminishing gradient. \\[\ncache = alpha *cache + (1-alpha) * grad^2 \\\\\ndata = data - \\frac {(lr * grad)} {(\\sqrt{cache} + eps)}\n\\]\n\n\nCode\ndef RMSprop(vector):\n    lr=0.5\n    alpha=0.99\n    cache = torch.zeros_like(vector)\n    vectors = [vector.squeeze(0).tolist()]\n    eps = 1e-10 # can be any small value\n\n    for i in range(EPOCHS):\n        z = LOSS_FUNC(vector)\n        z.backward()\n        with torch.no_grad():\n\n            cache = alpha * cache + ((1-alpha) * (vector.grad ** 2))  \n            vector.data = vector.data - (lr *  vector.grad / (torch.sqrt(cache) + eps)) \n            vector.grad.zero_()\n            vectors.append(vector.squeeze(0).tolist())\n    return np.array(vectors)\n\n\n\n\nCode\nanim = Animation(RMSprop,adaGrad)(VECTOR)\nHTML(anim)\n\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/Optimization/optimisers_in_dl.html#adadelta",
    "href": "posts/Optimization/optimisers_in_dl.html#adadelta",
    "title": "Optimizers in - Deep Learning",
    "section": "AdaDelta",
    "text": "AdaDelta\nIt is all similar to RMSProp with an additional delta attribute which just eliminates the use of lr away from the update parameter.\nThe authors note that the units in this update (as well as in SGD, Momentum, or Adagrad) do not match, i.e. the update should have the same hypothetical units as the parameter. To realize this, they first define another exponentially decaying average, this time not of squared gradients but of squared parameter updates:\n\\[\ncache = alpha *cache + (1-alpha) * grad^2 \\\\\ndelta = \\frac {\\sqrt {acc\\_delta + eps}*grad}{\\sqrt{cache+eps}}\\\\\ndata = data - delta\\\\\nacc\\_delta = alpha * acc\\_delta + (1-alpha) * delta^2 \\\\\n\\]\n\n\nCode\ndef AdaDelta(vector):\n    rho = 0.9\n    lr=1.0\n    cache = torch.zeros_like(vector)\n    acc_delta = torch.zeros_like(vector)\n    vectors = [vector.squeeze(0).tolist()]\n    eps = 1e-6 # can be any small value\n\n    for i in range(EPOCHS):\n        z = LOSS_FUNC(vector)\n        z.backward()\n        with torch.no_grad():\n            cache = rho * cache + ((1-rho) * (vector.grad ** 2))  \n            delta = torch.sqrt(acc_delta+eps) *vector.grad / torch.sqrt(cache+eps)\n            vector.data = vector.data - lr * delta\n            acc_delta = acc_delta * rho + ((1-rho) * (delta**2))\n            vector.grad.zero_()\n            vectors.append(vector.squeeze(0).tolist())\n    return np.array(vectors)\n\n\n\n\nCode\nanim = Animation(adaGrad,RMSprop,AdaDelta)(VECTOR)\nHTML(anim)\n\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/Optimization/optimisers_in_dl.html#adam",
    "href": "posts/Optimization/optimisers_in_dl.html#adam",
    "title": "Optimizers in - Deep Learning",
    "section": "Adam",
    "text": "Adam\nIt is basically RMSProp with Momentum\n\\[\nm = beta1*m + (1-beta1)*dx \\\\\nmt = \\frac{m} {(1-beta1^t)} \\\\\nv = beta2*v + (1-beta2)* dx^2 \\\\\nvt = \\frac{v} {(1-beta2^t)} \\\\\nx = x - \\frac{lr * mt}{np.sqrt(vt) + eps}\n\\]\nNotice that the update looks exactly as RMSProp update, except the “smooth” version of the gradient m is used instead of the raw (and perhaps noisy) gradient vector dx. Recommended values in the paper are eps = 1e-8, beta1 = 0.9, beta2 = 0.999. In practice Adam is currently recommended as the default algorithm to use, and often works slightly better than RMSProp. However, it is often also worth trying SGD+Nesterov Momentum as an alternative. The full Adam update also includes a bias correction mechanism, which compensates for the fact that in the first few time steps the vectors m,v are both initialized and therefore biased at zero, before they fully “warm up”. With the bias correction mechanism, the update looks as follows:\n\n\nCode\nimport math\ndef Adam(vector):\n    m = torch.zeros_like(vector)\n    v = torch.zeros_like(vector)\n    beta1 = 0.9\n    beta2 = 0.99\n    eps = 1e-8\n    lr=0.2\n    vectors = [vector.squeeze(0).tolist()]\n    for i in range(1 , EPOCHS+1):\n        z = LOSS_FUNC(vector)\n        z.backward()\n        with torch.no_grad():\n            m = m * beta1 + (1-beta1)*vector.grad\n            v = v * beta2 + (1-beta2)*(vector.grad**2)\n            denom = torch.sqrt(v)+eps\n            bias_correction_m = 1 - beta1**i\n            bias_correction_v = 1 - beta2**i\n            \n            step_size = lr * math.sqrt(bias_correction_v)/bias_correction_m\n            \n            \n            vector.data = vector.data - step_size*m/denom\n            vector.grad.zero_()\n            vectors.append(vector.squeeze(0).tolist())\n    return np.array(vectors)\n\n\n\n\nCode\nanim = Animation(adaGrad,RMSprop,Adam)(VECTOR)\nHTML(anim)\n\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/NLP/linguistics.html",
    "href": "posts/NLP/linguistics.html",
    "title": "Linguistics",
    "section": "",
    "text": "Linguistics, the scientific study of language, offers us a captivating insight into the way humans communicate. With over 7,000 identified languages worldwide, including spoken and sign languages, linguistics plays a pivotal role in numerous fields."
  },
  {
    "objectID": "posts/NLP/linguistics.html#the-significance-of-linguistics",
    "href": "posts/NLP/linguistics.html#the-significance-of-linguistics",
    "title": "Linguistics",
    "section": "The Significance of Linguistics:",
    "text": "The Significance of Linguistics:\nLinguistics is not merely an academic pursuit; it has real-world applications across diverse domains:\n\nNatural Language Processing (NLP): Computational linguistics is essential for developing NLP technologies, enabling machines to understand and generate human language. From chatbots to translation services, NLP powers many of the tools we use daily.\nSpeech Recognition: The study of linguistics underpins speech recognition systems, allowing us to communicate with our devices through voice commands, thereby simplifying tasks and enhancing accessibility.\nSpeech Pathology: Linguists contribute to diagnosing and treating speech and language disorders, improving the lives of individuals with communication difficulties.\nLexicography: Lexicographers rely on linguistic research to compile dictionaries, preserving the vast lexicons of various languages and making them accessible to the public.\nCommunications: Linguistics informs effective communication strategies, whether it’s in advertising, journalism, or cross-cultural interactions. Understanding language nuances is crucial in conveying messages accurately.\nGlobal Significance: Linguistics impacts everyone, directly or indirectly, who interacts with languages. From educators teaching language skills to diplomats navigating international relations, linguistic knowledge is a valuable asset.\n\nLanguage in the Real World: Linguists study languages as they are used in the real world, emphasizing the importance of understanding language as a means of communication rather than a tool for promoting one language over another. This approach acknowledges linguistic diversity and promotes cultural understanding.\nMeta-Linguistic Awareness: This refers to the ability to consciously reflect on the nature of language, encouraging a deeper understanding of how we communicate.\nLanguage exists in 2 Levels -\n\nSounds or Hand shapes forms - for example b, n, k which don’t have any meaning in themselves.\nCombination of forms - combining different sounds to make a word for example bunny or rabbit. They create meaning in the real world.\n\nThe idea that words are made up of two level of structures is called Duality of Patterning\nArbitrariness of Language: Words are arbitrarily chosen to represent concepts. Different words or combinations of words could represent the same thing, highlighting the flexibility and complexity of language.\nSign in Linguistics: “Sign” can mean both anything that conveys meaning, such as words like “rain,” and the physical signs used in sign languages, which involve hand shapes, facial expressions, and body movements."
  },
  {
    "objectID": "posts/NLP/linguistics.html#four-crucial-language-features",
    "href": "posts/NLP/linguistics.html#four-crucial-language-features",
    "title": "Linguistics",
    "section": "Four Crucial Language Features:",
    "text": "Four Crucial Language Features:\n\nDisplacement: The ability to talk about past, future, and distant events, setting human language apart from animal communication.\nArbitrariness: The arbitrary nature of word selection, showcasing the creativity and adaptability of human communication.\nReflexivity: The capacity to use language to examine and discuss linguistic elements, contributing to language evolution i.e talking about the language using language.\nDuality of Patterning: The concept that language is structured at two levels—individual sounds or signs and their combination to create meaning."
  },
  {
    "objectID": "posts/NLP/linguistics.html#deconstructing-language",
    "href": "posts/NLP/linguistics.html#deconstructing-language",
    "title": "Linguistics",
    "section": "Deconstructing Language:",
    "text": "Deconstructing Language:\n\nPhonetics: The Symphony of Sounds\n\nPhonetics is like the conductor of a linguistic symphony. It’s the study of individual sounds in spoken languages or handshapes in sign languages. Have you ever wondered why ‘cat’ and ‘bat’ sound different even though they share most of the same letters? Phonetics explores these auditory elements and reveals how our vocal apparatus produces an array of sounds, from the rolling ‘r’ to the whispered ‘sh’.\n\nPhonology: Cracking the Sound Code\n\nOnce we’ve examined individual sounds, we move on to phonology, which investigates how languages combine these phonetic elements into words. Think of it as the code that arranges the sounds into specific patterns. This is where you uncover why ‘pin’ and ‘spin’ are different words, thanks to the ‘s’ sound at the beginning. Phonology unveils the sound structure of language, helping us understand why certain combinations of sounds are preferred or avoided in a given language.\n\nMorphology: The Building Blocks of Words\n\nMorphology is all about breaking down longer words into smaller, meaningful units. It’s like dissecting a puzzle to reveal its pieces. By doing this, we uncover the building blocks of language, such as prefixes, suffixes, and roots. Morphology is what allows us to understand that ‘unhappiness’ consists of ‘un-’ (meaning ‘not’) and ‘happiness,’ forming a negative form of the word.\n\nSyntax: The Art of Sentence Structure\n\nNow that we’ve explored the anatomy of words, it’s time to put them together. Syntax is the study of the relationships between words in sentences. It’s like constructing a grammatical blueprint. Understanding syntax is essential for deciphering sentence structure and grammar, ensuring that your sentences make sense and convey your intended message clearly.\n\nSemantics: The Meanings We Convey\n\nWords are more than just sounds and shapes; they carry meaning. Semantics is the field that delves into the meanings of words and sentences. It helps us understand the significance of language in communication. For instance, it explains why ‘hot’ means something different in the context of ‘hot coffee’ compared to ‘hot topic.’ Semantics allows us to grasp the subtle nuances that words can convey.\n\nPragmatics: Language in a Social Context\n\nLanguage doesn’t exist in a vacuum; it’s a dynamic tool used in various social contexts. Pragmatics explores how language’s meaning evolves in larger social settings. It’s like understanding the unwritten rules of conversation. It enables us to navigate the intricacies of communication within different cultural and societal contexts, helping us know when to be polite, assertive, or even humorous."
  },
  {
    "objectID": "posts/NLP/linguistics.html#types-of-linguistics",
    "href": "posts/NLP/linguistics.html#types-of-linguistics",
    "title": "Linguistics",
    "section": "Types of Linguistics",
    "text": "Types of Linguistics\n\nApplied Linguistics: Applied linguistics uses linguistic theory to solve real-world problems, primarily in the field of language education. Linguists in this field apply their knowledge of language acquisition to develop teaching materials and methods. For instance, they design language courses and tests to help students learn a second language more effectively. They may also analyze language use in specific contexts, like business or healthcare, to enhance communication.\nSociolinguistics: Sociolinguistics is a subfield closely connected to sociology, the study of groups of people. Sociolinguists examine how language operates within communities, considering how language influences the community and vice versa. For example, they may investigate how dialects and accents vary among different regions or social groups and how these differences affect communication and identity.\nPsycholinguistics: Psycholinguistics, a branch closely tied to cognitive science, explores the relationship between language and the human mind. Researchers in this field study how people acquire language, process it in the brain, and produce speech. They may investigate topics such as language development in children, language disorders, and the mental processes involved in understanding and producing language.\nComputational Linguistics: Computational linguistics uses computers to create models of language. This field involves developing algorithms and software to process and generate human language. It plays a crucial role in applications like machine translation, speech recognition, and natural language processing. For instance, computational linguists design algorithms that power virtual assistants like Siri and Alexa.\n\nDictionary makers define one entry or unit as the largest unpredictable combination of forms and meaning. They call each of the entries lexemes or lexical items because they are part of the lexicon, which is another word for dictionaries.\nFor example the word rabbithole can be broken into 2 words rabbit and hole and if we look the meaning of the 2 words but then also we can’t predict the meaning of rabbithole basically the meaning is unpredictable.\nAt the same time the word deep hole is predictable. If we look at their meaning separately we can understand what deep hole means therefore we will not find a deep hole in the dictionary.\nFalling down rabbit holes\nIn the above example fall, -ing, down, rabbit, hole, -s are all smallest unpredictable combinations of forms and meaning and hence are called morphemes and the study of it called morphology.\nThe reason we divide language into morphemes is because it helps us see patterns across languages. Dividing the language into morphemes can help us see the difference and similarities between languages and information they convey."
  },
  {
    "objectID": "posts/NLP/linguistics.html#types-of-morphemes",
    "href": "posts/NLP/linguistics.html#types-of-morphemes",
    "title": "Linguistics",
    "section": "Types of Morphemes",
    "text": "Types of Morphemes\n\nFree Morphemes: Free morphemes can stand alone as individual words and carry meaning by themselves. For example, “rabbit” and “dog” are free morphemes because they are complete words with their own meanings.\nCompound Morphemes: Compound morphemes consist of two or more free morphemes combined together to form a single word. An example is “rabbithole,” where “rabbit” and “hole” are separate words, each with its meaning, but together they create a new word with a unique meaning.\nBound Morphemes: Bound morphemes cannot stand alone as independent words. They are typically affixes added to free morphemes to modify their meaning or grammatical properties. An example is the “-s” in “rabbits,” which indicates pluralization. It cannot stand alone as a word but must attach to a free morpheme.\n\nRabbits\n\nRabbit (root)\n-s (suffix)\n\nUnrabbity\n\nRabbit (root)\nY (suffix)\nUn (prefix)\n\nBound roots are morphemes that serve as the core of many words but do not carry meaning on their own. For example, the root “ceive” in words like “receive,” “deceive,” and “conceive” is common to all these words but doesn’t convey a distinct meaning by itself.\nTypes of Affixes:\nSuffix: Affixes added at the end of a word. For example, the “-ing” in “falling.”\nPrefix: Affixes added at the beginning of a word. For instance, “un-” in “unrabbity.”\nInfix: Affixes inserted in the middle of a word. This is less common in English but exists in some languages.\nCircumfix: Affixes added both at the start and at the end of a word. Circumfixes are rare in English but can be found in other languages.\nSuppletion is a fascinating aspect of language that involves replacing a word with a completely new one, rather than following a systematic word-formation process. It’s like a linguistic magic trick where one word disappears, and another one takes its place. Let’s dive deeper into suppletion with some simple examples.\nExample 1: English Suppletion In English, one of the most well-known examples of suppletion is the transformation of the word “go” into its past form. Instead of following the typical pattern of adding a suffix like “-ed” as we do with most English verbs, “go” becomes “went.” It’s like the word “went” magically appeared out of nowhere, with no visible clues to its origin.\nExample 2: Arabic Suppletion Suppletion isn’t limited to English; it happens in other languages too. In Arabic, a fascinating example is the word formation around the root “ktb,” which is associated with writing. From this root, we get words like “kitaab” (book) and “kutub” (books). These words seem to emerge as if by linguistic alchemy, without the usual affixes or prefixes.\nUnderstanding Constituents in Sentences Now, let’s shift our focus to understanding the building blocks of sentences. These building blocks are called “constituents.” Think of constituents as the sub-groups of a sentence. They are the essential elements that come together to form a complete thought.\nMorphosyntax: The Dual Study of Language Linguists use the term “grammar” to describe the structural patterns of language, encompassing both morphology (word formation) and syntax (sentence formation). These two branches are often studied separately but are intricately connected, like different layers of the same linguistic cake. The combination of morphology and syntax is aptly called “Morphosyntax,” which is just another word for grammar.\nBreaking Down Grammar Grammar can be divided into two main components: morphology and syntax.\n\nMorphology focuses on the study of words and their rules of formation. When we discuss morphology, we talk about categories like nouns, verbs, adjectives, and more.\nSyntax, on the other hand, deals with the study of sentences and their rules of formation. In syntax, we use terms like subjects, verbs, objects, and adverbials to dissect the structure of sentences.\n\nBy calling it by the transparent term morphosyntax we are highlighting this dualism.\nWhen we talk about word-formation (morphology) we use terms like\n\nNoun\nVerb\nAdjective\nAdverb\nPronoun\nDeterminer\nPreposition\nConjunction\n\nAnd when we talk about sentence-formation (syntax) we use terms like\n\nSubject\nVerb\nObject\nComplement\nAdverbial\n\nVerbs: The Double Agents One thing to note is that the term “verb” has a dual role in language. It’s not only a word-forming element but also a key player in sentence formation. So, when using the term “verb,” be sure to clarify whether you’re referring to it as a word or as a sentence component to avoid any confusion.\nDictionaries and Words You might have noticed that dictionaries primarily focus on words and don’t delve into sentence structures. They provide definitions and information about individual words, making it clear that their primary purpose is to explain words, not how they come together to form sentences.\nThe Middle Ground: Phrases Between individual words and complete sentences, there’s a middle ground known as a “phrase.” A phrase is a constituent that’s larger than a single word but smaller than a full sentence. It’s like a linguistic puzzle piece that fits into the sentence’s structure.\nThe Power of Tree Representation To understand how sentences are formed, linguists often use tree diagrams. These visual aids help organize and visualize the structure of sentences, making complex sentence formations easier to grasp.\nUsing phonemes we combine forms and form a morpheme using morpheme we create a word the meaning of the word is given by semantics. Once we give a definition to the word we can see how it changes with time and what relationships it has with other words.\nIn bilingual translation we sometimes translate a word which can have multiple meanings but we need more context or how the language carves up in semantic space to get to the proper meaning.\nAlso defining a strict meaning to a word is difficult for example how do we define the word sandwich.\nMay be a sandwich is filling between two pieces of bread but sandwich can also be served as roll or pitas or wraps may be we can update our definition of sandwich to be like filling between two somewhat bread like pieces, then what about ice-cream sandwich.\nSo with that definition is pizza, burrito, hotdog are they sandwich also with that definition we need to give meaning to filling and somewhat bread like pieces which is now more difficult task.\nSo the problem is not with the words rather definition we are using to define the meaning of the word.\nIn real life we give abstract meaning to all the words and then use an exemplar or a prototype the most typical representation of the category and then we have some other category members which are more or less central to the exemplar.\nWhen we define the word bird we have a notion of bird as like a small soft feather flying creature. But we also categorize penguin or vulture as birds. So using the prototype theory we can escape hatch from the definition.\nThere are some other set of words which don’t have any meaning to them but are used to make the sentence fit in more grammatically. Those are called functional words for example a, an, the etc."
  },
  {
    "objectID": "posts/NLP/linguistics.html#semantic-relationships",
    "href": "posts/NLP/linguistics.html#semantic-relationships",
    "title": "Linguistics",
    "section": "Semantic relationships",
    "text": "Semantic relationships\nSemantic relationships between words can be categorized in various ways:\nAntonym: Antonyms are words with opposite meanings. For example, “hot” and “cold” are antonyms because they represent opposing temperature conditions.\nSynonym: Synonyms are words that have similar meanings. For instance, “happy” and “joyful” are synonyms because they both convey a sense of happiness.\nHyponym: A hyponym is a term used to specify a particular member of a broader category. For example, “daisy” and “rose” are hyponyms of the broader category “flower.”\nHypernym: A hypernym is a word whose meaning includes the meanings of other words. “Flower” is a hypernym of “daisy” and “rose” because it encompasses both.\nContronym: Contronyms are words with two or more contradictory or opposite meanings. For example, “sanction” can mean both to approve and to impose a penalty, depending on the context.\nLeft:\na) Left: Departed\nExample: \"He left her sobbing at the airport.\"\nb) Left: What remains\nExample: \"There is still a lot of food left in the kitchen.\"\n\nTo dust\na) To dust: To sprinkle with\nEg, I watched him dust my B'day cake in a thick layer of white sugar.\nb) To dust: To remove particles of dust\nEg, The doctor informed her about the dust allergy.\nPragmatics is used to put meaning into a context. For example when some asks can you close the window. So there can be two possibilities either the person is asking whether you have the ability to do the task or not or whether you can actually close the window and in these two cases we implicitly know we have to go close the window and the person is not interested in our physical ability of closing the window.\nAnother good example of pragmatics is Sarcasm. For example when we praise someone the person understands whether it real praise or just sarcasm.\nImplicature, as described in this Wikipedia article, is a concept used to understand implied meanings in communication."
  },
  {
    "objectID": "posts/NLP/linguistics.html#ipa",
    "href": "posts/NLP/linguistics.html#ipa",
    "title": "Linguistics",
    "section": "IPA",
    "text": "IPA\nAn IPA (International Phonetic Alphabet) chart lists all the sounds humans can make, which are used in various languages. The study of how sounds are produced is known as Articulatory Phonetics. Analyzing recorded sounds falls under Acoustic Phonetics, while the study of how people perceive speech is called Perceptual Phonetics.\nIn phonetics, “phones” refer to actual sounds, while “phonemes” are the way we perceive these sounds in the context of a particular language. When two phones represent the same sound, they are called allophones of the same phoneme.\nThe symbols we use to write these sounds are called “graphemes.” A system that represents phonemes with graphemes is referred to as an “alphabet.”"
  },
  {
    "objectID": "posts/NLP/linguistics.html#thank-you.",
    "href": "posts/NLP/linguistics.html#thank-you.",
    "title": "Linguistics",
    "section": "Thank you.",
    "text": "Thank you."
  },
  {
    "objectID": "posts/NLP/nlp_overview.html",
    "href": "posts/NLP/nlp_overview.html",
    "title": "NLP Overview",
    "section": "",
    "text": "NLP stands for Natural Language Processing, which is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and human language. NLP encompasses the development of algorithms, models, and techniques that enable computers to understand, interpret, and generate human language in a valuable way. Here are some key aspects of NLP:    \nNLP is Classified in two sub-categories\n\nNLU (Natural Language Understanding) - NLU is about teaching machines to understand what humans are saying. It includes task like Semantic Parsing, Relation Extraction, Natural Language Inference, Word Sense Disambiguation. \nNLG (Natural Language Generation) - NLG is about teaching machines to generate human-like text or speech. It includes tasks like language translation, text generation, and text summarization, where the machine generates text that sounds natural to humans.\n\nVariety of Task under NLP:\n1.  Part-of-speech tagging: identify if each word is a noun, verb, adjective, etc.)\n2.  Named entity recognition NER): identify person names, organizations, locations, medical codes, time expressions, quantities, monetary values, etc)\n3.  Question answering\n4.  Speech recognition\n5.  Text-to-speech and Speech-to-text\n6.  Topic modeling\n7.  Sentiment classification\n9.  Language modeling\n10. Translation\n11. Intent Recognition\n12. Semantic Parsing\n13. Co-reference Resolution and Dependency Parsing\n14. Summarization \n15. Chatbots etc.\n16.  Text Classification\n17.  Topic Modeling\n18. Image Captioning\n19. Optical Character Recognition\n20. Visual Question Answering\n\n\n\n\n\nFor Generative Training :- Where the model has to learn about the data and its distribution\n1. News Article:- Archives\n2. Wikipedia Article \n3. Book Corpus \n4. Crawling the Internet for webpages.\n5. Social Media - Reddit, Stackoverflow, twitter\n6. Handcrafted Datasets.\nGenerative training on an abundant set of unsupervised data helps in performing Transfer learning for a downstream task where few parameters need to be learnt from sratch and less data is also required.\nFor Determinstic Training :- Where the model learns about Decision boundary within the data.\nGeneric\n    1. Kaggle Dataset\nSentiment\n    1. Product Reviews :- Amazon, Flipkart\nEmotion:-\n    1. ISEAR\n    2. Twitter dataset\nQuestion Answering:-\n    1. SQUAD\nDifferent task has different Handcrafted data.\n\n\n\nIn vernacular context we have crisis in data especially when it comes to state specific language in India. (Ex. Bengali, Gujurati etc.) Few Sources are:- 1. News (Jagran.com, Danik bhaskar) 2. Moview reviews (Web Duniya) 3. Hindi Wikipedia 4. Book Corpus 6. IIT Bombay (English-Hindi Parallel Corpus)\n\n\n\n\nScrapy :- Simple, Extensible framework for scraping and crawling websites. Has numerous feature into it.\nBeautiful-Soup :- For Parsing Html and xml documents.\nExcel\nwikiextractor:- A tool for extracting plain text from Wikipedia dumps\n\n\n\n\n\nTagTog\nProdigy (Explosion AI)\nMechanical Turk\nPyBossa\nChakki-works Doccano\n\nWebAnno\nBrat\nLabel Studio\n\n\n\n\n\n\nFlair\nAllen NLP\nDeep Pavlov\nPytext\nNLTK\nTransformer\nSpacy\ntorchtext\nEkphrasis\nGenism\nStanza\nSpark-NLP\n\nAny NLP task has to have few important components. 1. Data Pre-processing (Basically Junk removal from text) 2. Tokenization 3. Feature Selection 4. Token Vectorization 4. Model Building 5. Training and Inference.\n\n\nData preprocessing is a crucial step in natural language processing (NLP) that involves cleaning and transforming raw text data into a format that can be effectively used for NLP tasks. Here is a list of common NLP data preprocessing techniques:\n\nTokenization: Splitting the text into individual words or tokens. Tokenization is the foundation for many NLP tasks.\nLowercasing: Converting all text to lowercase to ensure uniformity and simplify analysis by treating words in a case-insensitive manner.\nStop Word Removal: Removing common words (e.g., “and,” “the,” “in”) that don’t carry much meaning and are often filtered out to reduce noise.\nPunctuation Removal: Stripping punctuation marks from text to focus on the actual words.\nSpecial Character Removal: Removing special characters or symbols that may not be relevant to the analysis.\nWhitespace Trimming: Removing extra spaces or leading/trailing spaces.\nHTML Tag Removal: When dealing with web data, removing HTML tags that may be present in the text.\nStemming: Reducing words to their root or base form. For example, “running” and “ran” would both be stemmed to “run.”\nLemmatization: Similar to stemming but reduces words to their dictionary or lemma form, which often results in a more linguistically accurate word.\nSpell Checking: Correcting spelling errors in the text to improve the quality of the data.\nText Normalization: Ensuring consistent representations for words, like converting abbreviations to their full form (e.g., “don’t” to “do not”).\nHandling Contractions: Expanding contractions (e.g., “can’t” to “cannot”) for better analysis.\nHandling Acronyms: Expanding acronyms (e.g., “NLP” to “natural language processing”) for clarity.\nNoise Removal: Eliminating irrelevant or noisy data, such as non-textual content or metadata.\nToken Filtering: Filtering tokens based on specific criteria (e.g., length, frequency) to remove outliers or less meaningful words.\nText Chunking: Dividing text into smaller chunks or sentences for analysis.\nHandling Missing Data: Dealing with missing values or incomplete text data.\nRemoving Duplicates: Identifying and removing duplicate or near-duplicate text entries.\n\n\n\n\nTokenization is the process of breaking down a text or a sequence of characters into smaller units, typically words or subwords, which are called tokens.\nThe primary purpose of tokenization is to split text into units that can be processed more easily. These units are the basic building blocks for various NLP tasks. Here are some key points about tokenization:\nToken: A token is a single unit or word that results from the tokenization process. For example, the sentence “I love NLP” can be tokenized into three tokens: “I,” “love,” and “NLP.”\nWord Tokenization: Word tokenization involves splitting text into words. In many cases, words are separated by whitespace or punctuation. Word tokenization is a common approach for many NLP tasks.\nSubword Tokenization: Subword tokenization splits text into smaller units, which are often subword parts or characters. This approach is used in models like BERT, which can capture the meaning of subwords and handle out-of-vocabulary words effectively.\nSentence Tokenization: Sentence tokenization divides text into individual sentences. It is used to process and analyze text at the sentence level.\n\n\n\nFrom Tokens features are created 1. N-grams: Extracting multi-word phrases (n-grams) to capture more context (e.g., “natural language processing” as a bigram).\n\nEntity Recognition: Identifying and labeling entities (e.g., names of people, organizations, locations) in the text.\nPart-of-Speech Tagging: Assigning parts of speech (e.g., noun, verb, adjective) to words in the text.\nEncoding/Decoding: Converting text into numerical representations, such as one-hot encoding or word embeddings, and vice versa.\n\n\n\n\n\nBag of Words \nTF-IDF \n\nRepresentation of Text for Sequence Task\nEvery text in a sentence is represented using one hector vector based on its position in the vocabulary\n\n\nWord Embeddings\n\nBOW and TF-IDF are spare representation of Tokens. In contrast Embedding refer to dense vector representations of Tokens in a continuous vector space. These embeddings are used to represent words or other linguistic units in a way that captures semantic relationships and contextual information.\nEmbeddings are a fundamental component of many NLP applications, enabling models to understand and work with textual data in a way that captures semantic information and relationships between words. They have revolutionized the field of NLP and have significantly improved the performance of various NLP tasks.\n1. Word2Vec\n2. Glove\n3. FastText        \n4. ELMO\n\n\n\n\nRNN\nLSTM\nBI-LSTM\nGRU\nCNNs\n\n\n\n\n\nSeq-Seq\nSeq-Seq Attention\nPointer Generator Network\nTransformer\nGPT\nTransformer-XL\nBERT\nGPT-2\n\n\n\n\n\n\n\nGrounding refers to the process of connecting or mapping natural language expressions to their corresponding real-world entities or concepts. It is a fundamental aspect of NLU systems, which aim to bridge the gap between human language and the world’s knowledge and objects. Grounding allows NLU systems to understand and interpret language in a way that aligns with the physical and conceptual world.\nThere are several forms of grounding in NLU:\n\nReferential Grounding: This involves linking words or phrases in natural language to specific entities in the real world. For example, in the sentence “The Eiffel Tower is a famous landmark,” referential grounding would involve identifying “the Eiffel Tower” as a reference to the actual structure in Paris.\nSpatial Grounding: This is about understanding the spatial relationships and locations described in language. For instance, when someone says, “The cat is on the table,” spatial grounding would involve identifying the cat’s location relative to the table.\nTemporal Grounding: This involves relating language to time. For example, understanding phrases like “tomorrow,” “last week,” or “in the future” and connecting them to specific points in time.\nOntological Grounding: This is about mapping language to a structured knowledge representation or ontology. For instance, understanding that “apple” can refer to both the fruit and the technology company and distinguishing between them based on the context.\nSemantic Grounding: This relates to understanding the meaning of words and phrases in a semantic context. It involves recognizing word sense disambiguation, word sense induction, and other techniques for interpreting the meaning of words within the given context.\n\n\n\n\nNatural Language Inference, which is a fundamental task in natural language understanding (NLU). It involves determining the relationship between two given sentences: a “premise” and a “hypothesis.” The goal is to determine whether the hypothesis is entailed, contradicted, or neutral with respect to the information presented in the premise. NLI is crucial for various NLU applications, including question answering, text summarization, sentiment analysis, and more.\nTwo widely recognized datasets for NLI are SNLI (Stanford Natural Language Inference) and MultiNLI. These datasets are used for training and evaluating NLI models:\n\nSNLI (Stanford Natural Language Inference):\n\nSNLI is one of the pioneering datasets for NLI. It contains a collection of sentence pairs labeled with one of three categories: “entailment,” “contradiction,” or “neutral.”\nThe dataset consists of sentences that are manually generated and crowd-sourced annotations to provide labels.\n\nMultiNLI (The Multi-Genre Natural Language Inference Corpus):\n\nMultiNLI is an extension of SNLI and is designed to be more challenging. It contains sentence pairs from various genres, which helps NLI models generalize better across different text types and styles.\nIt offers a more diverse set of language samples, making it a valuable resource for NLI research.\n\n\nExamples of NLI tasks include:\n\nGiven the premise “The cat is sitting on the windowsill” and the hypothesis “A feline is resting near the window,” the NLI model should determine that the hypothesis entails the premise (entailment).\nFor the premise “The dog is chasing the mailman,” and the hypothesis “The mailman is chasing the dog,” the model should recognize that the hypothesis contradicts the premise (contradiction).\nIf the premise is “The sun is shining brightly,” and the hypothesis is “The weather is beautiful,” the NLI model should indicate that the hypothesis is neutral with respect to the premise (neutral).\n\nNLI is an essential benchmark for evaluating the performance of NLU models, as it assesses their ability to comprehend and reason about the relationships between sentences, which is crucial for various natural language understanding tasks.\n\n\n\nCoreference resolution is a natural language processing (NLP) task that involves determining when two or more expressions (words or phrases) in a text refer to the same entity or concept. The primary goal of co-reference resolution is to identify which words or phrases in a text are related to one another in terms of their reference to a common entity, such as a person, place, or thing. This task is crucial for understanding the structure and meaning of text, as it helps in creating coherent and cohesive interpretations of documents.\nHere’s a breakdown of coreference resolution and its importance:\n\nDefinition of Coreference Resolution:\n\nCo-reference resolution involves identifying co-referent expressions. For example, in the sentence “John said he was tired,” the pronoun “he” co-refers to “John.”\nIt can be applied to different types of expressions, including pronouns (he, she, it), definite noun phrases (the cat, this book), and indefinite noun phrases (a dog, some people).\n\nDifferent Datasets:\n\nThere are several datasets used for training and evaluating coreference resolution models. Some popular datasets include:\n\nOntoNotes: A widely used dataset that provides text documents with annotations for coreference resolution.\nCoNLL-2012: Part of the CoNLL Shared Task series, this dataset contains news articles with annotated coreferences.\nACE: The Automatic Content Extraction (ACE) program includes a dataset for entity coreference, which is widely used for evaluation.\nWeb Anaphora Corpus: This dataset includes web documents and their coreference annotations.\n\n\nImportance of Coreference Resolution:\n\nText Coherence: Coreference resolution helps in maintaining text coherence by ensuring that different mentions of the same entity are connected correctly. It makes the text easier to understand and follow.\nInformation Extraction: In information extraction tasks, such as named entity recognition and relation extraction, it is essential to know which entities are being referred to in a text to extract relevant information.\nQuestion Answering: In question-answering systems, resolving co-references is crucial to understand and answer questions correctly.\nSummarization: In text summarization, identifying coreferences is necessary to produce coherent and concise summaries.\n\nExamples:\n\nIn a news article, coreference resolution is used to determine that “President Obama” and “he” refer to the same person.\nIn a chatbot application, coreference resolution helps the bot understand that “it” in “I bought a new phone. It has a great camera” refers to the phone.\nIn an academic paper, coreference resolution helps in identifying that “the study” and “the research” refer to the same research project.\n\n\ncoreference resolution is a crucial task in NLP that plays a significant role in various applications, including machine translation, sentiment analysis, and information retrieval, as it enables a more accurate and coherent understanding of text.\n\n\n\nConstituency parsing and dependency parsing are two common techniques in natural language processing (NLP) used to analyze the grammatical structure of sentences in a language. They help in understanding the relationships between words in a sentence, which is crucial for various NLP tasks such as machine translation, information extraction, and sentiment analysis.\n\nConstituency Parsing:\n\nDefinition: Constituency parsing, also known as phrase structure parsing, involves breaking down a sentence into smaller constituents (phrases) based on a predefined grammar, typically represented using a context-free grammar (CFG). It organizes words into hierarchical structures, with phrases contained within other phrases.\nDatasets: Constituency parsing typically uses datasets like the Penn Treebank, which contains sentences annotated with parse trees in a context-free grammar format.\nImportance: Constituency parsing provides a hierarchical representation of a sentence, which is helpful for syntactic analysis and understanding the grammatical relationships between words. This is essential for tasks like text generation, grammar correction, and summarization.\nExample:\nSentence: “The quick brown fox jumps over the lazy dog.”\nParse Tree:\n(S\n  (NP (DT The) (JJ quick) (JJ brown) (NN fox))\n  (VP (VBZ jumps)\n    (PP (IN over)\n      (NP (DT the) (JJ lazy) (NN dog))))\n)\n\nDependency Parsing:\n\nDefinition: Dependency parsing focuses on capturing the grammatical relationships between words in a sentence in terms of directed links (dependencies). Each word is associated with a head word to which it is syntactically related. This results in a tree structure where words are nodes, and dependencies are edges.\nDatasets: Dependency parsing uses datasets like the Universal Dependencies Project, which contains sentences annotated with dependency trees.\nImportance: Dependency parsing is valuable for tasks like information extraction, named entity recognition, and machine translation. It provides a more direct representation of the syntactic structure and word relationships in a sentence.\nExample:\nSentence: “The quick brown fox jumps over the lazy dog.”\nDependency Tree:\njumps(ROOT-0, jumps-4)\n├── fox(nsubj-4, fox-3)\n│   ├── The(det-1, The-0)\n│   ├── quick(amod-2, quick-1)\n│   └── brown(amod-3, brown-2)\n├── dog(nmod-4, dog-7)\n│   ├── over(case-6, over-5)\n│   └── lazy(amod-7, lazy-6)\n│       └── The(det-1, The-0)\n\n\nconstituency parsing and dependency parsing are two different approaches to representing the syntactic structure of sentences. Constituency parsing provides a hierarchical, phrasal structure, while dependency parsing focuses on word-to-word relationships. The choice between them depends on the specific NLP task and the type of linguistic information needed. Both are fundamental for understanding and processing natural language text.\nCore NLP Run\n \n\n\n\nSemantic parsing is a natural language processing (NLP) task that involves mapping natural language expressions to a structured, formal representation of their meaning. This structured representation can be in the form of logical forms, knowledge graphs, or programming code, depending on the specific application. Semantic parsing is crucial in various NLP applications, and it plays a vital role in natural language understanding and human-computer interaction.\nAt the core of many semantic parsing systems are grammar rules that define how to parse natural language sentences. These rules describe the syntax of the language and the structure of sentences. For example, a basic grammar rule might specify that a “flight” query consists of a “from” location, a “to” location, and optionally, a “with” clause for additional conditions.\nFor example, find flights from new york to london for tomorrow,\nmaps to a structured representation like:\n(query\n  (from new york)\n  (to london)\n  (on tomorrow))\nDifferent Datasets: There are several datasets commonly used in semantic parsing research. Each dataset serves a specific purpose and has its own characteristics. Some notable ones include:\n\nATIS (Airline Travel Information System): This dataset is used for training and evaluating semantic parsers in the context of airline booking and travel information. It contains queries about flights, reservations, and related information.\nGeoQuery: GeoQuery focuses on querying a geographic database using natural language. It includes questions about geographical locations, distances, and related queries.\nSpider: The Spider dataset is designed for complex SQL query generation from natural language questions. It is particularly challenging because it involves understanding complex database structures and generating precise SQL queries.\nSCAN: The SCAN dataset involves language-based navigation tasks where a model must interpret natural language instructions to navigate through grid-like environments.\nWikiSQL: This dataset contains questions about relational databases, requiring systems to generate SQL queries that extract information from a given database.\n\nExamples\n\nChatbots and Virtual Assistants: Virtual assistants like Siri and Alexa rely on semantic parsing to comprehend user commands and execute actions.\nData Analysis: Semantic parsing is used in tools for data analysis, where users can describe complex queries in natural language to extract insights from large datasets.\nProgramming Assistants: Developers can use semantic parsing to write code more efficiently by describing their intent in natural language, and the system generates the corresponding code.\n\nChallenges: Semantic parsing is a challenging task for several reasons:\n\nAmbiguity: Natural language is often ambiguous, and a single sentence can have multiple valid interpretations. Semantic parsers need to disambiguate and select the correct interpretation.\n\nAmbiguous Natural Language Query: “Find flights from New York to Chicago with a stopover.”\nThe ambiguity here lies in the term “with a stopover.” It could be interpreted in different ways:\nThe user is looking for flights that have a stopover (a connecting flight) during the journey.\nThe user is looking for direct flights from New York to Chicago, and the phrase \"with a stopover\" is providing additional information about the type of flight (e.g., the user prefers flights with a stopover).\nSemantic parsing aims to disambiguate such queries and convert them into a structured representation, which can then be further used to retrieve relevant information from a database or perform an action, like generating an SQL query.\n\nVariability: People express the same ideas in different ways, making it difficult to create comprehensive training datasets that cover all possible phrasings.\nDomain-specific Knowledge: Some semantic parsing tasks require domain-specific knowledge, which may not be readily available in training data.\nCompositionality: Many queries involve compositional meaning, where the meaning of the whole is determined by the meanings of its parts. Understanding how words and phrases combine is a complex task.\n\nsemantic parsing is important in bridging the gap between natural language and structured data, but it remains a challenging problem due to the complexities and nuances of human language. Researchers continue to work on improving semantic parsing techniques to make them more accurate and versatile.\n\n\n\nThere is no need for explanation, but one thing were focus is more on Multilingual MT.\nWe came a long way from rule-based and statistical approaches to cutting-edge Neural Networks. And above problems have shaped the world of NLP."
  },
  {
    "objectID": "posts/NLP/nlp_overview.html#data-collection",
    "href": "posts/NLP/nlp_overview.html#data-collection",
    "title": "NLP Overview",
    "section": "",
    "text": "For Generative Training :- Where the model has to learn about the data and its distribution\n1. News Article:- Archives\n2. Wikipedia Article \n3. Book Corpus \n4. Crawling the Internet for webpages.\n5. Social Media - Reddit, Stackoverflow, twitter\n6. Handcrafted Datasets.\nGenerative training on an abundant set of unsupervised data helps in performing Transfer learning for a downstream task where few parameters need to be learnt from sratch and less data is also required.\nFor Determinstic Training :- Where the model learns about Decision boundary within the data.\nGeneric\n    1. Kaggle Dataset\nSentiment\n    1. Product Reviews :- Amazon, Flipkart\nEmotion:-\n    1. ISEAR\n    2. Twitter dataset\nQuestion Answering:-\n    1. SQUAD\nDifferent task has different Handcrafted data.\n\n\n\nIn vernacular context we have crisis in data especially when it comes to state specific language in India. (Ex. Bengali, Gujurati etc.) Few Sources are:- 1. News (Jagran.com, Danik bhaskar) 2. Moview reviews (Web Duniya) 3. Hindi Wikipedia 4. Book Corpus 6. IIT Bombay (English-Hindi Parallel Corpus)\n\n\n\n\nScrapy :- Simple, Extensible framework for scraping and crawling websites. Has numerous feature into it.\nBeautiful-Soup :- For Parsing Html and xml documents.\nExcel\nwikiextractor:- A tool for extracting plain text from Wikipedia dumps\n\n\n\n\n\nTagTog\nProdigy (Explosion AI)\nMechanical Turk\nPyBossa\nChakki-works Doccano\n\nWebAnno\nBrat\nLabel Studio"
  },
  {
    "objectID": "posts/NLP/nlp_overview.html#some-common-packages",
    "href": "posts/NLP/nlp_overview.html#some-common-packages",
    "title": "NLP Overview",
    "section": "",
    "text": "Flair\nAllen NLP\nDeep Pavlov\nPytext\nNLTK\nTransformer\nSpacy\ntorchtext\nEkphrasis\nGenism\nStanza\nSpark-NLP\n\nAny NLP task has to have few important components. 1. Data Pre-processing (Basically Junk removal from text) 2. Tokenization 3. Feature Selection 4. Token Vectorization 4. Model Building 5. Training and Inference.\n\n\nData preprocessing is a crucial step in natural language processing (NLP) that involves cleaning and transforming raw text data into a format that can be effectively used for NLP tasks. Here is a list of common NLP data preprocessing techniques:\n\nTokenization: Splitting the text into individual words or tokens. Tokenization is the foundation for many NLP tasks.\nLowercasing: Converting all text to lowercase to ensure uniformity and simplify analysis by treating words in a case-insensitive manner.\nStop Word Removal: Removing common words (e.g., “and,” “the,” “in”) that don’t carry much meaning and are often filtered out to reduce noise.\nPunctuation Removal: Stripping punctuation marks from text to focus on the actual words.\nSpecial Character Removal: Removing special characters or symbols that may not be relevant to the analysis.\nWhitespace Trimming: Removing extra spaces or leading/trailing spaces.\nHTML Tag Removal: When dealing with web data, removing HTML tags that may be present in the text.\nStemming: Reducing words to their root or base form. For example, “running” and “ran” would both be stemmed to “run.”\nLemmatization: Similar to stemming but reduces words to their dictionary or lemma form, which often results in a more linguistically accurate word.\nSpell Checking: Correcting spelling errors in the text to improve the quality of the data.\nText Normalization: Ensuring consistent representations for words, like converting abbreviations to their full form (e.g., “don’t” to “do not”).\nHandling Contractions: Expanding contractions (e.g., “can’t” to “cannot”) for better analysis.\nHandling Acronyms: Expanding acronyms (e.g., “NLP” to “natural language processing”) for clarity.\nNoise Removal: Eliminating irrelevant or noisy data, such as non-textual content or metadata.\nToken Filtering: Filtering tokens based on specific criteria (e.g., length, frequency) to remove outliers or less meaningful words.\nText Chunking: Dividing text into smaller chunks or sentences for analysis.\nHandling Missing Data: Dealing with missing values or incomplete text data.\nRemoving Duplicates: Identifying and removing duplicate or near-duplicate text entries.\n\n\n\n\nTokenization is the process of breaking down a text or a sequence of characters into smaller units, typically words or subwords, which are called tokens.\nThe primary purpose of tokenization is to split text into units that can be processed more easily. These units are the basic building blocks for various NLP tasks. Here are some key points about tokenization:\nToken: A token is a single unit or word that results from the tokenization process. For example, the sentence “I love NLP” can be tokenized into three tokens: “I,” “love,” and “NLP.”\nWord Tokenization: Word tokenization involves splitting text into words. In many cases, words are separated by whitespace or punctuation. Word tokenization is a common approach for many NLP tasks.\nSubword Tokenization: Subword tokenization splits text into smaller units, which are often subword parts or characters. This approach is used in models like BERT, which can capture the meaning of subwords and handle out-of-vocabulary words effectively.\nSentence Tokenization: Sentence tokenization divides text into individual sentences. It is used to process and analyze text at the sentence level.\n\n\n\nFrom Tokens features are created 1. N-grams: Extracting multi-word phrases (n-grams) to capture more context (e.g., “natural language processing” as a bigram).\n\nEntity Recognition: Identifying and labeling entities (e.g., names of people, organizations, locations) in the text.\nPart-of-Speech Tagging: Assigning parts of speech (e.g., noun, verb, adjective) to words in the text.\nEncoding/Decoding: Converting text into numerical representations, such as one-hot encoding or word embeddings, and vice versa.\n\n\n\n\n\nBag of Words \nTF-IDF \n\nRepresentation of Text for Sequence Task\nEvery text in a sentence is represented using one hector vector based on its position in the vocabulary\n\n\nWord Embeddings\n\nBOW and TF-IDF are spare representation of Tokens. In contrast Embedding refer to dense vector representations of Tokens in a continuous vector space. These embeddings are used to represent words or other linguistic units in a way that captures semantic relationships and contextual information.\nEmbeddings are a fundamental component of many NLP applications, enabling models to understand and work with textual data in a way that captures semantic information and relationships between words. They have revolutionized the field of NLP and have significantly improved the performance of various NLP tasks.\n1. Word2Vec\n2. Glove\n3. FastText        \n4. ELMO\n\n\n\n\nRNN\nLSTM\nBI-LSTM\nGRU\nCNNs\n\n\n\n\n\nSeq-Seq\nSeq-Seq Attention\nPointer Generator Network\nTransformer\nGPT\nTransformer-XL\nBERT\nGPT-2"
  },
  {
    "objectID": "posts/NLP/nlp_overview.html#important-topics-that-have-shaped-nlp",
    "href": "posts/NLP/nlp_overview.html#important-topics-that-have-shaped-nlp",
    "title": "NLP Overview",
    "section": "",
    "text": "Grounding refers to the process of connecting or mapping natural language expressions to their corresponding real-world entities or concepts. It is a fundamental aspect of NLU systems, which aim to bridge the gap between human language and the world’s knowledge and objects. Grounding allows NLU systems to understand and interpret language in a way that aligns with the physical and conceptual world.\nThere are several forms of grounding in NLU:\n\nReferential Grounding: This involves linking words or phrases in natural language to specific entities in the real world. For example, in the sentence “The Eiffel Tower is a famous landmark,” referential grounding would involve identifying “the Eiffel Tower” as a reference to the actual structure in Paris.\nSpatial Grounding: This is about understanding the spatial relationships and locations described in language. For instance, when someone says, “The cat is on the table,” spatial grounding would involve identifying the cat’s location relative to the table.\nTemporal Grounding: This involves relating language to time. For example, understanding phrases like “tomorrow,” “last week,” or “in the future” and connecting them to specific points in time.\nOntological Grounding: This is about mapping language to a structured knowledge representation or ontology. For instance, understanding that “apple” can refer to both the fruit and the technology company and distinguishing between them based on the context.\nSemantic Grounding: This relates to understanding the meaning of words and phrases in a semantic context. It involves recognizing word sense disambiguation, word sense induction, and other techniques for interpreting the meaning of words within the given context.\n\n\n\n\nNatural Language Inference, which is a fundamental task in natural language understanding (NLU). It involves determining the relationship between two given sentences: a “premise” and a “hypothesis.” The goal is to determine whether the hypothesis is entailed, contradicted, or neutral with respect to the information presented in the premise. NLI is crucial for various NLU applications, including question answering, text summarization, sentiment analysis, and more.\nTwo widely recognized datasets for NLI are SNLI (Stanford Natural Language Inference) and MultiNLI. These datasets are used for training and evaluating NLI models:\n\nSNLI (Stanford Natural Language Inference):\n\nSNLI is one of the pioneering datasets for NLI. It contains a collection of sentence pairs labeled with one of three categories: “entailment,” “contradiction,” or “neutral.”\nThe dataset consists of sentences that are manually generated and crowd-sourced annotations to provide labels.\n\nMultiNLI (The Multi-Genre Natural Language Inference Corpus):\n\nMultiNLI is an extension of SNLI and is designed to be more challenging. It contains sentence pairs from various genres, which helps NLI models generalize better across different text types and styles.\nIt offers a more diverse set of language samples, making it a valuable resource for NLI research.\n\n\nExamples of NLI tasks include:\n\nGiven the premise “The cat is sitting on the windowsill” and the hypothesis “A feline is resting near the window,” the NLI model should determine that the hypothesis entails the premise (entailment).\nFor the premise “The dog is chasing the mailman,” and the hypothesis “The mailman is chasing the dog,” the model should recognize that the hypothesis contradicts the premise (contradiction).\nIf the premise is “The sun is shining brightly,” and the hypothesis is “The weather is beautiful,” the NLI model should indicate that the hypothesis is neutral with respect to the premise (neutral).\n\nNLI is an essential benchmark for evaluating the performance of NLU models, as it assesses their ability to comprehend and reason about the relationships between sentences, which is crucial for various natural language understanding tasks.\n\n\n\nCoreference resolution is a natural language processing (NLP) task that involves determining when two or more expressions (words or phrases) in a text refer to the same entity or concept. The primary goal of co-reference resolution is to identify which words or phrases in a text are related to one another in terms of their reference to a common entity, such as a person, place, or thing. This task is crucial for understanding the structure and meaning of text, as it helps in creating coherent and cohesive interpretations of documents.\nHere’s a breakdown of coreference resolution and its importance:\n\nDefinition of Coreference Resolution:\n\nCo-reference resolution involves identifying co-referent expressions. For example, in the sentence “John said he was tired,” the pronoun “he” co-refers to “John.”\nIt can be applied to different types of expressions, including pronouns (he, she, it), definite noun phrases (the cat, this book), and indefinite noun phrases (a dog, some people).\n\nDifferent Datasets:\n\nThere are several datasets used for training and evaluating coreference resolution models. Some popular datasets include:\n\nOntoNotes: A widely used dataset that provides text documents with annotations for coreference resolution.\nCoNLL-2012: Part of the CoNLL Shared Task series, this dataset contains news articles with annotated coreferences.\nACE: The Automatic Content Extraction (ACE) program includes a dataset for entity coreference, which is widely used for evaluation.\nWeb Anaphora Corpus: This dataset includes web documents and their coreference annotations.\n\n\nImportance of Coreference Resolution:\n\nText Coherence: Coreference resolution helps in maintaining text coherence by ensuring that different mentions of the same entity are connected correctly. It makes the text easier to understand and follow.\nInformation Extraction: In information extraction tasks, such as named entity recognition and relation extraction, it is essential to know which entities are being referred to in a text to extract relevant information.\nQuestion Answering: In question-answering systems, resolving co-references is crucial to understand and answer questions correctly.\nSummarization: In text summarization, identifying coreferences is necessary to produce coherent and concise summaries.\n\nExamples:\n\nIn a news article, coreference resolution is used to determine that “President Obama” and “he” refer to the same person.\nIn a chatbot application, coreference resolution helps the bot understand that “it” in “I bought a new phone. It has a great camera” refers to the phone.\nIn an academic paper, coreference resolution helps in identifying that “the study” and “the research” refer to the same research project.\n\n\ncoreference resolution is a crucial task in NLP that plays a significant role in various applications, including machine translation, sentiment analysis, and information retrieval, as it enables a more accurate and coherent understanding of text.\n\n\n\nConstituency parsing and dependency parsing are two common techniques in natural language processing (NLP) used to analyze the grammatical structure of sentences in a language. They help in understanding the relationships between words in a sentence, which is crucial for various NLP tasks such as machine translation, information extraction, and sentiment analysis.\n\nConstituency Parsing:\n\nDefinition: Constituency parsing, also known as phrase structure parsing, involves breaking down a sentence into smaller constituents (phrases) based on a predefined grammar, typically represented using a context-free grammar (CFG). It organizes words into hierarchical structures, with phrases contained within other phrases.\nDatasets: Constituency parsing typically uses datasets like the Penn Treebank, which contains sentences annotated with parse trees in a context-free grammar format.\nImportance: Constituency parsing provides a hierarchical representation of a sentence, which is helpful for syntactic analysis and understanding the grammatical relationships between words. This is essential for tasks like text generation, grammar correction, and summarization.\nExample:\nSentence: “The quick brown fox jumps over the lazy dog.”\nParse Tree:\n(S\n  (NP (DT The) (JJ quick) (JJ brown) (NN fox))\n  (VP (VBZ jumps)\n    (PP (IN over)\n      (NP (DT the) (JJ lazy) (NN dog))))\n)\n\nDependency Parsing:\n\nDefinition: Dependency parsing focuses on capturing the grammatical relationships between words in a sentence in terms of directed links (dependencies). Each word is associated with a head word to which it is syntactically related. This results in a tree structure where words are nodes, and dependencies are edges.\nDatasets: Dependency parsing uses datasets like the Universal Dependencies Project, which contains sentences annotated with dependency trees.\nImportance: Dependency parsing is valuable for tasks like information extraction, named entity recognition, and machine translation. It provides a more direct representation of the syntactic structure and word relationships in a sentence.\nExample:\nSentence: “The quick brown fox jumps over the lazy dog.”\nDependency Tree:\njumps(ROOT-0, jumps-4)\n├── fox(nsubj-4, fox-3)\n│   ├── The(det-1, The-0)\n│   ├── quick(amod-2, quick-1)\n│   └── brown(amod-3, brown-2)\n├── dog(nmod-4, dog-7)\n│   ├── over(case-6, over-5)\n│   └── lazy(amod-7, lazy-6)\n│       └── The(det-1, The-0)\n\n\nconstituency parsing and dependency parsing are two different approaches to representing the syntactic structure of sentences. Constituency parsing provides a hierarchical, phrasal structure, while dependency parsing focuses on word-to-word relationships. The choice between them depends on the specific NLP task and the type of linguistic information needed. Both are fundamental for understanding and processing natural language text.\nCore NLP Run\n \n\n\n\nSemantic parsing is a natural language processing (NLP) task that involves mapping natural language expressions to a structured, formal representation of their meaning. This structured representation can be in the form of logical forms, knowledge graphs, or programming code, depending on the specific application. Semantic parsing is crucial in various NLP applications, and it plays a vital role in natural language understanding and human-computer interaction.\nAt the core of many semantic parsing systems are grammar rules that define how to parse natural language sentences. These rules describe the syntax of the language and the structure of sentences. For example, a basic grammar rule might specify that a “flight” query consists of a “from” location, a “to” location, and optionally, a “with” clause for additional conditions.\nFor example, find flights from new york to london for tomorrow,\nmaps to a structured representation like:\n(query\n  (from new york)\n  (to london)\n  (on tomorrow))\nDifferent Datasets: There are several datasets commonly used in semantic parsing research. Each dataset serves a specific purpose and has its own characteristics. Some notable ones include:\n\nATIS (Airline Travel Information System): This dataset is used for training and evaluating semantic parsers in the context of airline booking and travel information. It contains queries about flights, reservations, and related information.\nGeoQuery: GeoQuery focuses on querying a geographic database using natural language. It includes questions about geographical locations, distances, and related queries.\nSpider: The Spider dataset is designed for complex SQL query generation from natural language questions. It is particularly challenging because it involves understanding complex database structures and generating precise SQL queries.\nSCAN: The SCAN dataset involves language-based navigation tasks where a model must interpret natural language instructions to navigate through grid-like environments.\nWikiSQL: This dataset contains questions about relational databases, requiring systems to generate SQL queries that extract information from a given database.\n\nExamples\n\nChatbots and Virtual Assistants: Virtual assistants like Siri and Alexa rely on semantic parsing to comprehend user commands and execute actions.\nData Analysis: Semantic parsing is used in tools for data analysis, where users can describe complex queries in natural language to extract insights from large datasets.\nProgramming Assistants: Developers can use semantic parsing to write code more efficiently by describing their intent in natural language, and the system generates the corresponding code.\n\nChallenges: Semantic parsing is a challenging task for several reasons:\n\nAmbiguity: Natural language is often ambiguous, and a single sentence can have multiple valid interpretations. Semantic parsers need to disambiguate and select the correct interpretation.\n\nAmbiguous Natural Language Query: “Find flights from New York to Chicago with a stopover.”\nThe ambiguity here lies in the term “with a stopover.” It could be interpreted in different ways:\nThe user is looking for flights that have a stopover (a connecting flight) during the journey.\nThe user is looking for direct flights from New York to Chicago, and the phrase \"with a stopover\" is providing additional information about the type of flight (e.g., the user prefers flights with a stopover).\nSemantic parsing aims to disambiguate such queries and convert them into a structured representation, which can then be further used to retrieve relevant information from a database or perform an action, like generating an SQL query.\n\nVariability: People express the same ideas in different ways, making it difficult to create comprehensive training datasets that cover all possible phrasings.\nDomain-specific Knowledge: Some semantic parsing tasks require domain-specific knowledge, which may not be readily available in training data.\nCompositionality: Many queries involve compositional meaning, where the meaning of the whole is determined by the meanings of its parts. Understanding how words and phrases combine is a complex task.\n\nsemantic parsing is important in bridging the gap between natural language and structured data, but it remains a challenging problem due to the complexities and nuances of human language. Researchers continue to work on improving semantic parsing techniques to make them more accurate and versatile.\n\n\n\nThere is no need for explanation, but one thing were focus is more on Multilingual MT.\nWe came a long way from rule-based and statistical approaches to cutting-edge Neural Networks. And above problems have shaped the world of NLP."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nParallelism and Concurrency\n\n\n\n\n\n\n\npython\n\n\n\n\nWe discuss the Theoretical Foundation of Parallelism and Concurrency and some low level primitives. Discuss how it is done on Python and compare it with other programming language.\n\n\n\n\n\n\nOct 15, 2023\n\n\nAman Pandey\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nBig Data Processing\n\n\n\n\n\n\n\nbig-data\n\n\nAWS\n\n\n\n\nExploring different tools and technologies to store, integrate, analyze, process, visualize data at large scale in a batch and stream fashion both with custom tools and AWS managed services\n\n\n\n\n\n\nOct 6, 2023\n\n\nAman Pandey\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\nData and Model Management | Model Monitoring and Logging.\n\n\n\n\n\n\n\nmachine-learning\n\n\ndeep-learning\n\n\n\n\nIn this post, we discuss ways of managing different versions of Data and Model artificats. Configuration Management and Data Validation. Monitoring the model using TIG stack. Logging and Alert.\n\n\n\n\n\n\nSep 16, 2023\n\n\nAman Pandey\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nDifferent types of Attentions\n\n\n\n\n\n\n\ndeep-learning\n\n\nattention\n\n\n\n\nA dive into different types of attention from it start with Bahdanau to Flash Attention.\n\n\n\n\n\n\nJun 22, 2023\n\n\nAman Pandey\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nImprove Model’s Prediction power using Regularization\n\n\n\n\n\n\n\ndeep-learning\n\n\n\n\nDifferent ways to increase the model’s prediction power using different regularization Techniques\n\n\n\n\n\n\nMar 20, 2023\n\n\nAman Pandey\n\n\n16 min\n\n\n\n\n\n\n  \n\n\n\n\nConvolution and Common architectures\n\n\n\n\n\n\n\ndeep-learning\n\n\nconvolution\n\n\n\n\nUnderstanding different types of CNN layers and some common architectures\n\n\n\n\n\n\nNov 8, 2022\n\n\nAman Pandey\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nOptimizers in - Deep Learning\n\n\n\n\n\n\n\ndeep-learning\n\n\noptimization\n\n\n\n\n3D visualization of different optimizers.\n\n\n\n\n\n\nNov 4, 2022\n\n\nAman Pandey\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nMetrics for Machine Learning\n\n\n\n\n\n\n\nmachine-learning\n\n\ndeep-learning\n\n\nmetrics\n\n\n\n\nA deep dive into different types of Metrics for evaluating Machine Learning Models.\n\n\n\n\n\n\nMay 22, 2022\n\n\nAman Pandey\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nObject Detection\n\n\n\n\n\n\n\ndeep-learning\n\n\nvision\n\n\n\n\nObject Detection - SSD | YOLO | R-CNN | Fast R-CNN | Faster R-CNN\n\n\n\n\n\n\nAug 16, 2021\n\n\nAman Pandey\n\n\n18 min\n\n\n\n\n\n\n  \n\n\n\n\nLinguistics\n\n\n\n\n\n\n\nnlp\n\n\n\n\nDeep into linguistics, For the curious souls to know why sometime languages don’t make sense and we want computers to understand it better than us.\n\n\n\n\n\n\nJul 16, 2021\n\n\nAman Pandey\n\n\n15 min\n\n\n\n\n\n\n  \n\n\n\n\nWeight Intialization\n\n\n\n\n\n\n\ndeep-learning\n\n\n\n\nLearning different ways of weight Intialization their effects and common pitfalls.\n\n\n\n\n\n\nJun 24, 2021\n\n\nAman Pandey\n\n\n16 min\n\n\n\n\n\n\n  \n\n\n\n\nMemory and Time Profiling\n\n\n\n\n\n\n\npython\n\n\n\n\nLet’s save some time and memory, coz both are expensive.\n\n\n\n\n\n\nMay 22, 2021\n\n\nAman Pandey\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nFunctional Programming capabilites of python\n\n\n\n\n\n\n\npython\n\n\n\n\nLambda, Comphrensions, Decorators and Generators with some sweet python tips and tricks.\n\n\n\n\n\n\nNov 4, 2020\n\n\nAman Pandey\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nWord Embeddings\n\n\n\n\n\n\n\ndeep-learning\n\n\nnlp\n\n\n\n\nWord2vec, Glove, Fastext, ELMO, Character-Aware Language Model\n\n\n\n\n\n\nOct 25, 2019\n\n\nAman Pandey\n\n\n17 min\n\n\n\n\n\n\n  \n\n\n\n\nRNN Models\n\n\n\n\n\n\n\ndeep-learning\n\n\nnlp\n\n\n\n\nWe go through RNN, LSTM, GRU building intution and coding from scratch to understand forward and backward pass.\n\n\n\n\n\n\nOct 15, 2019\n\n\nAman Pandey\n\n\n10 min\n\n\n\n\n\n\n  \n\n\n\n\nNLP Overview\n\n\n\n\n\n\n\ndeep-learning\n\n\nnlp\n\n\n\n\nWhat is NLP, what problems does it solve and components within it.\n\n\n\n\n\n\nOct 8, 2019\n\n\nAman Pandey\n\n\n19 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]