{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Convolution and Common architectures\"\n",
    "author: \"Aman Pandey\"\n",
    "date: \"11/08/2022\"\n",
    "description: \"Understanding different types of CNN layers and some common architectures\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "editor: visual\n",
    "categories: [deep-learning,convolution]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why CNN and not Regular Neural Nets\n",
    "\n",
    "**1. Regular Neural Nets don’t scale well to full images**\n",
    "\n",
    "In MNIST dataset,images are only of size 28x28x1 (28 wide, 28 high, 1 color channels), so a single fully-connected neuron in a first hidden layer of a regular Neural Network would have 28x28x1 = 786 weights. This amount still seems manageable,\n",
    "\n",
    "**But what if we move to larger images.**\n",
    "\n",
    "For example, an image of more respectable size, e.g. 200x200x3, would lead to neurons that have 200x200x3 = 120,000 weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting.\n",
    "\n",
    "**2.Parameter Sharing**\n",
    "<br>\n",
    "A feature detector that is useful in one part of the image is probably useful in another part of the image.Thus CNN are good in capturing translation invariance. \n",
    "\n",
    "**Sparsity of connections**\n",
    "In each layer,each output value depends only on a small number of inputs.This makes CNN networks easy to train on smaller training datasets and is less prone to overfitting.\n",
    "\n",
    "**2.3D volumes of neurons.**\n",
    "Convolutional Neural Networks take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular, unlike a regular Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth.\n",
    "\n",
    "![](images/ffnvscnn.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution\n",
    "\n",
    "\n",
    "In purely mathematical terms, convolution is a function derived from two given functions by integration which expresses how the shape of one is modified by the other.\n",
    "\n",
    "However we are interested in understanding the actual convolution operation in the context of neural networks.\n",
    "\n",
    "**An intuitive understanding of Convolution**\n",
    "<br>\n",
    "Convolution is an operation done  to extract features from the images as these features will be used by the network to learn about a particular image.In the case of a dog image,the feature could be the shape of a nose or the shape of an eye which will help the network later to identify an image as a dog.\n",
    "\n",
    "Convolution operation is performed with the help of the following three elements:\n",
    "\n",
    "**1.Input Image-** The image to convolve on\n",
    "\n",
    "**2.Feature Detector/Kernel/Filter-** They are the bunch of numbers in a matrix form intended to extract features from an image.They can be 1dimensional ,2-dimensional or 3-dimensional\n",
    "\n",
    "**3.Feature Map/Activation Map-** The resultant of the convolution operation performed between image and feature detector gives a Feature Map.\n",
    "\n",
    "![](images/convolution.PNG)\n",
    "\n",
    "![](https://i1.wp.com/timdettmers.com/wp-content/uploads/2015/03/convolution.png)\n",
    "\n",
    "\n",
    "![](images/feature1.PNG)\n",
    "![](images/feature2.PNG)\n",
    "![](images/feature3.PNG)\n",
    "\n",
    "\n",
    "\n",
    "**Convolution Operation**\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*VVvdh-BUKFh2pwDD0kPeRA@2x.gif)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Another way to look at it**\n",
    "\n",
    "![](images/conv3d1.png)\n",
    "\n",
    "\n",
    "Let say we have an input of $6 x 6$  and a filter size $3 x 3$ \n",
    "\n",
    "**Feature map is of size   $4 x 4$**\n",
    "![](images/conv1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Convolution over Volume**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**What if our input image has more than one channel?**\n",
    "\n",
    "Let say we have an input of $6 x 6 x 3$  and a filter size $3 x 3 x 3$ \n",
    "\n",
    "**Feature map is of size   $4 x 4$**\n",
    "\n",
    "![](images/conv3d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Operation with Multiple Filters\n",
    "\n",
    "Let say we have an input of $6 x 6 x 3$  and we use two filters size $3 x 3$ \n",
    "\n",
    "**Feature map is of size   $4 x 4 x 2$**\n",
    "\n",
    "![](images/conv2.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T15:44:50.785819Z",
     "start_time": "2019-02-11T15:44:50.759888Z"
    }
   },
   "source": [
    "# **General Representation**\n",
    "<br>\n",
    "$$Input Image [n(h)*n(w)*n(c)]-Filter-[f(h)*f(w)*n(c)],n(c')--Feature Map--[(n-f+1)*(n-f+1)*n(c')]$$\n",
    "\n",
    "**$n(h)$**-height of the image\n",
    "\n",
    "**$n(w)$**-width of the image\n",
    "\n",
    "**$n(c)$**-number of channel in an image\n",
    "\n",
    "**$f(h)$**-height of the filter\n",
    "\n",
    "**$f(w)$**-width of the filter\n",
    "\n",
    "**$f(c')$**-no of the filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Convolution layer\n",
    "\n",
    "![](images/convlayer.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Strides\n",
    "\n",
    "![](images/stride.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding\n",
    "\n",
    "![](images/padding.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling\n",
    "\n",
    "![](images/pooling.PNG)\n",
    "\n",
    "**Why we do Pooling?**\n",
    "\n",
    "The idea of max pooling is:\n",
    "1. to reduce the size of representation in such a way that we carry along those features which speaks louder in the image \n",
    "2. to lower the number of parameters to be computed,speeding the computation \n",
    "3. to make some of the features that detects significant things a bit more robust.\n",
    "\n",
    "\n",
    "**Analogy that I like to draw**\n",
    "![](https://qph.fs.quoracdn.net/main-qimg-0eb448b5633a1ff511ac2956f032f816-c)\n",
    "\n",
    "\n",
    "A good analogy to draw here would be to look into the history of shapes of pyramid.\n",
    "\n",
    "The Greek pyramid is the one without max pooling whereas the Mesopotamian pyramid is with max pooling involved where we are loosing more information but making our network simpler than the other one.\n",
    "\n",
    "**But don't we end up loosing information by max pooling?**\n",
    "\n",
    "Yes we do but the question we need to ask is how much information we can afford to loose without impacting much on the model prediction. \n",
    "\n",
    "Perhaps the criteria to choose how often(after how many convolutions) and at what part of the network (at the beginning or at the mid or at the end of the network) to use max pooling depends completely on what this network is being used for.\n",
    "\n",
    "For eg: \n",
    "1. Cats vs Dogs\n",
    "2. Identify the age of a person\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T17:12:06.494028Z",
     "start_time": "2019-02-11T17:12:06.482062Z"
    }
   },
   "source": [
    "# **General Representation-Updated**\n",
    "<br>\n",
    "\n",
    "**Including Padding and Stride**\n",
    "\n",
    "$$Input Image [n(h)*n(w)*n(c)]-Filter-[f(h)*f(w)*n(c)],n(c')--Feature Map--[((n-f+2p)/s+1)*((n-f+2p)/s+1)*n(c')]$$\n",
    "\n",
    "**$n(h)$**-height of the image\n",
    "\n",
    "**$n(w)$**-width of the image\n",
    "\n",
    "**$n(c)$**-number of channel in an image\n",
    "\n",
    "**$f(h)$**-height of the filter\n",
    "\n",
    "**$f(w)$**-width of the filter\n",
    "\n",
    "**$f(c')$**-no of the filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/network.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T17:17:19.153259Z",
     "start_time": "2019-02-11T17:17:18.724558Z"
    }
   },
   "source": [
    "# Examples\n",
    "\n",
    "**Input volume:** 32x32x3\n",
    "<br>\n",
    "10 5x5 filters with stride 1, pad 2\n",
    "<br>\n",
    "Output volume size: ?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "32x32x10\n",
    "\n",
    "\n",
    "\n",
    "**Input volume:** 32x32x3\n",
    "<br>\n",
    "10 5x5 filters with stride 1, pad 2\n",
    "<br>\n",
    "\n",
    "Number of parameters in this layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 x 1 Convolution\n",
    "\n",
    "At first,the idea of using 1x1 filter seems to not make sense as  1x1 convolution is just multiplying  by numbers.We will not be learning any feature here.\n",
    "\n",
    "But wait... What if we have a layer with dimension 32x32x196,here 196 is the number of channels and we want to do convolution\n",
    "\n",
    "So 1x1x192 convolution will do the work of dimensionality reduction by looking at each of the 196 different positions  and it will do the element wise product and give out one number.Using multiple such filters say 32 will give 32 variations of this number.\n",
    "\n",
    "![](images/1x1.gif)\n",
    "**Why do we use 1x1 filter**\n",
    "\n",
    "1. 1x 1 filter can help in shrinking the number of channels or increasing the number of channels without changing the height and width of the layer.\n",
    "\n",
    "2. It adds nonlinearity in the network which is useful in some of the architectures like Inception network.\n",
    "\n",
    "\n",
    "#![](https://cdn-images-1.medium.com/max/1600/1*KdLQiGlPWSxYJ-dvYM3tyQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Receptive Field\n",
    "\n",
    "The receptive field is defined as the region in the input space that a particular CNN’s feature is looking at (i.e. be affected by). A receptive field of a feature can be described by its center location and its size.\n",
    "\n",
    "![](images/receptive_field.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to remember\n",
    "\n",
    "1. **Filter** will always have the same number of channel as the image.\n",
    "\n",
    "2. **Convolving** gives a **2 D feature map** although our image and kernel used are of 3 dimensional\n",
    "\n",
    "3. **Padding** -Preserves the feature size\n",
    "\n",
    "3. **Pooling Operation**- Reduces the filter size by half\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is ImageNet\n",
    "\n",
    "[ImageNet](http://www.image-net.org/)\n",
    "\n",
    "ImageNet is formally a project aimed at (manually) labeling and categorizing images into almost 22,000 separate object categories for the purpose of computer vision research.\n",
    "\n",
    "However, when we hear the term “ImageNet” in the context of deep learning and Convolutional Neural Networks, we are likely referring to the ImageNet Large Scale Visual Recognition Challenge, or ILSVRC for short.\n",
    "\n",
    "The ImageNet project runs an annual software contest, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), where software programs compete to correctly classify and detect objects and scenes.\n",
    "\n",
    "The goal of this image classification challenge is to train a model that can correctly classify an input image into 1,000 separate object categories.\n",
    "\n",
    "Models are trained on ~1.2 million training images with another 50,000 images for validation and 100,000 images for testing.\n",
    "\n",
    "These 1,000 image categories represent object classes that we encounter in our day-to-day lives, such as species of dogs, cats, various household objects, vehicle types, and much more. You can find the full list of object categories in the ILSVRC challenge\n",
    "\n",
    "When it comes to image classification, the **ImageNet** challenge is the de facto benchmark for computer vision classification algorithms — and the leaderboard for this challenge has been dominated by Convolutional Neural Networks and deep learning techniques since 2012.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LeNet-5(1998)\n",
    "\n",
    "[Gradient Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)\n",
    "\n",
    "1. A pioneering 7-level convolutional network by LeCun  that classifies digits,\n",
    "2. Found its application by several banks to recognise hand-written numbers on checks (cheques) \n",
    "3. These numbers were digitized in 32x32 pixel greyscale which acted as an input images. \n",
    "4. The ability to process higher resolution images requires larger and more convolutional layers, so this technique is constrained by the availability of computing resources.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/2000/1*1TI1aGBZ4dybR6__DI9dzA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet(2012)\n",
    "\n",
    "[ImageNet Classification with Deep Convolutional Networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
    "\n",
    "1. One of the most influential publications in the field by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton that started the revolution of CNN in Computer Vision.This was the first time a model performed so well on a historically difficult ImageNet dataset.\n",
    "2. The network consisted 11x11, 5x5,3x3, convolutions and made up of 5 conv layers, max-pooling layers, dropout layers, and 3 fully connected layers.\n",
    "3. Used ReLU for the nonlinearity functions (Found to decrease training time as ReLUs are several times faster than the conventional tanh function) and used  SGD with momentum for training.\n",
    "4. Used data augmentation techniques that consisted of image translations, horizontal reflections, and patch extractions.\n",
    "5. Implemented dropout layers in order to combat the problem of overfitting to the training data.\n",
    "6. Trained the model using batch stochastic gradient descent, with specific values for momentum and weight decay.\n",
    "7. AlexNet was trained for 6 days simultaneously on two Nvidia Geforce GTX 580 GPUs which is the reason for why their network is split into two pipelines.\n",
    "8.  AlexNet significantly outperformed all the prior competitors and won the challenge by reducing the top-5 error from 26% to 15.3%\n",
    "![](images/alexnet.PNG)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZFNet(2013)\n",
    "\n",
    "[Visualizing and Understanding Convolutional Neural Networks](https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf)\n",
    "<br>\n",
    "This architecture was more of a fine tuning to the previous AlexNet structure by tweaking the hyper-parameters of AlexNet while maintaining the same structure but still developed some very keys ideas about improving performance.Few minor modifications done were the following:\n",
    "1. AlexNet trained on 15 million images, while ZF Net trained on only 1.3 million images.\n",
    "2. Instead of using 11x11 sized filters in the first layer (which is what AlexNet implemented), ZF Net used filters of size 7x7 and a decreased stride value. The reasoning behind this modification is that a smaller filter size in the first conv layer helps retain a lot of original pixel information in the input volume. A filtering of size 11x11 proved to be skipping a lot of relevant information, especially as this is the first conv layer.\n",
    "3. As the network grows, we also see a rise in the number of filters used.\n",
    "4. Used ReLUs for their activation functions, cross-entropy loss for the error function, and trained using batch stochastic gradient descent.\n",
    "5. Trained on a GTX 580 GPU for twelve days.\n",
    "6. Developed a visualization technique named **Deconvolutional Network**, which helps to examine different feature activations and their relation to the input space. Called **deconvnet** because it maps features to pixels (the opposite of what a convolutional layer does).\n",
    "7. It achieved a top-5 error rate of 14.8%\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*bFjBVvUL2Po_p2mKzC4iYQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VggNet(2014)\n",
    "\n",
    "[VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION](https://arxiv.org/pdf/1409.1556v6.pdf)\n",
    "\n",
    "This architecture is well konwn for **Simplicity and depth**.. VGGNet is very appealing because of its very uniform architecture.They proposed 6 different variations of VggNet however 16 layer with all 3x3 convolution produced the best result.\n",
    "\n",
    "Few things to note:\n",
    "1. The use of only 3x3 sized filters is quite different from AlexNet’s 11x11 filters in the first layer and ZF Net’s 7x7 filters. The authors’ reasoning is that the combination of two 3x3 conv layers has an effective receptive field of 5x5. This in turn simulates a larger filter while keeping the benefits of smaller filter sizes. One of the benefits is a decrease in the number of parameters. Also, with two conv layers, we’re able to use two ReLU layers instead of one.\n",
    "2. 3 conv layers back to back have an effective receptive field of 7x7.\n",
    "3. As the spatial size of the input volumes at each layer decrease (result of the conv and pool layers), the depth of the volumes increase due to the increased number of filters as you go down the network.\n",
    "4. Interesting to notice that the number of filters doubles after each maxpool layer. This reinforces the idea of shrinking spatial dimensions, but growing depth.\n",
    "5. Worked well on both image classification and localization tasks. The authors used a form of localization as regression (see page 10 of the paper for all details).\n",
    "6. Built model with the Caffe toolbox.\n",
    "7. Used scale jittering as one data augmentation technique during training.\n",
    "8. Used ReLU layers after each conv layer and trained with batch gradient descent.\n",
    "9. Trained on 4 Nvidia Titan Black GPUs for two to three weeks.\n",
    "10. It achieved a top-5 error rate of 7.3% \n",
    "\n",
    "![](https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png)\n",
    "\n",
    "\n",
    "![](images/standardconvnet.PNG)\n",
    "**In Standard ConvNet, input image goes through multiple convolution and obtain high-level features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Inception V1 ,the author proposed a number of upgrades which increased the accuracy and reduced the computational complexity.This lead to many new upgrades resulting in different versions of Inception Network :\n",
    "1. Inception v2\n",
    "2. Inception V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception Network (GoogleNet)(2014)\n",
    "[Going Deeper with Convolutions](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)\n",
    "\n",
    "Prior to this, most popular CNNs just stacked convolution layers deeper and deeper, hoping to get better performance,however **Inception Network** was one of the first CNN architectures that really strayed from the general approach of simply stacking conv and pooling layers on top of each other in a sequential structure and came up with the **Inception Module**.The Inception network  was complex. It used a lot of tricks to push performance; both in terms of speed and accuracy. Its constant evolution lead to the creation of several versions of the network. The popular versions are as follows:\n",
    "\n",
    "1. Inception v1.\n",
    "2. Inception v2 and Inception v3.\n",
    "3. Inception v4 and Inception-ResNet.\n",
    "<br>\n",
    "\n",
    "Each version is an iterative improvement over the previous one.Let us go ahead and explore them one by one\n",
    "![](images/inception.PNG)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception V1\n",
    "[Inception v1](https://arxiv.org/pdf/1409.4842v1.pdf)\n",
    "\n",
    "![](images/dog.PNG)\n",
    "**Problems this network tried to solve:**\n",
    "1. **What is the right kernel size for convolution**\n",
    "<br>\n",
    "A larger kernel is preferred for information that is distributed more globally, and a smaller kernel is preferred for information that is distributed more locally.\n",
    "<br>\n",
    "**Ans-** Filters with multiple sizes.The network essentially would get a bit “wider” rather than “deeper”\n",
    "<br>\n",
    "<br>\n",
    "3. **How to stack convolution which can be less computationally expensive**\n",
    "<BR>\n",
    "Stacking them naively computationally expensive.\n",
    "<br>\n",
    "**Ans-**Limit the number of input channels by adding an extra 1x1 convolution before the 3x3 and 5x5 convolutions\n",
    "<br>\n",
    "<br>\n",
    "2. **How to avoid overfitting in a very deep network**\n",
    "<br>\n",
    "Very deep networks are prone to overfitting. It also hard to pass gradient updates through the entire network.\n",
    "<br>\n",
    "**Ans-**Introduce two auxiliary classifiers (The purple boxes in the image). They essentially applied softmax to the outputs of two of the inception modules, and computed an auxiliary loss over the same labels. The total loss function is a weighted sum of the auxiliary loss and the real loss.\n",
    "\n",
    "The total loss used by the inception net during training.\n",
    "<br>\n",
    "            **total_loss = real_loss + 0.3 * aux_loss_1 + 0.3 * aux_loss_2**\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "![](images/inception_module.PNG)\n",
    "\n",
    "**Points to note**\n",
    "\n",
    "1. Used 9 Inception modules in the whole architecture, with over 100 layers in total! Now that is deep…\n",
    "2. No use of fully connected layers! They use an average pool instead, to go from a 7x7x1024 volume to a 1x1x1024 volume. This saves a huge number of parameters.\n",
    "3. Uses 12x fewer parameters than AlexNet.\n",
    "4. Trained on “a few high-end GPUs within a week”.\n",
    "5. It achieved a top-5 error rate of 6.67% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception V2\n",
    "[Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/pdf/1512.00567v3.pdf)\n",
    "\n",
    "\n",
    "Upgrades were targeted towards:\n",
    "1. Reducing representational bottleneck by replacing 5x5 convolution to two 3x3 convolution operations which further improves computational speed\n",
    "<br>\n",
    "The intuition was that, neural networks perform better when convolutions didn’t alter the dimensions of the input drastically. Reducing the dimensions too much may cause loss of information, known as a **“representational bottleneck”**\n",
    "<br>\n",
    "![](images/inceptionv2.PNG)\n",
    "2. Using smart factorization method where they factorize convolutions of filter size nxn to a combination of 1xn and nx1 convolutions.\n",
    "<br>\n",
    "For example, a 3x3 convolution is equivalent to first performing a 1x3 convolution, and then performing a 3x1 convolution on its output. They found this method to be 33% more cheaper than the single 3x3 convolution.\n",
    "![](images/inceptionv2_1.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet(2015)\n",
    "[Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "![](images/resnet_concept.PNG)\n",
    "**In ResNet, identity mapping is proposed to promote the gradient propagation. Element-wise addition is used. It can be viewed as algorithms with a state passed from one ResNet module to another one.**\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/720/1*ByrVJspW-TefwlH7OLxNkg.png)\n",
    "![](https://cdn-images-1.medium.com/max/720/1*2ns4ota94je5gSVjrpFq3A.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-Wide\n",
    "![](https://cdn-images-1.medium.com/max/960/1*7JzJ1RGh1Y4VoG1M4dseTw.png)\n",
    "left: a building block of [2], right: a building block of ResNeXt with cardinality = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet(2017)\n",
    "\n",
    "[Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993v3.pdf)\n",
    "<br>\n",
    "It is a logical extension to ResNet.\n",
    "\n",
    "**From the paper:**\n",
    "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion.\n",
    "\n",
    "**DenseNet Architecture**\n",
    "![](images/densenet_arc.PNG)\n",
    "\n",
    "Let us explore different componenets of the network\n",
    "<br>\n",
    "<br>\n",
    "**1. Dense Block**\n",
    "<br>\n",
    "Feature map sizes are the same within the dense block so that they can be concatenated together easily.\n",
    "![](https://cdn-images-1.medium.com/max/960/1*9ysRPSExk0KvXR0AhNnlAA.gif)\n",
    "\n",
    "**In DenseNet, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. Concatenation is used. Each layer is receiving a “collective knowledge” from all preceding layers.**\n",
    "![](images/densenet_concept.PNG)\n",
    "\n",
    "Since each layer receives feature maps from all preceding layers, network can be thinner and compact, i.e. number of channels can be fewer. The growth rate k is the additional number of channels for each layer.\n",
    "\n",
    "The paper proposed different ways to implement DenseNet with/without B/C by adding some variations in the Dense block to further reduce the complexity,size and to bring more compression in the architecture.\n",
    "\n",
    "    1. Dense Block (DenseNet)\n",
    "        -  Batch Norm (BN)\n",
    "        -  ReLU\n",
    "        -  3×3 Convolution \n",
    "    2. Dense Block(DenseNet B)\n",
    "        -  Batch Norm (BN)\n",
    "        -  ReLU\n",
    "        -  1×1 Convolution\n",
    "        -  Batch Norm (BN)\n",
    "        -  ReLU\n",
    "        -  3×3 Convolution\n",
    "    3. Dense Block(DenseNet C)\n",
    "        -  If a dense block contains m feature-maps, The transition layer generate $\\theta $ output feature maps, where                    $\\theta \\leq \\theata \\leq$ is referred to as the compression factor.\n",
    "        -  $\\theta$=0.5 was used in the experiemnt which reduced the number of feature maps by 50%.\n",
    "    \n",
    "    4. Dense Block(DenseNet BC)\n",
    "    -  Combination of Densenet B and Densenet C\n",
    "<br>\n",
    "**2. Trasition Layer**\n",
    "<br>\n",
    "The layers between two adjacent blocks are referred to as transition layers  where the following operations are done to change feature-map sizes:\n",
    "     -  1×1 Convolution\n",
    "     -  2×2 Average pooling \n",
    "\n",
    "\n",
    "**Points to Note:**\n",
    "1. it requires fewer parameters than traditional convolutional networks\n",
    "2. Traditional convolutional networks with L layers have L connections — one between each layer and its subsequent layer — our network has L(L+1)/ 2 direct connections.\n",
    "3. Improved flow of information and gradients throughout the network, which makes them easy to train\n",
    "4. They alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters.\n",
    "5. Concatenating feature maps instead of summing learned by different layers increases variation in the input of subsequent layers and improves efficiency. This constitutes a major difference between DenseNets and ResNets.\n",
    "6. It achieved a top-5 error rate of 6.66% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T21:49:38.821922Z",
     "start_time": "2019-02-22T21:49:38.816881Z"
    }
   },
   "source": [
    "# MobileNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Seperable Convolution\n",
    "![](images/separable_convolution.gif)\n",
    "\n",
    "**Divides a kernel into two, smaller kernels**\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1440/1*mL53fW0tJpNWEePp54y1Sg.png)\n",
    "\n",
    "**Instead of doing one convolution with 9 multiplications(parameters), we do two convolutions with 3 multiplications(parameters) each (6 in total) to achieve the same effect**\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1680/1*o3mKhG3nHS-1dWa_plCeFw.png)\n",
    "\n",
    "\n",
    "**With less multiplications, computational complexity goes down, and the network is able to run faster.**\n",
    "\n",
    "This was used in an architecture called [Effnet](https://arxiv.org/pdf/1801.06434v1.pdf) showing promising results.\n",
    "\n",
    "The main issue with the spatial separable convolution is that not all kernels can be “separated” into two, smaller kernels. This becomes particularly bothersome during training, since of all the possible kernels the network could have adopted, it can only end up using one of the tiny portion that can be separated into two smaller kernels.\n",
    "\n",
    "\n",
    "\n",
    "## Depthwise Convolution\n",
    "![](images/depthwise.gif)\n",
    "\n",
    "Say we need to increase the number of channels from 16 to 32 using 3x3 kernel.\n",
    "<br>\n",
    "\n",
    "**Normal Convolution**\n",
    "<br>\n",
    "Total No of Parameters = 3 x 3 x 16 x 32 = 4608\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*VvBTMkVRus6bWOqrK1SlLQ.png)\n",
    "\n",
    "**Depthwise Convolution**\n",
    "\n",
    "1. DepthWise Convolution = 16 x [3 x 3 x 1]\n",
    "2. PointWise Convolution = 32 x [1 x 1 x 16]\n",
    "\n",
    "Total Number of Parameters = 656\n",
    "\n",
    "\n",
    "**Mobile net uses depthwise seperable convolution to reduce the number of parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "\n",
    "[Convolution](https://indoml.com/2018/03/07/student-notes-convolutional-neural-networks-cnn-introduction/)\n",
    "<br>\n",
    "[Max Pool](https://wiseodd.github.io/techblog/2016/07/18/convnet-maxpool-layer/)\n",
    "<br>\n",
    "[Standford Slides](http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture05.pdf)\n",
    "<br>\n",
    "[Standford Blog](http://cs231n.github.io/convolutional-networks/)\n",
    "<br>\n",
    "[An intuitive guide to Convolutional Neural Networks](https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050)\n",
    "<br>\n",
    "[Convolutional Neural Networks](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2)\n",
    "<br>\n",
    "[Understanding of Convolutional Neural Network](https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148)\n",
    "<br>\n",
    "[Receptive Feild Calculation](https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807)\n",
    "<br>\n",
    "[Understanding Convolution in Deep Learning](http://timdettmers.com/2015/03/26/convolution-deep-learning/)\n",
    "<br>\n",
    "[Visualize Image Kernel](http://setosa.io/ev/image-kernels/)\n",
    "<br>\n",
    "[Visualizing and Understanding Convolution Networks](https://arxiv.org/abs/1311.2901)\n",
    "<br>\n",
    "[Standford CS231n Lecture Notes](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf)\n",
    "<br>\n",
    "[The 9 Deep Learning Papers You Need To Know About](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)\n",
    "<br>\n",
    "[CNN Architectures](https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5)\n",
    "<br>\n",
    "[Lets Keep It Simple](https://arxiv.org/pdf/1608.06037.pdf)\n",
    "<br>\n",
    "[CNN Architectures Keras](https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/)\n",
    "<br>\n",
    "[Inception Versions](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)\n",
    "<br>\n",
    "[DenseNet Review](https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803)\n",
    "<br>\n",
    "[DenseNet](https://towardsdatascience.com/densenet-2810936aeebb)\n",
    "<br>\n",
    "[ResNet](http://teleported.in/posts/decoding-resnet-architecture/)\n",
    "<br>\n",
    "[ResNet Versions](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035)\n",
    "<br>\n",
    "[Depthwise Convolution](https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
