{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"NLP Overview\"\n",
    "author: \"Aman Pandey\"\n",
    "date: \"10/08/2019\"\n",
    "description: \"What is NLP, what problems does it solve and components within it.\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "editor: visual\n",
    "categories: [deep-learning,nlp]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is NLP?\n",
    "    \n",
    "    NLP stands for Natural Language Processing, which is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and human language. NLP encompasses the development of algorithms, models, and techniques that enable computers to understand, interpret, and generate human language in a valuable way. Here are some key aspects of NLP:    \n",
    "\n",
    "NLP is Classified in two sub-categories\n",
    "\n",
    "1. `NLU` (Natural Language Understanding) - NLU is about teaching machines to understand what humans are saying. It includes task like Semantic Parsing, Relation Extraction, Natural Language Inference, Word Sense Disambiguation.\n",
    "</br></br>\n",
    "2. `NLG` (Natural Language Generation) -  NLG is about teaching machines to generate human-like text or speech. It includes tasks like language translation, text generation, and text summarization, where the machine generates text that sounds natural to humans.    \n",
    "\n",
    "Variety of Task under NLP:\n",
    "\n",
    "    1.  Part-of-speech tagging: identify if each word is a noun, verb, adjective, etc.)\n",
    "    2.  Named entity recognition NER): identify person names, organizations, locations, medical codes, time expressions, quantities, monetary values, etc)\n",
    "    3.  Question answering\n",
    "    4.  Speech recognition\n",
    "    5.  Text-to-speech and Speech-to-text\n",
    "    6.  Topic modeling\n",
    "    7.  Sentiment classification\n",
    "    9.  Language modeling\n",
    "    10. Translation\n",
    "    11. Intent Recognition\n",
    "    12. Semantic Parsing\n",
    "    13. Co-reference Resolution and Dependency Parsing\n",
    "    14. Summarization \n",
    "    15. Chatbots etc.\n",
    "    16.  Text Classification\n",
    "    17.  Topic Modeling\n",
    "    18. Image Captioning\n",
    "    19. Optical Character Recognition\n",
    "    20. Visual Question Answering\n",
    "    \n",
    "![](images/1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "### Sources\n",
    "\n",
    "For `Generative Training` :- Where the model has to learn about the data and its distribution \n",
    "\n",
    "    1. News Article:- Archives\n",
    "    2. Wikipedia Article \n",
    "    3. Book Corpus \n",
    "    4. Crawling the Internet for webpages.\n",
    "    5. Social Media - Reddit, Stackoverflow, twitter\n",
    "    6. Handcrafted Datasets.\n",
    "\n",
    "Generative training on an abundant set of unsupervised data helps in performing Transfer learning for a downstream task where few parameters need to be learnt from sratch and less data is also required.\n",
    "\n",
    "For `Determinstic Training` :- Where the model learns about Decision boundary within the data.\n",
    "\n",
    "    Generic\n",
    "        1. Kaggle Dataset\n",
    "    Sentiment\n",
    "        1. Product Reviews :- Amazon, Flipkart\n",
    "    Emotion:-\n",
    "        1. ISEAR\n",
    "        2. Twitter dataset\n",
    "    Question Answering:-\n",
    "        1. SQUAD\n",
    "    Different task has different Handcrafted data.\n",
    "    \n",
    "    \n",
    "### For Vernacular text\n",
    "In vernacular context we have crisis in data especially when it comes to state specific language in India. (Ex. Bengali, Gujurati etc.) \n",
    "Few Sources are:-\n",
    "1. News (Jagran.com, Danik bhaskar)\n",
    "2. Moview reviews (Web Duniya)\n",
    "3. Hindi Wikipedia\n",
    "4. Book Corpus\n",
    "6. IIT Bombay (English-Hindi Parallel Corpus)\n",
    "\n",
    "### Tools\n",
    "1. Scrapy :- Simple, Extensible framework for scraping and crawling websites. Has numerous feature into it.\n",
    "2. Beautiful-Soup :- For Parsing Html and xml documents. \n",
    "3. Excel \n",
    "4. wikiextractor:- A tool for extracting plain text from Wikipedia dumps\n",
    "\n",
    "### Data Annotation Tool\n",
    "\n",
    "1. TagTog\n",
    "2. Prodigy (Explosion AI)\n",
    "3. Mechanical Turk \n",
    "4. PyBossa\n",
    "5. Chakki-works Doccano   \n",
    "6. WebAnno\n",
    "7. Brat\n",
    "8. Label Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Common Packages\n",
    "\n",
    "1. [Flair](https://github.com/zalandoresearch/flair)\n",
    "2. [Allen NLP](https://github.com/allenai/allennlp)\n",
    "3. [Deep Pavlov](https://github.com/deepmipt/deeppavlov)\n",
    "4. [Pytext](https://github.com/facebookresearch/PyText)\n",
    "5. [NLTK](https://www.nltk.org/)\n",
    "6. [Transformer](https://github.com/huggingface/transformers)\n",
    "7. [Spacy](https://spacy.io/)\n",
    "8. [torchtext](https://torchtext.readthedocs.io/en/latest/)\n",
    "9. [Ekphrasis](https://github.com/cbaziotis/ekphrasis)\n",
    "10. [Genism](https://radimrehurek.com/gensim/)\n",
    "11. [Stanza](https://github.com/stanfordnlp/stanza)\n",
    "12. [Spark-NLP](https://github.com/JohnSnowLabs/spark-nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any NLP task has to have few important components.\n",
    "1. Data Pre-processing (Basically Junk removal from text)\n",
    "2. Tokenization\n",
    "3. Feature Selection\n",
    "4. Token Vectorization\n",
    "4. Model Building\n",
    "5. Training and Inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing \n",
    "\n",
    "Data preprocessing is a crucial step in natural language processing (NLP) that involves cleaning and transforming raw text data into a format that can be effectively used for NLP tasks. Here is a list of common NLP data preprocessing techniques:\n",
    "\n",
    "1. **Tokenization:** Splitting the text into individual words or tokens. Tokenization is the foundation for many NLP tasks.\n",
    "\n",
    "2. **Lowercasing:** Converting all text to lowercase to ensure uniformity and simplify analysis by treating words in a case-insensitive manner.\n",
    "\n",
    "3. **Stop Word Removal:** Removing common words (e.g., \"and,\" \"the,\" \"in\") that don't carry much meaning and are often filtered out to reduce noise.\n",
    "\n",
    "4. **Punctuation Removal:** Stripping punctuation marks from text to focus on the actual words.\n",
    "\n",
    "5. **Special Character Removal:** Removing special characters or symbols that may not be relevant to the analysis.\n",
    "\n",
    "6. **Whitespace Trimming:** Removing extra spaces or leading/trailing spaces.\n",
    "\n",
    "7. **HTML Tag Removal:** When dealing with web data, removing HTML tags that may be present in the text.\n",
    "\n",
    "8. **Stemming:** Reducing words to their root or base form. For example, \"running\" and \"ran\" would both be stemmed to \"run.\"\n",
    "\n",
    "9. **Lemmatization:** Similar to stemming but reduces words to their dictionary or lemma form, which often results in a more linguistically accurate word.\n",
    "\n",
    "10. **Spell Checking:** Correcting spelling errors in the text to improve the quality of the data.\n",
    "\n",
    "11. **Text Normalization:** Ensuring consistent representations for words, like converting abbreviations to their full form (e.g., \"don't\" to \"do not\").\n",
    "\n",
    "12. **Handling Contractions:** Expanding contractions (e.g., \"can't\" to \"cannot\") for better analysis.\n",
    "\n",
    "13. **Handling Acronyms:** Expanding acronyms (e.g., \"NLP\" to \"natural language processing\") for clarity.\n",
    "\n",
    "14. **Noise Removal:** Eliminating irrelevant or noisy data, such as non-textual content or metadata.\n",
    "\n",
    "15. **Token Filtering:** Filtering tokens based on specific criteria (e.g., length, frequency) to remove outliers or less meaningful words.\n",
    "\n",
    "16. **Text Chunking:** Dividing text into smaller chunks or sentences for analysis.\n",
    "\n",
    "17. **Handling Missing Data:** Dealing with missing values or incomplete text data.\n",
    "\n",
    "18. **Removing Duplicates:** Identifying and removing duplicate or near-duplicate text entries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down a text or a sequence of characters into smaller units, typically words or subwords, which are called tokens. \n",
    "\n",
    "The primary purpose of tokenization is to split text into units that can be processed more easily. These units are the basic building blocks for various NLP tasks. Here are some key points about tokenization:\n",
    "\n",
    "`Token`: A token is a single unit or word that results from the tokenization process. For example, the sentence \"I love NLP\" can be tokenized into three tokens: \"I,\" \"love,\" and \"NLP.\"\n",
    "\n",
    "`Word Tokenization`: Word tokenization involves splitting text into words. In many cases, words are separated by whitespace or punctuation. Word tokenization is a common approach for many NLP tasks.\n",
    "\n",
    "`Subword Tokenization`: Subword tokenization splits text into smaller units, which are often subword parts or characters. This approach is used in models like BERT, which can capture the meaning of subwords and handle out-of-vocabulary words effectively.\n",
    "\n",
    "`Sentence Tokenization`: Sentence tokenization divides text into individual sentences. It is used to process and analyze text at the sentence level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "From Tokens features are created\n",
    "1. **N-grams:** Extracting multi-word phrases (n-grams) to capture more context (e.g., \"natural language processing\" as a bigram).\n",
    "\n",
    "2. **Entity Recognition:** Identifying and labeling entities (e.g., names of people, organizations, locations) in the text.\n",
    "\n",
    "3. **Part-of-Speech Tagging:** Assigning parts of speech (e.g., noun, verb, adjective) to words in the text.\n",
    "\n",
    "4. **Encoding/Decoding:** Converting text into numerical representations, such as one-hot encoding or word embeddings, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Vectorization\n",
    "\n",
    "1. `Bag of Words`\n",
    "![](https://uc-r.github.io/public/images/analytics/feature-engineering/bow-image.png)\n",
    "\n",
    "\n",
    "2. `TF-IDF`\n",
    "![](https://miro.medium.com/max/3604/1*ImQJjYGLq2GE4eX40Mh28Q.png)\n",
    "\n",
    "\n",
    "Representation of Text for Sequence Task\n",
    "\n",
    "    Every text in a sentence is represented using one hector vector based on its position in the vocabulary\n",
    "![](https://cdn-images-1.medium.com/max/1200/1*YEJf9BQQh0ma1ECs6x_7yQ.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `Word Embeddings`\n",
    "\n",
    "BOW and TF-IDF are spare representation of Tokens. In contrast Embedding refer to dense vector representations of Tokens in a continuous vector space. These embeddings are used to represent words or other linguistic units in a way that captures semantic relationships and contextual information.\n",
    "\n",
    "Embeddings are a fundamental component of many NLP applications, enabling models to understand and work with textual data in a way that captures semantic information and relationships between words. They have revolutionized the field of NLP and have significantly improved the performance of various NLP tasks.\n",
    "\n",
    "    1. Word2Vec\n",
    "    2. Glove\n",
    "    3. FastText        \n",
    "    4. ELMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "1.  RNN\n",
    "2.  LSTM\n",
    "3.  BI-LSTM\n",
    "4.  GRU\n",
    "5.  CNNs\n",
    "\n",
    "### Architectures\n",
    "1.  Seq-Seq\n",
    "\n",
    "7.  Seq-Seq Attention\n",
    "8.  Pointer Generator Network\n",
    "8.  Transformer\n",
    "9.  GPT\n",
    "10. Transformer-XL\n",
    "11. BERT\n",
    "12. GPT-2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constituency Parsing\n",
    "# Dependency Parsing\n",
    "# semantics\n",
    "# NLI\n",
    "# Grounding\n",
    "# Co-reference resolutin\n",
    "# Multilingual MT\n",
    "# corenlp.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
